% ====================================================
\section{Multi-fidelity Monte Carlo}\label{sec:MFMC}
% ====================================================
%
The standard Monte Carlo finite element estimator is often computationally prohibitive due to the large number of high-fidelity samples required to achieve a prescribed accuracy. To mitigate this cost, multilevel Monte Carlo (MLMC) methods \cite{Gi:2008,Gi:2015} and multi-fidelity Monte Carlo (MFMC) approaches \cite{PeWiGu:2016} have been developed. MFMC,  in particular, considers surrogate models of varying fidelities through the framework of {\it approximate control variate Monte Carlo} \cite{GoGeElJa:2020}, which exploits statistical 
correlations between models of varying fidelity to construct a recursive, variance-reducing estimator. When the low-fidelity models are strongly correlated with the high-fidelity counterpart, MFMC yields substantial variance reduction. Moreover, if the expected value of the low-fidelity model can be estimated at relatively low cost, the control variate correction becomes both efficient and effective. This strategy preserves estimator unbiasedness while significantly improving computational efficiency through sample reuse and optimal weighting across model hierarchies. 



Note that both MFMC and MLMC share the goal of variance reduction, but differ in structure and sampling \cite{ArGuMoWi:2025,PeGuWi:2018}. In MLMC, corrections between fine discretization levels are accumulated to the coarse, with independent samples and decreasing sample counts at higher resolutions. MFMC, by contrast, adds corrections of low-fidelity models to the high-fidelity model and incorporates increasing numbers of inexpensive, low-fidelity samples. A key distinction is MFMCâ€™s reuse of samples across fidelity levels, which avoids the cost of generating independent samples at each level and further improves efficiency. Overall, MFMC offers a flexible and scalable alternative to standard MC and MLMC, particularly suited for problems where high-fidelity models are accurate but computationally intensive.


\subsection{Multi-fidelity Monte Carlo revisit}
We now briefly review the core principles underlying multi-fidelity Monte Carlo, drawing on the framework in \cite{PeWiGu:2016}.  The MFMC framework combines a high-fidelity model $u_{h,1} = u_h$ with a series of low-fidelity models $u_{h,k}: \Omega \rightarrow U$ for $k=2,\ldots,K$.  The high-fidelity model yields accurate but computationally expensive evaluations, whereas the low-fidelity models offer cheaper approximations with reduced accuracy. MFMC optimally allocates computational resources across the fidelity levels to reduce the overall estimator variance, reducing the reliance on costly high-fidelity samples while preserving estimator accuracy and robustness.


For convenience, we may use $u$ and $u_{h,k}$ to refer to the exact $u(\omega)$ and discrete $u_{h,k}(\omega)$ solutions with random variable $\omega$ in the later context. For each pair of $u_{h,k}(\omega)$ and $u_{h,j}(\omega)$, we define the variance and the Pearson correlation coefficient as
%
\begin{equation*}
    \sigma_k^2 = \mathbb{V}\left[u_{h,k}(\omega)\right],\qquad \rho_{k,j} 
                       = \frac{\text{Cov}\left[ u_{h,k}(\omega), u_{h,j}(\omega)\right]}{\sigma_k\sigma_j}, \quad k,j=1,\dots, K,
\end{equation*}
%
where the covariance is $\text{Cov}[u_{h,k}, u_{h,j}] := \mathbb{E}[\langle u_{h,k} - \mathbb{E}[u_{h,k}], u_{h,j} - \mathbb{E}[u_{h,j}]\rangle_U]$ and, by definition, $\rho_{k,k}=1$. The multi-fidelity Monte Carlo finite element estimator $A^{\text{MF}}$ augments a high-fidelity Monte Carlo estimate with control variate corrections from low-fidelity models
%
\begin{equation}\label{eq:MFMC_estimator}
    A^{\text{MF}} := A^{\text{MC}}_{1,N_1} + \sum_{k=2}^K \alpha_k\left(\overline{A}_{k,N_k} - \overline{A}_{k,N_{k-1}} \right),
\end{equation}
%
where $A^{\text{MC}}_{1,N_1}$ is the standard Monte Carlo estimator using $N_1$ samples of the high-fidelity model, $\alpha_k\in \mathbb{R}$ are control variate weights, and $\overline{A}_{k,N_k}$ denotes the sample average of model $u_{h,k}$ over $N_k$ samples. The control variate construction requires the nesting condition $N_{k-1}\le N_k$ for $k=2,\ldots, K$, as $\overline{A}_{k,N_{k}}$ reuses all $N_{k-1}$ samples from $\overline{A}_{k,N_{k-1}}$, possibly supplemented by additional $N_{k} - N_{k-1}$ samples. This reuse introduces statistical dependence between $\overline{A}_{k,N_{k-1}}$ and $\overline{A}_{k,N_{k}}$. To eliminate sampling dependence in the correction terms, we partition the $N_k$ samples into two disjoint sets of sizes  $N_{k-1}$ and $N_k - N_{k-1}$. This allows us to reformulate the multi-fidelity estimator \eqref{eq:MFMC_estimator} as
%
\begin{equation}\label{eq:MFMC_estimator_independent}
    A^{\text{MF}} = A^{\text{MC}}_{1,N_1} +  \sum_{k=2}^K \alpha_k\left(1-\frac{N_{k-1}}{N_{k}}\right)\left(A_{k,N_k\backslash N_{k-1}}^{\text{MC}}-A_{k,N_{k-1}}^{\text{MC}}\right),
\end{equation}
%
where $A^{\text{MC}}_{k,N_k \backslash N_{k-1}}$ denotes the Monte Carlo average over the $N_k - N_{k-1}$ samples not included in $A^{\text{MC}}_{k,N_{k-1}}$, and is defined to be zero whenever $N_k=N_{k-1}$. In this formulation, the two terms in each correction are evaluated on independent sample sets, which simplifies variance analysis. We can now express the MFMC estimator in compact form
%
\begin{equation*}\label{eq:MFMC_estimator_Correction}
A^{\text{MF}} = Y_1 + \sum_{k=2}^K \alpha_k Y_k,
\end{equation*}
%
where the correction terms $Y_k$ are defined as
%
\begin{equation} \label{eq:MFMC_Yk}
Y_1 :=A^{\text{MC}}_{1,N_1},\qquad Y_k:=\overline{A}_{k,N_k} - \overline{A}_{k,N_{k-1}}=\left(1-\frac{N_{k-1}}{N_{k}}\right)\left(A_{k,N_k\backslash N_{k-1}}^{\text{MC}}-A_{k,N_{k-1}}^{\text{MC}}\right), \quad k=2\ldots, K.
\end{equation}
%
Since $Y_k$ is defined as the difference between two independent Monte Carlo estimators of the same model, it is unbiased: $\mathbb{E}[Y_k] = 0$ for $k\ge 2$. Consequently, the MFMC estimator is itself unbiased, satisfying $\mathbb{E}[A^{\text{MF}}] = \mathbb{E}[u_{h,1}]$. The variances of the correction terms $Y_k$ are
%
\begin{equation}\label{eq:Var_Yk}
    \mathbb{V}\left[Y_1\right] = \frac{\sigma_1^2}{N_1}, \quad \mathbb{V}\left[Y_k\right] = \left(1-\frac{N_{k-1}}{N_{k}}\right)^2\left(\frac{\sigma_k^2}{N_{k-1}}+\frac{\sigma_k^2}{N_k-N_{k-1}}\right) = \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\sigma_k^2.
\end{equation}
%
Although $Y_k$ and $Y_j$ for $2\le k<j \le K$ share overlapping sample sets and are therefore statistically dependent, they remain uncorrelated, as established in Lemma~\ref{lemma:Y_k_Y_j} in the appendix. However, each correction term $Y_k$ with $k\ge 2$ is correlated with the high-fidelity estimator $Y_1$. Using the covariance identity derived from \cite[Lemma~3.2]{PeWiGu:2016}, yields
%
\begin{equation}\label{eq:Cov_Yk}
% \text{Cov}(Y_k,Y_j) =0,\quad \text{for } \;2\le k<j \le K,\qquad 
\text{Cov}[Y_1,Y_k] = - \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\rho_{1,k}\sigma_1\sigma_k, \quad \text{for } \; k\ge 2.
\end{equation}
%
Combining \eqref{eq:Var_Yk} and \eqref{eq:Cov_Yk}, the total variance of the MFMC estimator is 
%
\begin{align}
    \nonumber
    \mathbb{V}\left[A^{\text{MF}}\right] &= \mathbb{V}\left[Y_1\right] + \mathbb{V}\left[\sum_{k=2}^K \alpha_kY_k\right]+2\;\text{Cov}\left[Y_1,\sum_{k=2}^K \alpha_k Y_k \right],\\
    \nonumber
    &=\mathbb{V}\left[Y_1\right] + \sum_{k=2}^K \alpha_k^2 \mathbb{V}\left[Y_k\right]+2\sum_{2\le k<j\le K} \alpha_k\alpha_j\; \text{Cov}[Y_k,Y_j] +2\sum_{k=2}^K \alpha_k\;\text{Cov}\left[Y_1, Y_k\right],\\
    % \nonumber
    % &=\mathbb{V}\left(Y_1\right) + \sum_{k=2}^K \alpha_k^2 \mathbb{V}\left(Y_k\right) +2\sum_{k=2}^K \alpha_k\;\text{Cov}\left(Y_1, Y_k\right),\\
    \label{eq:MFMC_variance}
    &=\frac{\sigma_1^2}{N_1} + \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\left(\alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k\right).
\end{align}
%
The normalized mean square error of the multi-fidelity Monte Carlo estimator, $\mathcal{E}_{A^{\text{MF}}}^2$, quantifies its accuracy and is decomposed into two components -- the bias error $\mathcal{E}_{\text{Bias}}^2$ and the statistical error $\mathcal{E}_{\text{Stat}}^2$, the decomposition is written as 
%
\[
\mathcal{E}_{A^{\text{MF}}}^2= \frac{\left\Vert\mathbb{E}[u]-\mathbb{E}\left[A^{\text{MF}}\right] \right\Vert_{U}^2+\mathbb E\left[\left\Vert\mathbb{E}\left[A^{\text{MF}}\right]-A^{\text{MF}} \right\Vert_{U}^2\right]}{\left\Vert\mathbb{E}[u] \right\Vert_{U}^2} =\frac{\left\Vert\mathbb{E}[u]-\mathbb{E}\left[A^{\text{MF}}\right] \right\Vert_{U}^2}{\left\Vert\mathbb{E}[u] \right\Vert_{U}^2}+ \frac{\mathbb{V}\left[A^{\text{MF}}\right]}{\left\Vert\mathbb{E}[u] \right\Vert_{U}^2}=\mathcal{E}_{\text{Bias}}^2 + \mathcal{E}_{\text{Stat}}^2,
\]
%
where the variance term $\mathbb{V}[A^{\text{MF}}]$  can be explicitly expressed using \eqref{eq:MFMC_variance}. A splitting ratio $\theta$ is introduced as before to balance the contributions between these two components. The spatial resolution required to achieve the biased tolerance $\theta \epsilon^2$ is determined by estimating the number of spatial grid points $M_L$ at refinement level $L$, given by
%
\begin{equation}
    \label{eq:SLSGC_MLS_SpatialGridsNo}
    M_L = M_0s^{-L} \ge \left(\frac{\theta\epsilon}{c_u}\right)^{-\frac 1 {\alpha}} \qquad \text{ and } \qquad     L = \left\lceil \frac{1}{\alpha}\log_s \left(\frac{c_u M_0^\alpha}{\theta\epsilon}\right) \right\rceil,
\end{equation}
%
where $M_0$ is the number of grid points at the coarsest level, $s>1$ is the spatial refinement factor, $\alpha$ represents the convergence rate of the spatial discretization, and $c_u$ is a constant characterizing the discretization scheme. To determine the optimal sample sizes $N_k$ and control variate weights $\alpha_k$ in the MFMC estimator \eqref{eq:MFMC_estimator_independent}, we express the total computational cost for the MFMC estimator
%
\[
\mathcal{W}^{\text{MF}} = \sum_{k=1}^K C_kN_k,
\]
%
where $C_k$ is the cost of generating a single sample of model $u_{h,k}$, and $N_k$ is the corresponding sample count. Unlike previous formulations \cite{PeWiGu:2016} that derive sample sizes based on a fixed computational budget, our approach directly expresses the sample sizes and computational resources in terms of the desired accuracy $\epsilon$. This formulation offers greater flexibility in applications where accuracy targets are more relevant than rigid cost constraints. We formulate an optimization problem to determine the optimal sample sizes $N_k$ and weights $\alpha_k$ by minimizing the total sampling cost $\mathcal{W}^{\text{MF}}$, subject to three constraints. First, the normalized statistical error $\mathcal{E}_{\text{Stat}}^2$ enforces the desired estimator accuracy $(1-\theta)\epsilon^2$. Second,  the monotonicity constraints $N_{k-1}\le N_k$ for $k=2,\ldots, K$ ensures consistent sample reuse across fidelity levels. Third, all sample sizes must be non-negative. This leads to the following constrained optimization problem
%
\begin{equation}\label{eq:Optimization_pb_sample_size}
    \begin{array}{ll}
    \min \limits_{\begin{array}{c}\scriptstyle N_1,\ldots, N_K\in \mathbb{R} \\[-4pt]
\scriptstyle \alpha_2,\ldots,\alpha_K\in \mathbb{R}
\end{array}} &\displaystyle\sum\limits_{k=1}^K C_kN_k,\\
       \;\,\text{subject to} &\mathbb{V}\left[A^{\text{MF}}\right]- \epsilon_{\text{tar}}^2 = 0,\\[2pt]
       &\displaystyle -N_1\le 0,\quad \displaystyle N_{k-1}-N_k\le 0, \;\; k=2\ldots,K.
    \end{array}
\end{equation}
%
Since the finite variance implicitly indicates $N_1 > 0$, the problem remains well-posed. The solution to this problem, which yields explicit expressions for the optimal real-valued sample sizes and weights, is presented in Theorem~\ref{thm:Sample_size_est}. The proof of Theorem~\ref{thm:Sample_size_est} is provided in the appendix.




%
\begin{theorem}[Optimal MFMC Sample Allocation]
\label{thm:Sample_size_est}
Consider an ensemble of $K$ models $\{u_{h,k}\}_{k=1}^K$ each characterized by the standard deviation $\sigma_k$ of its output, the correlation coefficient $\rho_{1,k}$ with the highest-fidelity model $u_{h,1}$, and the computational cost per sample evaluation $C_k$. Define $\Delta_k = \rho_{1,k}^2 - \rho_{1,k+1}^2$ for $k = 1, \dots, K$, with the boundary convention $\rho_{1,K+1} = 0$. Assume the following conditions hold
%
\begin{alignat*}{3}
&(i)\;\; \textit{Correlation monotonicity}: \quad && |\rho_{1,1}| > \cdots > |\rho_{1,K}|, \\ 
&(ii)\;\; \textit{Cost-correlation ratio}: \quad && \frac{\Delta_{k}}{C_k} > \frac{\Delta_{k-1}}{C_{k-1}}, \quad k=2,\ldots,K. 
\end{alignat*}
%
Under these assumptions, the solution to the optimization problem \eqref{eq:Optimization_pb_sample_size} yields optimal weights $\alpha_k^*$ and sample sizes $N_k^*$
%
\begin{align}
    % \label{eq:MFMC_coefficients}
    % &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\\
    \label{eq:MFMC_SampleSize}
    &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\qquad \;N_k^*=\frac{\sigma_1^2}{\epsilon_\text{tar}^2}\sqrt{\frac{\Delta_{k}}{C_k}}\sum_{j=1}^K\sqrt{C_j\Delta_{j}}.
\end{align}
%
The resulting MFMC estimator \eqref{eq:MFMC_variance} achieves a variance of
%
\begin{equation}
\label{eq:MFMC_variance_optimal}
\mathbb{V}\left[A^{\text{MF}}\right] =
% \frac{\sigma_1^2}{N_1^*} - \sum_{k=2}^K \left(\frac{1}{N_{k-1}^*} - \frac{1}{N_k^*}\right)\rho_{1,k}^2\sigma_1^2=
\sigma_1^2\sum_{k=1}^K\frac{\Delta_{k}}{N_k^*},
\end{equation}
%
with total computational cost
%
\begin{equation}\label{eq:MFMC_sampling_cost}
    \mathcal{W}^\text{MF} = \sum_{k=1}^K C_k N_k^* = \frac{\sigma_1^2}{\epsilon_{\text{tar}}^2}\left(\sum_{k=1}^K\sqrt{C_k\Delta_{k}}\right)^2.
\end{equation}
%
\end{theorem}

In practical implementation, the correlation coefficients $\rho_{1,k}$ and computational costs $C_k$ are typically unknown a priori and must be estimated via pilot sampling. Additionally, the theoretically optimal sample sizes $N_k^* \in \mathbb{R}$ require integer rounding for implementation. Departing from the conditional rounding strategy in \cite{GrGuJuWa:2023, PeWiGu:2016} (floor function when $N_k^* \ge 1$, ceiling otherwise), we adopt a uniform rounding scheme: all sample sizes are rounded up using the ceiling function $\lceil N_k^* \rceil$. This approach guarantees $\mathbb{V}[A^{\mathrm{MF}}] \leq \epsilon_{\mathrm{tar}}^2$ since increased sample sizes reduce estimator variance in \eqref{eq:MFMC_variance_optimal}. The computational cost after rounding satisfies
%
\begin{equation}\label{eq:sampling_cost_bound}
    \sum_{k=1}^K C_k N_k^*\le \sum_{k=1}^K C_k \left\lceil N_k^*\right\rceil<\sum_{k=1}^K C_k N_k^* + \sum_{k=1}^K C_k,
\end{equation}
%
where the additive overhead $\sum_{k=1}^K C_k$ arises from the bound $N_k^*\le \lceil N_k^*\rceil< N_k^*+1$. Under Theorem~\ref{thm:Sample_size_est}'s cost-correlation ratio assumption, optimal sample sizes exhibit strict monotonicity $N_1^* < \cdots < N_K^*$. We impose the feasibility condition $\sum_{k=1}^K C_k N_k^* \geq \sum_{k=1}^K C_k$ to exclude degenerate cases where $N_k^* < 1$ for all $k$ models; in such cases, the additive rounding overhead is asymptotically dominated by $\sum_{k=1}^K C_k N_k^*$. Consequently, integer-rounded cost preserve the asymptotic scaling of \eqref{eq:MFMC_sampling_cost}.

% define $B_k := C_k(\rho_{1,k}^2 - \rho_{1,k+1}^2)$ for $k=1,\dots,K$ with $\rho_{1,K+1} = 0$. Condition (ii) of Theorem~\ref{thm:Sample_size_est} implies
% % Substituting into the sampling cost expression,  \eqref{eq:MFMC_sampling_cost} becomes
% % %
% % \begin{equation*}\label{eq:MFMC_sampling_cost_2}
% %     \mathcal{W}^{\text{MF}} = \sum_{k=1}^K C_k N_k^* = \frac{\sigma_1^2}{\epsilon_{\text{tar}}^2}\left(\sum_{k=1}^K\sqrt{B_k} \right)^2.
% % \end{equation*}
% %
% % The quantity $B_k$ depends on the product of the cost per sample $C_k$ and the difference between two successive correlations $(\rho_{1,k}^2 - \rho_{1,k+1}^2)$. Depending on how these components interact, $B_k$ may decay, grow, or remain constant as $k$ increases.
% %
% \begin{equation}
% \label{eq:Bk_Ck_decay_rate}
%     \frac{\sqrt{B_{k}}}{\sqrt{B_{k-1}}}>\frac{C_{k}}{C_{k-1}}, \quad k=2,\ldots,K.
% \end{equation}
% %
% This inequality induces a strictly increasing sequence $\{\sqrt{B_k}/C_k\}_{k=1}^K$. Consequently, as $K \to \infty$, the sequence $\sqrt{B_k}$ decays slower (or grows faster) than $C_k$ in relative terms. Combined with the feasibility condition ($\sum_{k=1}^K C_kN_k^*\ge \sum_{k=1}^K C_k$), this ensures the overhead $\sum_{k=1}^K C_k$ is asymptotically negligible compared to $\sum_{k=1}^K C_k N_k^* \propto \left( \sum_{k=1}^K \sqrt{B_k} \right)^2$. 


% This inequality implies that, in the asymptotic regime where $K$ is large,  the sequence $\sqrt{B_k}$ decays more slowly -- or grows more rapidly -- than the cost sequence $C_k$, regardless of the specific trend of $\sqrt{B_k}$. \JLcolor{Using this fact and the assumption that $\sum_{k=1}^K C_kN_k^*\ge \sum_{k=1}^K C_k$}, the additive overhead term $\sum_{k=1}^K C_k$ in the cost bounds becomes asymptotically negligible relative to the leading-order term $\sum_{k=1}^K C_kN_k^*$. 


% Using the fact that $N_k$ increases and the value of $\alpha_k$, we observe that the MFMC estimator variance $\mathbb{V}\left(A^{\text{MFMC}}\right)$ in \eqref{eq:MFMC_variance2} always decreases as the model number $K$ increases. This reflects the fact that the low fidelity models are used as control variates to reduce the variance of the high fidelity model. However, this $K$ cannot be arbitrarily large, since the first summation term in \eqref{eq:MFMC_sampling_cost} grows, the second summation reflect the variance decay of the MFMC estimator. Thus this is a tie between these two terms. If $K$ is sufficiently large,  in order to achieve an optimal sampling cost, we need to study the decay and growth of these two terms. We will choose the $K$ such that the product of two summation terms in \eqref{eq:MFMC_sampling_cost} is minimum, i.e. If $K$ is sufficiently large, we need to find $K\in \mathbb{N}$ such that 
% \begin{equation}\label{eq:Optimal_K}
%    K = \text{argmin} \sum_{k=1}^K\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}\sum_{k=1}^K\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2. 
% \end{equation}
The efficiency gain relative to standard Monte Carlo is quantified through the cost ratio
%
\begin{equation}\label{eq:MFMC_sampling_cost_efficiency}
    \xi(\boldsymbol{\rho}) = \frac{\mathcal{W}^\text{MF}}{\mathcal{W}^\text{MC}} = \frac{1}{C_1} \left(\sum_{k=1}^K\sqrt{C_k\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)}\right)^2,
\end{equation}
%
where $\boldsymbol{\rho} = (\rho_{1,1},\ldots, \rho_{1,K})$ is the correlation coefficient vector and smaller $\xi$ indicates greater efficiency gains for the MFMC estimator.


% Further more, we observe that
% \begin{align*}
%     \mathcal{W}_\text{MC}\mathbb{V}\left(A^{\text{MC}}\right) &=\frac{C_1\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{U}^2},\\
%  \mathcal{W}_\text{MFMC}\mathbb{V}\left(A^{\text{MFMC}}\right) &=  \frac{\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{U}^2}\sum_{k=1}^K\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}\sum_{k=1}^K\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2.
% \end{align*}
% This implies that if both Monte Carlo and multifidelity Monte Carlo have  a same sampling cost, then $\mu=  \mathbb{V}\left(A^{\text{MFMC}}\right)/\mathbb{V}\left(A^{\text{MC}}\right)$. Therefore, 

