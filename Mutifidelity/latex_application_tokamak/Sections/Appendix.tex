% ====================================================
\section{Appendix}\label{sec:Appendix}
% ====================================================
\subsection{More lemmas and theorems}
\begin{lemma}\label{lemma:Y_k_Y_j}
Let $2\le k<j\le K$. Then the correction terms $Y_k$ and $Y_j$ defined in \eqref{eq:MFMC_Yk} are uncorrelated; that is,
\begin{equation*}
    \operatorname{Cov} \left[Y_k,Y_j\right]=0.
\end{equation*}
\end{lemma}

\begin{proof}
Fix indices $2\le k<j\le K$. Since $N_{k-1}\le N_k\le N_{j-1}$, we can partition the $N_{j-1}$ samples into three mutually disjoint subsets with sizes $N_{k-1}$, $N_{k}-N_{k-1}$, and $N_{j-1} - N_{k}$. The samples in these three subsets are mutually independent. From the definition of $Y_k$ in \eqref{eq:MFMC_Yk},  the covariance between $Y_k$ and $Y_j$ is then given by
\begin{align*}
    \operatorname{Cov}\left[Y_k,Y_j\right] &= \left(\frac{N_{k-1}}{N_k}-1\right) \left(\frac{N_{j-1}}{N_j}-1\right)\operatorname{Cov}\left[A_{k, N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}} - A_{j,N_{j}\backslash N_{j-1}}^{\text{MC}}\right] \\
    & = M \left(\operatorname{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}}\right] - \operatorname{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j}\backslash N_{j-1}}^{\text{MC}}\right] \right)\\
    & = M \operatorname{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}}\right]
\end{align*}
where we define $M = (N_{k-1}/N_k-1) (N_{j-1}/N_j-1)$. The second term in the covariance vanishes due to independence between the samples used in $A_{j,N_{j}\backslash N_{j-1}}^{\text{MC}}$ and those in $Y_k$. Next, we express $A_{j,N_{j-1}}^{\text{MC}}$ as a weighted Monte Carlo estimator over the three disjoint sample subsets
%
\begin{equation*}
    A_{j,N_{j-1}}^{\text{MC}} = \frac{N_{k-1}}{N_{j-1}}A_{j,N_{k-1}}^{\text{MC}} + \frac{N_k - N_{k-1}}{N_{j-1}} A_{j,N_{k}\backslash N_{k-1}}^{\text{MC}} + \frac{N_{j-1} - N_k}{N_{j-1}} A_{j,N_{j-1}\backslash N_{k}}^{\text{MC}}.
\end{equation*}
%
Substituting this expansion into the covariance expression yields
%
\begin{align*}
    % \frac{\operatorname{Cov}\left[Y_k,Y_j\right]}{M} &= 
    &\operatorname{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}}\right]
    % &= \operatorname{Cov}\left[A_{N_{k-1}}^k, \frac{N_{k-1}}{N_{j-1}}A_{N_{k-1}}^j + \frac{N_k - N_{k-1}}{N_{j-1}} A_{N_{k}\backslash N_{k-1}}^j + \frac{N_{j-1} - N_k}{N_{j-1}} A_{N_{j-1}\backslash N_{k}}^j\right] \\
    % &- \operatorname{Cov}\left[ A_{N_{k}\backslash N_{k-1}}^k, \frac{N_{k-1}}{N_{j-1}}A_{N_{k-1}}^j + \frac{N_k - N_{k-1}}{N_{j-1}} A_{N_{k}\backslash N_{k-1}}^j + \frac{N_{j-1} - N_k}{N_{j-1}} A_{N_{j-1}\backslash N_{k}}^j\right]\\
    =\operatorname{Cov}\left[A_{k,N_{k-1}}^{\text{MC}}, \frac{N_{k-1}}{N_{j-1}}A_{j,N_{k-1}}^{\text{MC}}\right]-\operatorname{Cov}\left[ A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, \frac{N_k - N_{k-1}}{N_{j-1}} A_{j,N_{k}\backslash N_{k-1}}^{\text{MC}} \right]\\
    &=\frac{\operatorname{Cov}\left[N_{k-1}A_{k,N_{k-1}}^{\text{MC}}, N_{k-1} A_{j,N_{k-1}}^{\text{MC}}\right]}{N_{j-1}N_{k-1}}-\frac{\operatorname{Cov}\left[(N_k-N_{k-1}) A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, (N_k - N_{k-1}) A_{j,N_{k}\backslash N_{k-1}}^{\text{MC}} \right]}{N_{j-1}(N_k-N_{k-1})}\\
    &=\frac{\operatorname{Cov}\left[\sum_{i=1}^{N_{k-1}}u_{k}^{(i)},\sum_{i=1}^{N_{k-1}}u_{j}^{(i)}\right]}{N_{j-1}N_{k-1}}
    -\frac{\operatorname{Cov}\left[\sum_{i=1}^{N_k-N_{k-1}}u_{k}^{(i)}, \sum_{i=1}^{N_k-N_{k-1}}u_{j}^{(i)}\right]}{N_{j-1}(N_k-N_{k-1})}\\
    &=\frac{\sum_{i=1}^{N_{k-1}}\operatorname{Cov}\left[u_{k}^{(i)},u_{j}^{(i)}\right]}{N_{j-1}N_{k-1}} -\frac{\sum_{i=1}^{N_k-N_{k-1}}\operatorname{Cov}\left[u_{k}^{(i)}, u_{j}^{(i)}\right]}{N_{j-1}(N_k-N_{k-1})}=\frac{N_{k-1}\rho_{k,j}\sigma_k\sigma_j}{N_{j-1}N_{k-1}}-\frac{(N_k-N_{k-1})\rho_{k,j}\sigma_k\sigma_j}{N_{j-1}(N_k-N_{k-1})}=0
\end{align*}
\end{proof}

\subsection{Proof of Theorem \ref{thm:Sample_size_est}}

\begin{proof}
The proof consists of three main parts: (A) verification of feasibility for the closed-form solution, (B) derivation of optimality through KKT conditions, and (C) proof of global optimality via convexity and cost comparison.

\noindent {\bf  Part A: Feasibility Verification}

First, we verify that the proposed solution $(\alpha_k^*, N_k^*)$ satisfies all constraints:
\begin{enumerate}
    \item \textit{Variance constraint}: $N_k^*$ satisfies the variance constraint ($\mathbb{V}[A^{\text{MF}}] = \epsilon_{\text{tar}}^2$).
    
    \item \textit{Monotonicity constraint}: Using assumption (ii), we show strict increase in sample sizes:
    \begin{align*}
    \frac{N_k^*}{N_{k-1}^*} &= \sqrt{ \frac{(\rho_{1,k}^2 - \rho_{1,k+1}^2)/C_k}{(\rho_{1,k-1}^2 - \rho_{1,k}^2)/C_{k-1}} } = \sqrt{ \frac{C_{k-1}}{C_k} \cdot \frac{\rho_{1,k}^2 - \rho_{1,k+1}^2}{\rho_{1,k-1}^2 - \rho_{1,k}^2} } > 1,
    \end{align*}
    where the inequality follows from assumption (ii). Thus $N_k^* > N_{k-1}^*$ for $k=2,\ldots,K$.
    
    \item \textit{Non-negativity}: $N_k^* > 0$ since $\sigma_1 > 0$, $\epsilon_{\text{tar}} > 0$, and $\rho_{1,k}^2 - \rho_{1,k+1}^2 > 0$ by assumption (i).
\end{enumerate}

\noindent {\bf Part B: Optimality via KKT Conditions (inner-block cost derivation)}

Consider feasible solutions in which the sample sizes $N_k^*$ are non-decreasing but may include segments where they are constant. Let $\{\ell_1, \ldots, \ell_q\}\subseteq \{1,\ldots, K\}$ be the indices marking the start of each constant block, with $\ell_1=1$ and $\ell_{q+1} = K+1$, such that
%
\[
N_{\ell_1}<N_{\ell_2}<\ldots < N_{\ell_{q}},\quad\quad  N_{\ell_i}=N_{\ell_i+1}=\ldots = N_{\ell_{i+1}-1} <N_{\ell_{i+1}}, \qquad \text{for}\;\;  i=1,\ldots,q-1.
\]
%
This partitions the indices ${1,\dots,K}$ into $q$ contiguous blocks of constant sample sizes, increasing from one block to the next. 

Three special cases illustrate the structure of such feasible solutions. When $q=1$, all sample sizes are equal, i.e., $N_1=\ldots=N_K$. From \eqref{eq:MFMC_variance}, we then have $N_k=\sigma_1^2/\epsilon_{\text{tar}}^2$ for $k=1,\ldots, K$, $\alpha_k\in \mathbb{R}$, and the total cost is $\mathcal{W}^\text{MF} = \sigma_1^2/\epsilon_{\text{tar}}^2 \sum_{k=1}^K C_k$. When $q=K$, the sample sizes are strictly increasing, i.e., $N_1<\ldots<N_K$,  and we will show that this configuration yields the globally optimal solution. For intermediate values $1<q<K$, the sample sizes exhibit piecewise-constant blocks. In the following, to analyze this general regime, we formulate the corresponding Lagrangian and derive the associated Karush–Kuhn–Tucker (KKT) conditions.



The Lagrangian for the constrained problem, enforcing the variance constraint with multiplier $\lambda_0$ and monotonicity with $\lambda_1,\ldots, \lambda_k$, is
%
\begin{equation*}
L =\sum_{k=1}^K C_kN_k +\lambda_0 \left(\frac{\sigma_1^2}{N_1} + \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)G_k- \epsilon_{\text{tar}}^2\right)-\lambda_1 N_1+\sum_{k=2}^K\lambda_k(N_{k-1} - N_k),
\end{equation*}
%
with $\alpha_1 = 1, \alpha_{K+1} = 0$, $\lambda_{K+1} = 0$ and $G_k = \alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k$.  The KKT conditions includes
%
\[
\begin{array}{ll}
\left[\text{Stationarity}\right]&\frac{\partial L}{\partial \alpha_j}=0,\quad \frac{\partial L}{\partial N_k}=0,\quad j=2\ldots,K, \quad k=1\ldots,K,\\
\left[\text{Primal feasibility}\right]&\mathbb{V}\left[A^{\text{MF}}\right]- \epsilon_{\text{tar}}^2 = 0, \\ 
\left[\text{Primal feasibility}\right] &-N_1\le 0,\qquad N_{k-1}-N_k \le 0, \quad k=2\ldots,K,\\ 
\left[\text{Dual feasibility}\right]  &\lambda_k \ge 0,\quad k=1\ldots,K, \\ 
\left[\text{Complementary slackness}\right]  &\lambda_1 N_1=0,\qquad\lambda_k(N_{k-1}-N_k)=0,\quad k=2\ldots,K.
\end{array}
\]
%


Within each constant block, complementary slackness implies $\lambda_{\ell_i}= 0$ all $i\ge 1$, since the sample sizes within each block are equal. Under the blockwise structure $N_{\ell_i}=N_{\ell_i+1}=\ldots=N_{\ell_{i+1}-1}< N_{\ell_{i+1}}$ for $i=1,\ldots,q$, the Lagrangian simplifies to
%
\begin{equation*}
L= \sum_{i=1}^q N_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_k +\lambda_0 \left(\frac{\sigma_1^2}{N_{\ell_1}} + \sum_{i=2}^q \left(\frac{1}{N_{\ell_i-1}} - \frac{1}{N_{\ell_i}}\right)G_{\ell_i}- \epsilon_{\text{tar}}^2\right)-\lambda_{\ell_1} N_{\ell_1}+\sum_{i=2}^q\lambda_{\ell_{i}}(N_{\ell_{i-1}} - N_{\ell_{i}}),
\end{equation*}
%
Stationarity of the Lagrangian with respect to $\alpha_{\ell_i}$ yields
%
\begin{align}
\label{eq:partial_L_alpha_k}
    \frac{\partial L}{\partial \alpha_{\ell_i}}&=\lambda_0\left(\frac{1}{N_{\ell_i-1}} - \frac{1}{N_{\ell_i}}\right)\left(2\alpha_{\ell_i}\sigma_{\ell_i}^2 - 2\rho_{1,\ell_i}\sigma_1\sigma_{\ell_i}\right),\quad i=1,\dots,q-1,
    % \frac{\partial L}{\partial N_1}&=C_1 + \lambda_0\left(-\frac{\sigma_1^2}{N_1^2} - \frac{\alpha_2^2\sigma_2^2-2\alpha2\rho_{1,2}\sigma_1\sigma_2}{N_1^2}\right)-\lambda_1+\lambda_2,\\
    % \label{eq:partial_L_N_k}
    % \frac{\partial L}{\partial N_k}&=C_k+\lambda_0\left(\frac{G_k}{N_k^2}-\frac{G_{k+1}}{N_k^2}\right)-\lambda_k+\lambda_{k+1}, \quad k=1,\dots,K,
    % \frac{\partial L}{\partial N_K}&=C_K + \lambda_0\left(\frac{\alpha_K^2\sigma_K^2 - 2\alpha_K\rho_{1,K}\sigma_1\sigma_K}{N_K^2}\right)-\lambda_K.
\end{align}
%
Solving $\partial L/\partial \alpha_{\ell_i} = 0$ gives the optimal weights
%
\[
\alpha_{k}^* = \frac{\rho_{1,k}\sigma_1}{\sigma_{k}}, \text{ if } k=\ell_i, \text{ for }i= 1,\ldots,q.
\]
%
Note only indices $k=\ell_i$ contribute non-trivially to the estimator weights in \eqref{eq:MFMC_Yk}. For indices $k = \ell_i+1,\ldots, \ell_{i+1}-1$, the corresponding Monte Carlo estimators cancel due to shared sample sets, and thus do not appear explicitly in the final estimator. Using the optimal coefficients $\alpha_k^*$, and observing that $N_{\ell_{i}-1} = N_{\ell_{i-1}}$ for $i \ge 2$, we define $\Delta_k = (\rho_{1,k}^2 - \rho_{1,k+1}^2)\sigma_1^2$, where $\rho_{1,K+1} = 0$. Substituting into the variance expression yields the simplified form
%
\begin{equation}\label{eq:MFMC_var_convex}
    \mathbb{V}\left[A^{\text{MF}}\right] = \frac{\sigma_1^2}{N_1}+\sum_{i=2}^q
\left(\frac{1}{N_{\ell_{i}}}-\frac{1}{N_{\ell_{i}-1}}\right)\rho_{1,\ell_i}^2\sigma_1^2=\sum_{i=1}^{q} \frac{ \left(\rho_{1,\ell_i}^2-\rho_{1,\ell_{i+1}}^2\right)\sigma_1^2}{N_{\ell_i}}=\sum_{i=1}^{q} \frac{\Delta_{\ell_i}}{N_{\ell_i}}.
\end{equation}
%

Substituting the optimal coefficients into the Lagrangian and using the block-wise sample size notation, we obtain
%
\begin{equation*}
L= \sum_{i=1}^q N_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_k +\lambda_0 \left( \sum_{i=1}^{q} \frac{\Delta_{\ell_i}}{N_{\ell_i}}- \epsilon_{\text{tar}}^2\right)-\lambda_1 N_1+\sum_{i=2}^q\lambda_{\ell_{i}}(N_{\ell_{i-1}} - N_{\ell_{i}}).
\end{equation*}
%
The stationarity condition for $N_{\ell_i}$ yields
%
\[
\frac{\partial L}{\partial N_{\ell_i}} =\sum_{k=\ell_i}^{\ell_{i+1}-1} C_{k} -  \frac{\lambda_0\Delta_{\ell_i}}{N_{\ell_i}^2}-\lambda_{\ell_{i}}+\lambda_{\ell_{i+1}}=0,\quad i = 1, \ldots,q,
\]
%
Since the sample sizes increase strictly ($N_{\ell_{i-1}} < N_{\ell_i}$ for $i = 2, \ldots, q$), all inequality constraints are inactive, implying $\lambda_{\ell_i} = 0$ by complementary slackness. The optimal sample sizes are then
%
\begin{equation}\label{eq:sample_size_1}
    N_{\ell_i} = \sqrt{\lambda_0} \sqrt{\frac{\Delta_{\ell_i}}{\sum_{k=\ell_i}^{\ell_{i+1}-1} C_{k}}}, \;\text{ for }\; i=1,\ldots,q,
\end{equation}
%
Substituting $\alpha_k^*$ and $N_{\ell_i}$ in \eqref{eq:sample_size_1} into the variance expression \eqref{eq:MFMC_var_convex} yields
%
\begin{equation*} \label{eq:MFMC_variance2}
    \mathbb{V}\left[A^{\text{MF}}\right] = \frac{1}{\sqrt{\lambda_0}}\sum_{i=1}^q\sqrt{\Delta_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_k},
\end{equation*}
%
Enforcing the variance constraint $\mathbb{V}[A^{\text{MF}}] = \epsilon_{\text{tar}}^2$ leads to
%
\[
\sqrt{\lambda_0}=\frac{1}{\epsilon_{\text{tar}}^2} \sum_{i=1}^{q} \sqrt{\Delta_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_{k}},
\]
%
Substituting $\sqrt{\lambda_0}$ into \eqref{eq:sample_size_1} gives the explicit optimal sample sizes
%
\[
N_{\ell_i}^* = \frac{1}{\epsilon_{\text{tar}}^2}\sqrt{\frac{\Delta_{\ell_i}}{\sum_{k=\ell_i}^{\ell_{i+1}-1} C_{k}}}  \sum_{j=1}^{q} \sqrt{\Delta_{\ell_j}\sum_{k=\ell_j}^{\ell_{j+1}-1} C_{k}} \;\text{ for }\; i=1,\ldots,q.
\]
%

Note that for a fixed block partition (fixed $\ell_i$) of sample size $N_{\ell_i}^*$ with $\alpha_{\ell_i}^*$, then 
we introduce the change of variables $y_{\ell_i} = 1/N_{\ell_i}^*$, we reformulate the optimization problem \eqref{eq:Optimization_pb_sample_size} with block structure  as
%
\begin{equation}\label{eq:Optimization_pb_sample_size3}
    \begin{array}{ll}
    \min \limits_{\begin{array}{c}\scriptstyle y_{\ell_1},\ldots, y_{\ell_q}\in \mathbb{R}
\end{array}} &\displaystyle \sum_{i=1}^q C_{\ell_i}y_{\ell_i}^{-1},\\
       \;\,\text{subject to} &\displaystyle \sum_{i=1}^q \Delta_{\ell_i} y_{\ell_i}= \epsilon_{\text{tar}}^2,\\[2pt]
       &\displaystyle -y_{\ell_1}\le 0,\quad \displaystyle y_{\ell_i}-y_{\ell_{i-1}}\le 0, \;\; k=2\ldots,K.
    \end{array}
\end{equation}
%
Since $N_1 > 0$ is required for finite variance \eqref{eq:MFMC_var_convex}, the problem remains well-posed. The transformed problem in is convex as established: the objective function is convex in $y_{\ell_i}$, the equality constraint is affine, and the feasible set defined by monotonicity constraints is convex. Since any local minimum of a convex optimization problem is globally optimal, and we have found a KKT point $y_{\ell_i}^* = 1/N_{\ell_i}^*$ satisfying all optimality conditions of \eqref{eq:Optimization_pb_sample_size3}, this indicates that $N_k^*$ is the global minimizer within the block structure, but not the global minimizer of the original optimization problem \eqref{eq:Optimization_pb_sample_size}.


% Note that by ensuring the condition $(ii)$ is satisfied, we can guarantee that $N_k^*$ increases strictly as $k$ grows. 
The total cost $\mathcal{W}_{\text{block}}^{\text{MF}}$ associated with these optimal sample sizes in this block partition is
%
\begin{equation*}
\mathcal{W}_{\text{block}}^{\text{MF}} = \sum_{i=1}^q \sum_{k=\ell_i}^{\ell_{i+1}-1} C_k N_k = \sum_{i=1}^q N_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_k =\frac{1}{\epsilon_{\text{tar}}^2}\left(\sum_{i=1}^{q} \sqrt{\Delta_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_{k}}\right)^2.
\end{equation*}
%

\noindent {\bf Part C: Global optimality (inter-block cost comparison)}

To identify the global minimizer of the optimization problem \eqref{eq:Optimization_pb_sample_size}, we evaluate all minimizers among all possible block partitions
and select the one with the lowest cost. By the Cauchy-Schwarz inequality, the following inequality holds
%
\[
\sum_{i=1}^q \sqrt{ \Delta_{\ell_i} \sum_{k=\ell_i}^{\ell_{i+1}-1} C_k }  = \sum_{i=1}^q \sqrt{ \sum_{k=\ell_i}^{\ell_{i+1}-1}\Delta_k \sum_{k=\ell_i}^{\ell_{i+1}-1} C_k } \ge \sum_{i=1}^q \sum_{k=\ell_i}^{\ell_{i+1}-1} \sqrt{\Delta_k C_k} = \sum_{k=1}^K \sqrt{\Delta_k C_k}
\]
%
Equality holds if and only if each block contains exactly one model (i.e., $q=K$), strictly increasing sample size structure. However, we note that equality could also hold if multiple models in a block happen to have identical $\Delta_k/C_k$ ratios. But under general problem conditions, we cannot guarantee this coincidence. Therefore, the optimal block structure must have $q=K$. Let $\mathcal{W}^\text{MF}$ be the total cost when sample sizes increase strictly with model index $k$, i.e., without block repetitions. Thus $\mathcal{W}_{\text{block}}^{\text{MF}} \geq \mathcal{W}^{\text{MF}}$, confirming the singleton block structure ($q=K$) is globally optimal. 


In this case, the optimal coefficients and sample sizes reduce to
%
\begin{align*}
    % \label{eq:MFMC_coefficients}
    % &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\\
    \label{eq:MFMC_SampleSize}
    &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\;\; \;N_k^*=\frac{1}{\epsilon_\text{tar}^2}\sqrt{\frac{\Delta_k}{C_k}}\sum_{j=1}^K\sqrt{C_j\Delta_j},\;\; \mathcal{W}^\text{MF} = \sum_{k=1}^K C_k N_k^* = \frac{1}{\epsilon_{\text{tar}}^2}\left(\sum_{k=1}^K\sqrt{C_k\Delta_k}\right)^2\quad \text{with}\;\;\rho_{K+1}=0.
\end{align*}
%








% The total cost expression follows directly by substituting $N_k^*$ into $\sum C_k N_k^*$.


\end{proof}



\subsection{Proof of Theorem \ref{thm:Sample_cost_est}}
\begin{proof}\label{eq:Sample_cost_est}
To derive the total sampling cost of the multi-fidelity Monte Carlo estimator, we first express the cost per sample for high- and low-fidelity models using conditions (ii) and (v) from Theorem \ref{thm:Sample_cost_est}, along with the mesh scaling relation \eqref{eq:MeshGrowth}
%
\[
C_1\simeq M_L^\gamma \simeq s^{L\gamma},
\]
%
Substituting these expressions into the MFMC sampling cost formula \eqref{eq:MFMC_sampling_cost}, we obtain
%
\begin{align*}
    \mathcal{W}^\text{MF} &\simeq \epsilon^{-2}\left(\sqrt{C_1\left(1 - \rho_{1,i_2}^2\right)}+\sum_{k=2}^{K^*} \sqrt{C_{i_k}\left(\rho_{1,{i_k}}^2 - \rho_{1,i_{k+1}}^2\right)} \right)^2 \simeq \epsilon^{-2}\left(s^{\frac{(\gamma-\beta)}{2}L}\right)^2.
\end{align*}

% %
% \begin{align*}
%     \mathcal{W}^\text{MF} &\simeq \epsilon^{-2}\left(\sqrt{C_1\left(\rho_{1,1}^2 - \rho_{1,2}^2\right)}+\sum_{k=2}^{L+1} \sqrt{C_k\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)} \right)^2 \simeq \epsilon^{-2} \left(c_1 M_L^{\frac{\gamma-\beta}{2}}+c_2\sum_{k=2}^{L+1}M_{L-k+1}^\frac{\gamma_1-\beta_1}{2}\right)^2,\\
%     &=\epsilon^{-2} \left(c_1 M_L^{\frac{\gamma-\beta}{2}}+c_2\sum_{p=0}^{L-1}M_{p}^\frac{\gamma_1-\beta_1}{2}\right)^2\simeq \epsilon^{-2}\left(c_1s^{\frac{(\gamma-\beta)}{2}L}+c_2\sum_{p=0}^{L-1}s^{\frac{(\gamma_1-\beta_1)}{2}p}\right)^2,
% \end{align*}
% %
Since $K^*\simeq L$, we use the geometric sum approximation
%
\begin{equation}
\label{eq:Geo_sum_for_s}
\sum_{p=0}^L s^{\eta p}\simeq\left\{\begin{array}{ll}
\frac{1}{1-s^{\eta}}, & \eta<0,\\
|\log \epsilon|, & \eta = 0,\\
\epsilon^{-\frac{\eta}{\alpha}}, & \eta>0,
\end{array}
\right.
\end{equation}
%
Applying \eqref{eq:SLSGC_MLS_SpatialGridsNo} and condition (i), we substitute $s^{\frac{(\gamma-\beta)}{2}L}\simeq \epsilon^{\frac{\beta-\gamma}{2\alpha}}$, leading to the asymptotic result
\[
\mathcal{W}^\text{MF} \simeq \epsilon^{-2+\frac{\beta-\gamma}{\alpha}}.
\]
\end{proof}



% \begin{proof}\label{eq:Sample_cost_est}
% To derive the total sampling cost of the multi-fidelity Monte Carlo estimator, we first express the cost per sample for high- and low-fidelity models using conditions (ii) and (v) from Theorem \ref{thm:Sample_cost_est}, along with the mesh scaling relation \eqref{eq:MeshGrowth}
% %
% \[
% C_1\simeq M_L^\gamma \simeq s^{L\gamma},\qquad  C_k \simeq M_{L-i_k+1}^{\gamma_1}\simeq s^{(L-i_k+1)\gamma_1},
% \]
% %
% Substituting these expressions into the MFMC sampling cost formula \eqref{eq:MFMC_sampling_cost} and using conditions (iii) and (iv), we obtain
% %
% \begin{align*}
%     \mathcal{W}^\text{MF} &\simeq \epsilon^{-2}\left(\sqrt{C_1\left(1 - \rho_{1,i_2}^2\right)}+\sum_{k=2}^{K^*} \sqrt{C_{i_k}\left(\rho_{1,{i_k}}^2 - \rho_{1,i_{k+1}}^2\right)} \right)^2 \simeq \epsilon^{-2}\left(c_1s^{\frac{(\gamma-\beta)}{2}L}+c_2\sum_{p=0}^{K^*-2}s^{\frac{(\gamma_1-\beta_1)}{2}p}\right)^2.
% \end{align*}

% % %
% % \begin{align*}
% %     \mathcal{W}^\text{MF} &\simeq \epsilon^{-2}\left(\sqrt{C_1\left(\rho_{1,1}^2 - \rho_{1,2}^2\right)}+\sum_{k=2}^{L+1} \sqrt{C_k\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)} \right)^2 \simeq \epsilon^{-2} \left(c_1 M_L^{\frac{\gamma-\beta}{2}}+c_2\sum_{k=2}^{L+1}M_{L-k+1}^\frac{\gamma_1-\beta_1}{2}\right)^2,\\
% %     &=\epsilon^{-2} \left(c_1 M_L^{\frac{\gamma-\beta}{2}}+c_2\sum_{p=0}^{L-1}M_{p}^\frac{\gamma_1-\beta_1}{2}\right)^2\simeq \epsilon^{-2}\left(c_1s^{\frac{(\gamma-\beta)}{2}L}+c_2\sum_{p=0}^{L-1}s^{\frac{(\gamma_1-\beta_1)}{2}p}\right)^2,
% % \end{align*}
% % %
% Since $K^*\simeq L$, we use the geometric sum approximation
% %
% \begin{equation}
% \label{eq:Geo_sum_for_s}
% \sum_{p=0}^L s^{\eta p}\simeq\left\{\begin{array}{ll}
% \frac{1}{1-s^{\eta}}, & \eta<0,\\
% |\log \epsilon|, & \eta = 0,\\
% \epsilon^{-\frac{\eta}{\alpha}}, & \eta>0,
% \end{array}
% \right.
% \end{equation}
% %
% Applying \eqref{eq:SLSGC_MLS_SpatialGridsNo} and condition (i), we substitute $s^{\frac{(\gamma-\beta)}{2}L}\simeq \epsilon^{\frac{\beta-\gamma}{2\alpha}}$. The remaining summation term $\sum_{p=0}^{K^*-2}s^{\frac{(\gamma_1-\beta_1)}{2}p}$ follows from \eqref{eq:Geo_sum_for_s}, yielding
% %
% \[
% \mathcal{W}^\text{MF} \simeq \left\{\begin{array}{ll}
% \epsilon^{-2}\left(c_1\epsilon^{\frac{\beta-\gamma}{2\alpha}}+c_2\right)^2, & \beta_1>\gamma_1,\\
% \epsilon^{-2}\left(c_1\epsilon^{\frac{\beta-\gamma}{2\alpha}}+c_2|\log\epsilon|\right)^2, & \beta_1=\gamma_1,\\
% \epsilon^{-2}\left(c_1\epsilon^{\frac{\beta-\gamma}{2\alpha}}+c_2\epsilon^{-\frac{\gamma_1-\beta_1}{2\alpha}}\right)^2, & \beta_1<\gamma_1.
% \end{array}
% \right.
% \]
% %
% When $c_1\gg c_2$ (or equivalently, $C_1$ is much larger than $C_i$ for $i\ge 2$), the dominant term in the expression is associated with $c_1$, allowing us to neglect the contributions from $c_2$, leading to the asymptotic result
% \[
% \mathcal{W}^\text{MF} \simeq \epsilon^{-2+\frac{\beta-\gamma}{\alpha}}.
% \]
% \end{proof}
% %
% \begin{theorem}
% \label{thm:Sample_cost_est}
%  Suppose there exist positive constants $\alpha, \gamma$ such that for high-fidelity models $ u_{h,1}$ at spatial grid levels $L=1,\ldots,L_m$
% %
% \begin{alignat*}{8}
%     % &(i)\;\; |\rho_{1,1}|>\ldots>|\rho_{1,K}|,& \quad \quad
%     % &(ii)\;\; \frac{C_{k-1}}{C_k}>\frac{\rho_{1,k-1}^2-\rho_{1,k}^2}{\rho_{1,k}^2-\rho_{1,k+1}^2},\;\;k=2,\ldots,K, \quad \rho_{1,K+1}=0,\\
%     &(i)\;\; \left\Vert\mathbb{E}\left(u- u_{h,1}\right)\right\Vert_Z\simeq M_{L}^{-\alpha},\qquad
%     % &(ii)\;\; \left(\rho_{1,L}^{H}\right)^2-\left(\rho_{1,L+1}^H\right)^2 \simeq M_{L}^{-\beta},
%     % \qquad
%     &(ii)\;\; C_1 \simeq M_{L}^{\gamma},
% \end{alignat*}
% %
% where $C_1$ is the cost per sample of the high-fidelity model at level $L$. Moreover, for the low-fidelity models with index $\{i_k | \; i_k\in \mathcal{I}^*,\;k=2, \ldots, K^*\}$, suppose there exist positive constants $\beta, \beta_1, \gamma_1$ such that 
% %
% \begin{alignat*}{8}
%     % &(i)\;\; |\rho_{1,1}|>\ldots>|\rho_{1,K}|,& \quad \quad
%     % &(ii)\;\; \frac{C_{k-1}}{C_k}>\frac{\rho_{1,k-1}^2-\rho_{1,k}^2}{\rho_{1,k}^2-\rho_{1,k+1}^2},\;\;k=2,\ldots,K, \quad \rho_{1,K+1}=0,\\
%     &(iii)\;\; 1-\rho_{1,i_2}^2 \simeq M_{L}^{-\beta},
%     \qquad
%     &(iv)\;\; \rho_{1,i_k}^2-\rho_{1,i_{k+1}}^2 \simeq M_{L-i_k+1}^{-\beta_1},
%     \qquad
% &(v)\;\; C_{i_k} \simeq M_{L-i_k+1}^{\gamma_1},
% \end{alignat*}
% %
% where $\rho_{1,i_k}$ is the correlated coefficient between the high-fidelity model $ u_{L,1}$ and low fidelity model $ u_{h,i_k}$, and $C_{i_k}$ is the cost per sample for $u_{h,i_k}$. Then for any positive $\epsilon<e^{-1}$, there exists level $L$ and sample size $N_{i_k}$ for $ i_k\in \mathcal{I}^*$, as given in \eqref{eq:MFMC_SampleSize}, such that the multi-fidelity estimator $A^{\text{MF}}$ has an nMSE with
% \[
% \frac{\left\Vert\mathbb{E}(u)-A^{\text{MF}} \right\Vert_{L^2(\boldsymbol W,Z)}}{\left\Vert\mathbb{E}(u) \right\Vert_{L^2( \boldsymbol W,Z)}}<\epsilon,
% \]
% with total sampling cost
% %
% \begin{equation*}
%     \mathcal{W}^{\text{MF}} \simeq \left\{\begin{array}{ll}
% \epsilon^{-2}\left(c_1\epsilon^{\frac{\beta-\gamma}{2\alpha}}+c_2\right)^2, & \beta_1>\gamma_1,\\
% \epsilon^{-2}\left(c_1\epsilon^{\frac{\beta-\gamma}{2\alpha}}+c_2|\log\epsilon|\right)^2, & \beta_1=\gamma_1,\\
% \epsilon^{-2}\left(c_1\epsilon^{\frac{\beta-\gamma}{2\alpha}}+c_2\epsilon^{-\frac{\gamma_1-\beta_1}{2\alpha}}\right)^2, & \beta_1<\gamma_1.
% \end{array}
% \right.
% \end{equation*}
% %
% Moreover, if $c_1\gg c_2$, then even if conditions (iv) and (v) do not hold for low-fidelity models, the dominant cost term simplifies to 
% \[
% \mathcal{W}^\text{MF} \simeq \epsilon^{-2+\frac{\beta-\gamma}{\alpha}}.
% \]
% \end{theorem}
% %


% \begin{proof}\label{eq:Sample_cost_est}
% To derive the total sampling cost of the multi-fidelity Monte Carlo estimator, we first express the cost per sample for high- and low-fidelity models using conditions (ii) and (v) from Theorem \ref{thm:Sample_cost_est}, along with the mesh scaling relation \eqref{eq:MeshGrowth}
% %
% \[
% C_1\simeq M_L^\gamma \simeq s^{L\gamma},\qquad  C_k \simeq M_{L-i_k+1}^{\gamma_1}\simeq s^{(L-i_k+1)\gamma_1},
% \]
% %
% Substituting these expressions into the MFMC sampling cost formula \eqref{eq:MFMC_sampling_cost} and using conditions (iii) and (iv), we obtain
% %
% \begin{align*}
%     \mathcal{W}^\text{MF} &\simeq \epsilon^{-2}\left(\sqrt{C_1\left(1 - \rho_{1,i_2}^2\right)}+\sum_{k=2}^{K^*} \sqrt{C_{i_k}\left(\rho_{1,{i_k}}^2 - \rho_{1,i_{k+1}}^2\right)} \right)^2 \simeq \epsilon^{-2}\left(c_1s^{\frac{(\gamma-\beta)}{2}L}+c_2\sum_{p=0}^{K^*-2}s^{\frac{(\gamma_1-\beta_1)}{2}p}\right)^2.
% \end{align*}

% % %
% % \begin{align*}
% %     \mathcal{W}^\text{MF} &\simeq \epsilon^{-2}\left(\sqrt{C_1\left(\rho_{1,1}^2 - \rho_{1,2}^2\right)}+\sum_{k=2}^{L+1} \sqrt{C_k\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)} \right)^2 \simeq \epsilon^{-2} \left(c_1 M_L^{\frac{\gamma-\beta}{2}}+c_2\sum_{k=2}^{L+1}M_{L-k+1}^\frac{\gamma_1-\beta_1}{2}\right)^2,\\
% %     &=\epsilon^{-2} \left(c_1 M_L^{\frac{\gamma-\beta}{2}}+c_2\sum_{p=0}^{L-1}M_{p}^\frac{\gamma_1-\beta_1}{2}\right)^2\simeq \epsilon^{-2}\left(c_1s^{\frac{(\gamma-\beta)}{2}L}+c_2\sum_{p=0}^{L-1}s^{\frac{(\gamma_1-\beta_1)}{2}p}\right)^2,
% % \end{align*}
% % %
% Since $K^*\simeq L$, we use the geometric sum approximation
% %
% \begin{equation}
% \label{eq:Geo_sum_for_s}
% \sum_{p=0}^L s^{\eta p}\simeq\left\{\begin{array}{ll}
% \frac{1}{1-s^{\eta}}, & \eta<0,\\
% |\log \epsilon|, & \eta = 0,\\
% \epsilon^{-\frac{\eta}{\alpha}}, & \eta>0,
% \end{array}
% \right.
% \end{equation}
% %
% Applying \eqref{eq:SLSGC_MLS_SpatialGridsNo} and condition (i), we substitute $s^{\frac{(\gamma-\beta)}{2}L}\simeq \epsilon^{\frac{\beta-\gamma}{2\alpha}}$. The remaining summation term $\sum_{p=0}^{K^*-2}s^{\frac{(\gamma_1-\beta_1)}{2}p}$ follows from \eqref{eq:Geo_sum_for_s}, yielding
% %
% \[
% \mathcal{W}^\text{MF} \simeq \left\{\begin{array}{ll}
% \epsilon^{-2}\left(c_1\epsilon^{\frac{\beta-\gamma}{2\alpha}}+c_2\right)^2, & \beta_1>\gamma_1,\\
% \epsilon^{-2}\left(c_1\epsilon^{\frac{\beta-\gamma}{2\alpha}}+c_2|\log\epsilon|\right)^2, & \beta_1=\gamma_1,\\
% \epsilon^{-2}\left(c_1\epsilon^{\frac{\beta-\gamma}{2\alpha}}+c_2\epsilon^{-\frac{\gamma_1-\beta_1}{2\alpha}}\right)^2, & \beta_1<\gamma_1.
% \end{array}
% \right.
% \]
% %
% When $c_1\gg c_2$ (or equivalently, $C_1$ is much larger than $C_i$ for $i\ge 2$), the dominant term in the expression is associated with $c_1$, allowing us to neglect the contributions from $c_2$, leading to the asymptotic result
% \[
% \mathcal{W}^\text{MF} \simeq \epsilon^{-2+\frac{\beta-\gamma}{\alpha}}.
% \]
% \end{proof}