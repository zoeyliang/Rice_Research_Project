\section{Model selection}\label{sec:Model_Selection}
%  \[
% p_{k>m} = \mathbb{P}(\rho_{1,k}^* > \rho_{1,m}^*) \approx \Phi\left( \frac{\widehat\rho_{1,k} - \widehat\rho_{1,m}}{\sqrt{\sigma_k^2 + \sigma_m^2}} \right)
% \]
% \[
% k \prec m \quad iff \quad p_{k>m} > \tau
% \]
Accurate correlation estimation enables robust model selection for multifidelity Monte Carlo estimation. When pilot sample size $\delta$ yields sufficiently small confidence intervals for $\widehat\rho_{1,k}$ such that intervals for different models are disjoint, these estimates reliably rank models by their true correlation strength $\rho_{1,k}$. This precise ranking is essential for identifying cost-efficient model combinations while satisfying the theoretical requirements of Theorem~\ref{thm:Sample_size_est}.


Given $K$ candidate models $\mathcal{S} = \{ u_{h,k} \}_{k=1}^K$, we seek an optimal subset $\mathcal{S}^* \subseteq \mathcal{S}$ that minimizes the total computational cost $\mathcal{W}^{\text{MF}}$ while satisfying two critical conditions \cite{PeWiGu:2016}. Firstly, the selected models must maintain the correlation monotonicity and cost-correlation ratio required by Theorem~\ref{thm:Sample_size_est}. Secondly, the subset must maximize the cost-efficiency metric $\xi(\boldsymbol{\rho},\mathcal{I}^*) = \sum_{k \in \mathcal{I}^*} \sqrt{C_k \Delta_k}$ that directly determines $\mathcal{W}^{\text{MF}}$. Here $\mathcal{I}^* \subseteq \{1,\ldots,K\}$ denotes the ordered indices of selected models in $\mathcal{S}^*$, always including the high-fidelity model ($1 \in \mathcal{I}^*$), with the size of $\mathcal{I}^*$ as $K^* = |\mathcal{I}^*| \leq K$.


An exhaustive search \cite{PeWiGu:2016} over all $2^K$ subsets becomes computationally prohibitive for $K \geq 9$, particularly in dynamic estimation contexts requiring frequent reselection. We therefore develop an efficient backtracking algorithm that incrementally constructs candidate subsets while pruning infeasible branches. The algorithm exploits three key properties: First, indices of models are pre-sorted by decreasing $|\widehat\rho_{1,k}|$, enabling sequential evaluation that respects correlation monotonicity. Second, Theorem~\ref{thm:Sample_size_est}'s cost-correlation ratio provides an early termination criterion when $C_{i-1}/C_i \leq \widehat\Delta_{i-1}/\widehat\Delta_i$. Third, the current optimal $\xi_{\text{min}}$ serves as an upper bound that prunes branches where partial sums already exceed this value. While worst-case complexity remains $\mathcal{O}(2^K)$ as the exhaustive approach, typical performance ranges from linear to quadratic due to aggressive pruning, with significant speedups when correlation decays rapidly (since $\Delta_{k-1}/\Delta_{k}$ is large, condition 2 easy to violate). Algorithm~\ref{algo:enhanced_mfmc_selection} returns the optimal indices $\mathcal{I}^*$, correlation coefficients $\boldsymbol{\rho}$, costs $\boldsymbol{C}$, minimal efficiency $\xi_{\text{min}}$, and control variate weights $\boldsymbol{\alpha}$.

 

% %
% \begin{equation*}\label{eq:Optimization_pb_model_selection}
%     \begin{array}{lll}
%     \displaystyle\min_{S^*} &\displaystyle \xi,\\
%        \text{s.t.} &\displaystyle |\rho_{1,1}|>\ldots>|\rho_{1,K^*}|,\\
%        &\displaystyle \frac{C_{i-1}}{C_i}>\frac{\rho_{1,i-1}^2-\rho_{1,i}^2}{\rho_{1,i}^2-\rho_{1,i+1}^2}, \quad i=1,\ldots,{K^*}, \quad \rho_{1,K^*+1}=0,\\
%     \end{array}
% \end{equation*}
% %

% \normalem
% \begin{algorithm}[!ht]
% \label{algo:MFMC_Algo_model_selection}
% \DontPrintSemicolon    
%    \KwIn{$K$ candidate models $\widehat  u_{h, k}$ with coefficients $\rho_{1,k}$, $\sigma_1$, $\sigma_k$ and cost per sample $C_k$.}\vspace{1ex}
    
%     \KwOut{Selected $K^*$ models $\widehat u_{h, i}$ in $\mathcal{S}^*$, with coefficients $\rho_{1,i}$, $\alpha_i$ and $C_i$ for each model $\widehat u_{h, i}$.}\vspace{1ex}
%     \hrule \vspace{1ex}

%    % Estimate $\rho_{1,k}$ and $C_k$ for each model $u_{h, k}$ using $N_0$ samples.
   
   
%    Sort $u_{h, k}$ by decreasing $\rho_{1,k}$ to create $\mathcal{S}=\{\widehat u_{h, k}\}_{k=1}^K$. 
   
%    Initialize $w^*=C_1$, $\mathcal{S}^*=\{\widehat u_{h, 1}\}$. Let $ \mathcal{\widehat S}$ be all $2^{K-1}$ ordered subsets of $\mathcal{S}$, each containing $\widehat u_{h, 1}$. 
%    % Set $ \mathcal{\widehat S}_1=\mathcal{S}^*$.

%     % $(2 \le j \le 2^{K-1})$
%     \For{each subset $\mathcal{\widehat S}_j$\,}{

%     {
%     \If{ condition $(ii)$ from Theorem \ref{thm:Sample_size_est} is satisfied}{
%     Compute the objective function value $w$ using \eqref{eq:MFMC_sampling_cost_efficiency}.
    
%     \If{$w<w^*$}{
%     {
%     Update $\mathcal{S}^* = \mathcal{\widehat S}_j$ and $w^* = w$.
%     }
%     } 
%     }
%     }
%     $j=j+1$.
%     }
%     Compute $\alpha_i$ for $\mathcal{S}^*$, $i=2,\dots, K^*$ by \eqref{eq:MFMC_coefficients}.
% \caption{Multi-fidelity Model Selection--\JLcolor{\cite[Algorithm~1]{PeWiGu:2016}}}
% \end{algorithm}
% \ULforem

\normalem
\begin{algorithm}[!ht]
\label{algo:enhanced_mfmc_selection}
\DontPrintSemicolon
\SetAlgoVlined
\SetKwProg{Fn}{Function}{}{}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{%
  Correlation vector $\widehat{\boldsymbol{\rho}} = [\widehat{\rho}_{1,k}]_{k=1}^K$, cost vector $\boldsymbol{C} = [C_k]_{k=1}^K$, standard deviation vector $\widehat{\boldsymbol{\sigma}} = [\widehat{\sigma}_k]_{k=1}^K$.
}
\Output{%
  Selected model index $\mathcal{I}$, minimal efficiency ratio $\widehat{\xi}_{\min}$.
}
\hrule

\Fn{[$\mathcal{I}$,  $\xi_{\min}$] = $\textsc{ModelSelectionBacktrack}(\boldsymbol{\rho}, \boldsymbol{C},K)$}{
  Sort models by non-increasing $|\rho_{1,k}|$ with permutation $r$ \;
  Relabel $\boldsymbol{\rho} \gets \boldsymbol{\rho}[r]$, $\boldsymbol{C} \gets \boldsymbol{C}[r]$ \;
  Initialize: $\text{idx}^{\text{cur}}$ $\gets [1]$, $\xi_{\min} \gets 1$, $\text{idx}^{\text{glb}}$ $\gets [1]$ \;
  $[\text{idx}^{\text{glb}}, \xi_{\text{min}}] = \textsc{Backtrack}(\text{idx}^{\text{cur}}, \xi_{\min}, 2,\boldsymbol{\rho}, \boldsymbol{C},K)$ \;
  $\mathcal{I}\gets r[\text{idx}^{\text{glb}}]$ \;
  % \Return{idx$\_$for$\_$model, $\xi_{\min}$}
}

\vspace{3mm}
\Fn{$[\text{idx}^{\text{glb}}, \xi_{\text{min}}] = \textsc{Backtrack}(\text{idx}^{\text{cur}}, \xi, k_{\text{next}}, \boldsymbol{\rho}, \boldsymbol{C},K)$}{
  \If{$\xi < \xi_{\min}$}{
    $\xi_{\min} \gets \xi$ \;
    $\text{idx}^{\text{glb}} \gets \text{idx}^{\text{cur}}$ \;
  }
  
  \If{$k_{\text{next}} > K$}{ 
    \Return\;
  }
  
  \For{$k \gets k_{\text{next}}$ \KwTo $K$}{ 
    $\rho_{\text{prev}} \gets \boldsymbol{\rho}[\text{last}(\text{idx}^{\text{cur}})]$ \tcc*{$\text{last}(\text{idx}^{\text{cur}})$: Last entry of $\text{idx}^{\text{cur}}$ }
    $C_{\text{prev}} \gets \boldsymbol{C}[\text{last}(\text{idx}^{\text{cur}})]$ \;
    
    \If{$\frac{C_{\text{prev}}}{C_k} \leq \frac{\rho_{\text{prev}}^2 - \rho_k^2}{\rho_k^2}$}{ 
      \textbf{continue} \tcc*{Condition (ii) violation}
    }
    
    Compute $\xi$ via \eqref{eq:MFMC_sampling_cost_efficiency} for indices  
      $[\text{idx}^{\text{cur}}, k]$ \;
    
    \If{$\xi \geq \xi_{\min}$ \textbf{or} $\xi > 1$}{
      \textbf{continue} \tcc*{No efficiency gain}
    }
    
    \textsc{Backtrack}($[\text{idx}^{\text{cur}}, k], \xi, k+1,\boldsymbol{\rho}, \boldsymbol{C},K)$ \;
  }
}

\vspace{3mm} 
$[\mathcal{I},  \widehat{\xi}_{\min}] \gets \textsc{ModelSelectionBacktrack}(\widehat{\boldsymbol{\rho}}, \boldsymbol{C},K)$ \;
% $\boldsymbol{\rho}^* \gets \boldsymbol{\rho}[\mathcal{I}]$, $\boldsymbol{C}^* \gets \boldsymbol{C}[\mathcal{I}]$, $\boldsymbol{\sigma}^* \gets \boldsymbol{\sigma}[\mathcal{I}]$, $\alpha_i^* \gets \dfrac{\rho_{1,i}^* \sigma_1^*}{\sigma_i^*}$ for $i = 2,\dots,|\mathcal{I}|$
% $\mathcal{S}^* \gets \{u_{h,i}\}_{i \in \mathcal{I}}$ \;

\caption{Multi-fidelity Model Selection with Backtracking Pruning}
\end{algorithm}
\ULforem





\normalem
\begin{algorithm}[!ht]
\label{algo:MFMC_Algo}
\DontPrintSemicolon

    
   \KwIn{Selected model set $\mathcal{S}^* = \{u_{h,k}\}_{k=1}^{K^*}$, parameters $\rho_{1,k}$, $\alpha_k$ and $C_k$ for each $u_{h, k}$,  tolerance $\epsilon$. }\vspace{1ex}
    
    \KwOut{Sample sizes $N_k$ for $K^*$ models, multifidelity estimate $A^{\text{MF}}$.}\vspace{1ex}
    \hrule \vspace{1ex}
    

    Compute the sample size $N_k$ for $1\leq k\leq K^*$ by \eqref{eq:MFMC_SampleSize} and generate i.i.d. $N_1$ and $N_k-N_{k-1}$ samples for $k=2,\ldots, K^*$.

    Evaluate $u_{h, 1}$ to obtain $u_1^{(i)}$ for $i = 1,\ldots,N_1$ and compute $A_{1,N_1}^{\text{MC}}$ by \eqref{eq:MC_estimator}.

    Initialize $A^{\text{MF}} \gets A_{1,N_1}^{\text{MC}}$.
    
    \For{$k = 2,\ldots,K^* $\,}{

    Evaluate $u_{h, k}$ to obtain $ u_k^{(i)}$ for $i = 1,\ldots,N_{k-1}$ and compute $A_{k,N_{k-1}}^{\text{MC}}$ by \eqref{eq:MC_estimator}.

    Evaluate $ u_{h, k}$ to obtain $ u_k^{(i)}$ for $i = 1,\ldots,N_k-N_{k-1}$ and compute $A_{k,N_k\backslash N_{k-1}}^{\text{MC}}$ by \eqref{eq:MC_estimator}.

    Update estimator $A^{\text{MF}}\gets A^{\text{MF}}+ \alpha_k\left(1-\frac{N_{k-1}}{N_k}\right)\left(A_{k,N_k\backslash N_{k-1}}^{\text{MC}}-A_{k,N_{k-1}}^{\text{MC}}\right)$.

    % Store $N_{k-1}$ and $N_{k}-N_{k-1}$ samples as $N_k$ samples.
    }

    % Compute $A^{\text{MF}}$ by \eqref{eq:MFMC_esti .mator_independent}.
    
\caption{Multifidelity Monte Carlo Simulation}
\end{algorithm}
\ULforem

% \normalem
% \begin{algorithm}[!ht]
% \label{algo:MFMC_Algo}
% \DontPrintSemicolon

    
%    \KwIn{Models $f_k$ in $\mathcal{S}^*$, parameters $\rho_k$, $\alpha_k$ and $C_k$ for each $f_k$ in $\mathcal{S}^*$,  tolerance $\epsilon$. }\vspace{1ex}
    
%     \KwOut{Sample sizes $N_k$ for $K^*$ models, expectation 
%     estimate $A^{\text{MFMC}}$.}\vspace{1ex}
%     \hrule \vspace{1ex}
%     Compute initial sample sizes $\boldsymbol{N}=[N_1,\ldots, N_{K^*}]$ using \eqref{eq:MFMC_SampleSize}. Set $\boldsymbol{N}_{\text{old}} = \boldsymbol{0}$ and $\boldsymbol{dN} = \boldsymbol{N}$. 
    
%     Initialize sample means $A_{1,N_1}^{\text{MC}}, A_{k,N_{k-1}}^{\text{MC}}, A_{k,N_k\backslash N_{k-1}}^{\text{MC}}=0. $
    
%     \While{$\sum_k dN_k>0$\,}{

%     Evaluate $dN_{1}$ samples for $f_1$ to obtain $f_1(\boldsymbol{\omega}^i)$. Update $A_{1,N_1}^{\text{MC}} = \frac{\boldsymbol{N}_{\text{old}}^1 A_{1,N_1}^{\text{MC}}+\sum_i f_1(\boldsymbol{\omega}^i)}{\boldsymbol{N}_{\text{old}}^1+dN_1}$ and $\sigma_1$.

%     Store $dN_1$ samples.
    
%     \For{$2\le k\le K^*$\,}{
    
%         % \For{$i = 1,\ldots,dN_k $\,}
%     % {
%     Evaluate previously stored $dN_{k-1}$ samples for $f_k$ to obtain $f_k(\boldsymbol{\omega}^i)$. Update $A_{k,N_{k-1}}^{\text{MC}} = \frac{\boldsymbol{N}_{\text{old}}^k A_{k,N_{k-1}}^{\text{MC}}+\sum_i f_k(\boldsymbol{\omega}^i)}{\boldsymbol{N}_{\text{old}}^k+dN_{k-1}}$. 
    
%     Collect new $dN_{k}-dN_{k-1}$ samples. Evaluate $f_k$ to obtain $f_k(\boldsymbol{\omega}^i)$. Update $A_{k,N_k\backslash N_{k-1}}^{\text{MC}} = \frac{\boldsymbol{N}_{\text{old}}^k A_{k,N_k\backslash N_{k-1}}^{\text{MC}}+\sum_i f_k(\boldsymbol{\omega}^i)}{\boldsymbol{N}_{\text{old}}^k+dN_{k}-dN_{k-1}}$. 

    
%     Compute $\sigma_k, \rho_{1,k}$.

%     Store $dN_{k-1}$ and $dN_{k}-dN_{k-1}$ samples as $dN_k$ samples.
    
%     \If{Condition (i) \& (ii) in Theorem \ref{thm:Sample_size_est} is not satisfied \,}{
%     Reselect models via Algorithm \ref{algo:MFMC_Algo_model_selection} with a larger sample size and restart.

%     Break. 
%     }

%     }
    
    
    
%     \vspace{4mm}
%     $\boldsymbol{N}_{\text{old}} \leftarrow \boldsymbol{N}$
    
%     Update $\alpha_k$ and the sample size $\boldsymbol{N}$ by \eqref{eq:MFMC_coefficients} 
%  and \eqref{eq:MFMC_SampleSize}.

%     $\boldsymbol{dN} \leftarrow \max \left\{\boldsymbol N-\boldsymbol N_{\text{old}}, \boldsymbol{0}\right\}.$

    
%     }
%     Compute $A^{\text{MFMC}}$ using $A_{1,N_1}^{\text{MC}}, A_{k,N_{k-1}}^{\text{MC}}, A_{k,N_k\backslash N_{k-1}}^{\text{MC}}$ and $\alpha_k$ from step 4, 7, 8, 15, by \eqref{eq:MFMC_estimator_independent}.
% \caption{Multi-fidelity Monte Carlo}
% \end{algorithm}
% \ULforem


% \begin{theorem}
% \label{thm:Sampling_cost_est}
% Let $f_k$ be $K$ models that satisfy the following conditions
% %
% \begin{alignat*}{8}
%     &(i)\;\; |\rho_{1,1}|>\ldots>|\rho_{1,K}|& \qquad \qquad
%     &(ii)\;\; \frac{C_{k-1}}{C_k}>\frac{\rho_{1,k-1}^2-\rho_{1,k}^2}{\rho_{1,k}^2-\rho_{1,k+1}^2},\;\;k=2,\ldots,K.
% \end{alignat*}
% %
% Suppose there exists $0<s<q<1$ such that 
% $C_k = c_s s^{k}$, $\rho_{1,k}^2 = q^{ k-1}$, then 
% \begin{equation*}
%     \mathcal{W}_\text{MFMC} = 
% \end{equation*}

% \end{theorem}
% \begin{proof}
% Since $q>s$, condition (ii) is satisfied.
% \begin{align*}
% \rho_{1,k}^2 - \rho_{1,k+1}^2&=q^k\left(\frac1 q-1\right),\quad \rho_{1,k-1}^2 - \rho_{1,k}^2=q^k\frac 1 q\left(\frac1 q-1\right)\\
%     \mathcal{W}_\text{MFMC} &= \frac{\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2\epsilon^2}\sum_{k=1}^K\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}\sum_{k=1}^K\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2,\\
%     &=\frac{\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2\epsilon^2} \sum_{k=1}^K\sqrt{q^{k}s^{ k}}\left(\sqrt{\frac{s(1-q)}{1-\rho_{1,2}^2}} + \sum_{k=2}^K\left(\sqrt{\frac{s^{k}}{q^{ k}}} - \sqrt{\frac{q s^{ k}}{s q^{ k}}}\right)q^{k} + \left(\sqrt{\frac{s^{ K}(1-q)}{q^{K}}}-\sqrt{\frac{q s^{ K}}{s q^{K}}}\right)q^{K}\right)\\
%     &\propto \frac{1}{\epsilon^2} \sum_{k=1}^K\left(q^{\frac{1}{2}}s^{\frac{1}{2}}\right)^k
% \end{align*}
    
% \end{proof}



\subsection{Cost of multi-fidelity Monte Carlo with stochastic collocation}
\label{sec:Cost_MFMC_with_SC}
To ensure that the discretization error meets the required tolerance $\theta \epsilon^2$ for MFMC, we consider a geometrically uniformly refined hierarchy of spatial discretizations $\{\mathcal{T}_\ell\}_{0\le \ell \le L_{\max}}$ with the corresponding spatial grid sizes $\{M_\ell\}_{0\le \ell \le L_{\max}}$ satisfying 

%
\begin{equation}
\label{eq:MeshGrowth}
M_\ell = s M_{\ell-1} \qquad \text{ for }\quad  s>1,
\end{equation}
%
where $L_{\max}$  represents computationally feasible mesh refinement limit of spatial level. Given a target accuracy $\epsilon$, the required spatial resolution level $L \leq L_{\max}$ and grid size $M_L$ \eqref{eq:SLSGC_MLS_SpatialGridsNo} must satisfy the bias constraint in the nMSE splitting. Within this framework, we consider a set of $K = L+1$ computational models before model selection: the high-fidelity model $u_{L,1}$ at level $L$, and low-fidelity counterparts $\{u_{\ell(k),k}\}_{k=2}^K$ constructed via sparse grid stochastic collocation on coarser grids $\mathcal{T}_{\ell(k)}$ with $\ell(k) = L + 1 - k$ (thus $\ell(k) < L$). Through optimal model selection, we identify a subset $\{u_{\ell(k),k}\}_{k \in \mathcal{I}^*}$ of the low-fidelity models. The following theorem establishes the fundamental asymptotic computational cost complexity for the multi-fidelity approach.


%
\begin{theorem}[Asymptotic Cost for MFMC-SC Estimator]\label{thm:Sample_cost_est}
Consider a hierarchy of models $\{u_{\ell(k),k}\}_{k=1}^{K}$ where $u_{L,1}$ is the high-fidelity model at spatial discretization level $L$ with $M_L$ degrees of freedom, and low-fidelity models $\{u_{\ell(k),k}\}_{k=2}^K$ are constructed via sparse grid stochastic collocation on coarser grids $\mathcal{T}_{\ell(k)}$ with $\ell(k) = L + 1 - k$. Let $\mathcal{I}^* = \{i_k \mid k = 1, \dots, K^*\}$ index optimally ordered models after model selection with correlations $\rho_{1,i_k}$ to $u_{L,1}$ and costs $C_{i_k}$. For the family of high-fidelity models $\{u_{L,1}\}_{L\le L_{\max}}$ with $M_L$ degrees of freedom, assume  there exist positive constants $\alpha, \beta, \gamma$ such that
%
\begin{alignat*}{8}
    \text{(i)}\quad & \left\Vert \mathbb{E}[u] - \mathbb{E}[u_{L,1}] \right\Vert_Z \simeq M_L^{-\alpha}, \qquad
    \text{(ii)}\quad & 1 - \rho_{1,i_2}^2 \simeq M_L^{-\beta}, \qquad
    \text{(iii)}\quad & C_1 \simeq M_L^{\gamma},
\end{alignat*}
%
For the low-fidelity models, assume there exist positive constants $\beta_1, \gamma_1$ such that for each $k=2,\dots,K^*$
%
\begin{alignat*}{8}
    % &(i)\;\; |\rho_{1,1}|>\ldots>|\rho_{1,K}|,& \quad \quad
    % &(ii)\;\; \frac{C_{k-1}}{C_k}>\frac{\rho_{1,k-1}^2-\rho_{1,k}^2}{\rho_{1,k}^2-\rho_{1,k+1}^2},\;\;k=2,\ldots,K, \quad \rho_{1,K+1}=0,\\
    % &(iii)\;\; 1-\rho_{1,i_2}^2 \simeq M_{L}^{-\beta},
    % \qquad
    &(iv)\quad \rho_{1,i_k}^2-\rho_{1,i_{k+1}}^2 \simeq M_{\ell(i_k)}^{-\beta_1},
    \qquad
&(v)\quad C_{i_k} \simeq M_{\ell(i_k)}^{\gamma_1},
\end{alignat*}
%
with $\rho_{1,i_{K^*+1}}^2 := 0$. Then for any $\epsilon \in (0,e^{-1})$, there exist $L \leq L_{\max}$ and sample sizes $\{N_{i_k}\}_{i_k\in \mathcal{I}^*}$ such that the multifidelity estimator $A^{\mathrm{MF}}$ achieves nMSE  with
%
\[
\frac{\left\Vert \mathbb{E}[u] - A^{\mathrm{MF}} \right\Vert_{L^2(\boldsymbol{W},Z)}}{\left\Vert \mathbb{E}[u] \right\Vert_{L^2(\boldsymbol{W},Z)}} < \epsilon,
\]
%
with computational cost and the term $\Phi(\epsilon)$ depends on the low-fidelity scaling regime is bounded by
%
\begin{equation}\label{eq:lfm_scaling}
\mathcal{W}^{\mathrm{MF}} \lesssim \epsilon^{-2} \left( \epsilon^{-\frac{\gamma-\beta}{2\alpha}} + \Phi(\epsilon) \right)^2,\qquad 
\Phi(\epsilon) \lesssim \begin{cases} 
1 & \beta_1 > \gamma_1, \\
|\log \epsilon| & \beta_1 = \gamma_1, \\
\epsilon^{-\frac{\gamma_1 - \beta_1}{2\alpha}} & \beta_1 < \gamma_1.
\end{cases}
\end{equation}
%
Moreover, when the constant factor $c_1$ for the high-fidelity term satisfies $c_1 \gg c_2$ (where $c_2$ is the aggregate constant for low-fidelity terms), indicating that the high-fidelity cost dominates the sum of all low-fidelity costs, the cost simplifies to
%
\[
\mathcal{W}^{\mathrm{MF}} \simeq \epsilon^{-2 - \frac{\gamma-\beta}{\alpha}}.
\]
%
This dominance occurs independently of the asymptotic regime when $c_1$ is sufficiently larger than $c_2$.
\end{theorem}



% The cancellation of the two successive terms in the ratio representation on both sides of \eqref{eq:Theorem_cond_ii} indicates that 
% \[
% \frac{B_1}{C_1^2}C_k^2\le B_k\le \frac{B_K}{C_K^2}C_k^2, \quad k=1,\ldots,K.
% \]
% This indicates that
% \[
% \sqrt{\frac{1-\rho_{1,2}^2}{C_1}}\sum_{k=1}^K C_k\le \sum_{k=1}^K\sqrt{B_k}\le \sqrt{\frac{\rho_{1,K}^2}{C_K}}\sum_{k=1}^K C_k, \quad \text{or} \quad \sum_{k=1}^K
% \sqrt{B_k}\simeq \sum_{k=1}^K C_k,
% \]
% where $A\simeq B$ represents $a_1 B\le A\le a_2 B$ for positive $A$ and $B$, with constants $a_1, a_2$ independent of sample size $N_k$ but depends on model number $K$. This indicates that the sum in \eqref{eq:sampling_cost_bound} behaves in a similar style as $\sum_{k=1}^K C_k$. If $\epsilon$ is chosen sufficiently small, we can ignore the small term $\sum_{k=1}^K C_k$ in  \eqref{eq:sampling_cost_bound}. 





% Since $K$ is independent of $\epsilon$,  the total sampling cost behaves like $\epsilon^{-2}$. 

% Moreover, the inequality \eqref{eq:Theorem_cond_ii} also indicates that
% \[
% 0<\frac{C_L}{\sqrt{B_L}}\le \frac{C_k}{\sqrt{B_k}}<\frac{C_{k-1}}{\sqrt{B_{k-1}}}\le\frac{C_1}{\sqrt{B_1}},\quad k=2,\dots, L
% \]
% Therefore, the sequence $\frac{C_k}{\sqrt{B_k}} - \frac{C_{k-1}}{\sqrt{B_{k-1}}}\in (\frac{C_L}{\sqrt{B_L}} - \frac{C_{1}}{\sqrt{B_{1}}},0)$  is bounded. 

% Note that
% \[
% H_1:=\sum_{k=1}^L\sqrt{B_k}, \quad \sqrt{\frac{1-\rho_{1,2}^2}{C_1}}\sum_{k=1}^L C_k\le \sum_{k=1}^L\sqrt{B_k}\le \sqrt{\frac{\rho_{1,L}^2}{C_L}}\sum_{k=1}^L C_k, \quad H_1\uparrow \sqrt{\frac{\rho_{1,L}^2}{C_L}}\sum_{k=1}^L C_k \;\;\text{as}\;\; L\rightarrow \infty
% \]
% \[
% H_2:=\sum_{j=1}^L\left(\frac{C_j}{\sqrt{B_j}} - \frac{C_{j-1}}{\sqrt{B_{j-1}}}\right)\rho_{1,j}^2, \quad  0<H_2\le \sqrt{\frac{C_1}{1-\rho_{1,2}^2}}, \quad H_2\downarrow 0 \;\;\text{as}\;\; L\rightarrow \infty.
% \]