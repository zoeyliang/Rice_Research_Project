% ====================================================
\section{Multi-fidelity Monte Carlo}\label{sec:MFMC}
% ====================================================
%
The standard Monte Carlo finite element estimator is often computationally prohibitive due to the large number of high-fidelity samples required to achieve a prescribed accuracy. To mitigate this cost, multilevel Monte Carlo (MLMC) methods \cite{Gi:2008,Gi:2015} and multi-fidelity Monte Carlo (MFMC) approaches \cite{PeWiGu:2016} have been developed. MFMC,  in particular, considers surrogate models of varying fidelities through the framework of {\it approximate control variate Monte Carlo} \cite{GoGeElJa:2020}, which exploits statistical 
correlations between models of varying fidelity to construct a recursive, variance-reducing estimator. When the low-fidelity models are strongly correlated with the high-fidelity counterpart, MFMC yields substantial variance reduction. Moreover, if the expected value of the low-fidelity model can be estimated at relatively low cost, the control variate correction becomes both efficient and effective. This strategy preserves estimator unbiasedness while significantly improving computational efficiency through sample reuse and optimal weighting across model hierarchies. 



Note that both MFMC and MLMC share the goal of variance reduction, but differ in structure and sampling \cite{ArGuMoWi:2025,PeGuWi:2018}. In MLMC, corrections between fine discretization levels are accumulated to the coarse, with independent samples and decreasing sample counts at higher resolutions. MFMC, by contrast, adds corrections of low-fidelity models to the high-fidelity model and incorporates increasing numbers of inexpensive, low-fidelity samples. A key distinction is MFMCâ€™s reuse of samples across fidelity levels, which avoids the cost of generating independent samples at each level and further improves efficiency. Overall, MFMC offers a flexible and scalable alternative to standard MC and MLMC, particularly suited for problems where high-fidelity models are accurate but computationally intensive.


% The control variate approach  reduces the variance of MC estimator by considering a  new random variable $u_{h,2}\left(\cdot, \boldsymbol{\omega}\right)$ such that it correlates with $u_{h}\left(\cdot, \boldsymbol{\omega}\right)$ but cheaper than $u_{h}\left(\cdot, \boldsymbol{\omega}\right)$ to evaluate, and whose expectation is either known or can be approximated with relatively small cost. Then we construct a new random variable
% %
% \[
% u^*=u_{h}\left(\cdot, \boldsymbol{\omega}\right) + \alpha\left (u_{h,2}\left(\cdot, \boldsymbol{\omega}\right)  - \mu_2\right)
% \]
% %
% If either $u_{h,2}\left(\cdot, \boldsymbol{\omega}\right)$ is poorly correlated with $u_{h}\left(\cdot, \boldsymbol{\omega}\right)$ or the cost of estimating $\mu_2$ is large, the cost improvement of control variate is not obvious compared to the standard MC. The two-model control variate estimator using Monte Carlo sampling is
% %
% \begin{equation*}
% A^{\text{TF}}  =  \frac{1}{N_1}\sum_{i=1}^{N_1} u_{h}\left(\cdot, \boldsymbol{\omega}^{(i)}\right) + \alpha\left (\frac{1}{N_1}\sum_{i=1}^{N_1} u_{h,2}\left(\cdot, \boldsymbol{\omega}^{(i)}\right)  - \mu_2\right),
% \end{equation*}
% %
% where $u_{h,2}$ denotes a low-fidelity model and $\mu_2 = \mathbb{E}(u_{h,2})$ is its exact control variate mean and we can show that $\mathbb{E}(A^{\text{TF}}) = \mathbb{E}(u_h)$. The variance of this estimator is minimized by choosing $\alpha = -\frac{\text{Cov}(u_h, u_{h,2})}{\sigma_{u_{h,2}}^2}  = -\rho \sigma_{u_h}/\sigma_{u_{h,2}}$, where $\rho$ is the Pearson correlation coefficient between $u_h$ and $u_{h,2}$. In practice, this is achieved by evaluating both models using the same random samples $\boldsymbol{\omega}^{(i)}$, which maximizes correlation through sample reuse. With this choice, the variance becomes $\mathbb{V}(A^{\text{TF}}) = (1-\rho^2)\mathbb{V}(u_h)$. The {\it approximate control variate Monte Carlo} \cite{GoGeElJa:2020} handles the issue when the statistic of the lower fidelity model is unknown and the cost of the lower fidelity model is non trivial. The effectiveness of the control variate approach therefore depends on selecting a low-fidelity model that is strongly correlated with the high-fidelity model. It is clear that if $u_{h}$ and $u_{h,2}$ are highly correlated, then $\mathbb{V}(A^{\text{TF}})$ is much smaller than $\mathbb{V}(u_h)$ and MC simulation of $\mathbb{E}(A^{\text{TF}})$ converges much faster than that of $\mathbb{E}(u_h)$.



% Thus by introducing a control variate $\alpha (u_{h,2}-\mu_2)$, we can reduce the variance. Note that both $u_{h}^{(i)}$ and $u_{h,2}^{(i)}$ can be correlated or linearly independent (uncorrelated). To make the variance as small as possible, we want $\rho$ to be as large as possible, so it's better to make the two quantities correlated. If $u_{h,2}$ is a low fidelity model compared to $u_{h}$, this indicates that $u_{h,2}$ should reuse the samples of  $u_{h}$. 


We now briefly review the core principles underlying multi-fidelity Monte Carlo, drawing on the framework in \cite{PeWiGu:2016}.  The MFMC framework combines a high-fidelity model $u_{h,1} = u_h$ with a series of low-fidelity models $u_{h,k}: \Omega \rightarrow U$ for $k=2,\ldots,K$.  The high-fidelity model yields accurate but computationally expensive evaluations, whereas the low-fidelity models offer cheaper approximations with reduced accuracy. MFMC optimally allocates computational resources across the fidelity levels to reduce the overall estimator variance, reducing the reliance on costly high-fidelity samples while preserving estimator accuracy and robustness.


For convenience, we may use $u$ and $u_{h,k}$ to refer to the exact $u(\cdot,\boldsymbol \omega)$ and discrete $u_{h,k}(\cdot,\boldsymbol \omega)$ solutions with random variable $\boldsymbol \omega$ in the later context. For each pair of $u_{h,k}(\cdot,\boldsymbol \omega)$ and $u_{h,j}(\cdot,\boldsymbol \omega)$, we define the variance and the Pearson correlation coefficient as
%
\begin{equation*}
    \sigma_k^2 = \mathbb{V}\left[u_{h,k}(\cdot,\boldsymbol \omega)\right],\qquad \rho_{k,j} = \frac{\text{Cov}\left[ u_{h,k}(\cdot,\boldsymbol \omega), u_{h,j}(\cdot,\boldsymbol \omega)\right]}{\sigma_k\sigma_j}, \quad k,j=1,\dots, K,
\end{equation*}
%
where the covariance is $\text{Cov}[u_{h,k}, u_{h,j}] := \mathbb{E}[\langle u_{h,k} - \mathbb{E}[u_{h,k}], u_{h,j} - \mathbb{E}[u_{h,j}]\rangle_U]$ and, by definition, $\rho_{k,k}=1$. The multi-fidelity Monte Carlo finite element estimator $A^{\text{MF}}$ augments a high-fidelity Monte Carlo estimate with control variate corrections from low-fidelity models
%
\begin{equation}\label{eq:MFMC_estimator}
    A^{\text{MF}} := A^{\text{MC}}_{1,N_1} + \sum_{k=2}^K \alpha_k\left(\overline{A}_{k,N_k} - \overline{A}_{k,N_{k-1}} \right),
\end{equation}
%
where $A^{\text{MC}}_{1,N_1}$ is the standard Monte Carlo estimator using $N_1$ samples of the high-fidelity model, $\alpha_k\in \mathbb{R}$ are control variate weights, and $\overline{A}_{k,N_k}$ denotes the sample average of model $u_{h,k}$ over $N_k$ samples. The control variate construction requires the nesting condition $N_{k-1}\le N_k$ for $k=2,\ldots, K$, as $\overline{A}_{k,N_{k}}$ reuses all $N_{k-1}$ samples from $\overline{A}_{k,N_{k-1}}$, possibly supplemented by additional $N_{k} - N_{k-1}$ samples. This reuse introduces statistical dependence between $\overline{A}_{k,N_{k-1}}$ and $\overline{A}_{k,N_{k}}$. To eliminate sampling dependence in the correction terms, we partition the $N_k$ samples into two disjoint sets of sizes  $N_{k-1}$ and $N_k - N_{k-1}$. This allows us to reformulate the multi-fidelity estimator \eqref{eq:MFMC_estimator} as
%
\begin{equation}\label{eq:MFMC_estimator_independent}
    A^{\text{MF}} = A^{\text{MC}}_{1,N_1} +  \sum_{k=2}^K \alpha_k\left(1-\frac{N_{k-1}}{N_{k}}\right)\left(A_{k,N_k\backslash N_{k-1}}^{\text{MC}}-A_{k,N_{k-1}}^{\text{MC}}\right),
\end{equation}
%
where $A^{\text{MC}}_{k,N_k \backslash N_{k-1}}$ denotes the Monte Carlo average over the $N_k - N_{k-1}$ samples not included in $A^{\text{MC}}_{k,N_{k-1}}$, and is defined to be zero whenever $N_k=N_{k-1}$. In this formulation, the two terms in each correction are evaluated on independent sample sets, which simplifies variance analysis. We can now express the MFMC estimator in compact form
%
\begin{equation*}\label{eq:MFMC_estimator_Correction}
A^{\text{MF}} = Y_1 + \sum_{k=2}^K \alpha_k Y_k,
\end{equation*}
%
where the correction terms $Y_k$ are defined as
%
\begin{equation} \label{eq:MFMC_Yk}
Y_1 :=A^{\text{MC}}_{1,N_1},\qquad Y_k:=\overline{A}_{k,N_k} - \overline{A}_{k,N_{k-1}}=\left(1-\frac{N_{k-1}}{N_{k}}\right)\left(A_{k,N_k\backslash N_{k-1}}^{\text{MC}}-A_{k,N_{k-1}}^{\text{MC}}\right), \quad k=2\ldots, K.
\end{equation}
%
Since $Y_k$ is defined as the difference between two independent Monte Carlo estimators of the same model, it is unbiased: $\mathbb{E}[Y_k] = 0$ for $k\ge 2$. Consequently, the MFMC estimator is itself unbiased, satisfying $\mathbb{E}[A^{\text{MF}}] = \mathbb{E}[u_{h,1}]$. The variances of the correction terms $Y_k$ are
%
\begin{equation}\label{eq:Var_Yk}
    \mathbb{V}\left[Y_1\right] = \frac{\sigma_1^2}{N_1}, \quad \mathbb{V}\left[Y_k\right] = \left(1-\frac{N_{k-1}}{N_{k}}\right)^2\left(\frac{\sigma_k^2}{N_{k-1}}+\frac{\sigma_k^2}{N_k-N_{k-1}}\right) = \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\sigma_k^2.
\end{equation}
%
Although $Y_k$ and $Y_j$ for $2\le k<j \le K$ share overlapping sample sets and are therefore statistically dependent, they remain uncorrelated, as established in Lemma~\ref{lemma:Y_k_Y_j} in the appendix. However, each correction term $Y_k$ with $k\ge 2$ is correlated with the high-fidelity estimator $Y_1$. Using the covariance identity derived from \cite[Lemma~3.2]{PeWiGu:2016}, yields
%
\begin{equation}\label{eq:Cov_Yk}
% \text{Cov}(Y_k,Y_j) =0,\quad \text{for } \;2\le k<j \le K,\qquad 
\text{Cov}[Y_1,Y_k] = - \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\rho_{1,k}\sigma_1\sigma_k, \quad \text{for } \; k\ge 2.
\end{equation}
%
Combining \eqref{eq:Var_Yk} and \eqref{eq:Cov_Yk}, the total variance of the MFMC estimator is 
%
\begin{align}
    \nonumber
    \mathbb{V}\left[A^{\text{MF}}\right] &= \mathbb{V}\left[Y_1\right] + \mathbb{V}\left[\sum_{k=2}^K \alpha_kY_k\right]+2\;\text{Cov}\left[Y_1,\sum_{k=2}^K \alpha_k Y_k \right],\\
    \nonumber
    &=\mathbb{V}\left[Y_1\right] + \sum_{k=2}^K \alpha_k^2 \mathbb{V}\left[Y_k\right]+2\sum_{2\le k<j\le K} \alpha_k\alpha_j\; \text{Cov}[Y_k,Y_j] +2\sum_{k=2}^K \alpha_k\;\text{Cov}\left[Y_1, Y_k\right],\\
    % \nonumber
    % &=\mathbb{V}\left(Y_1\right) + \sum_{k=2}^K \alpha_k^2 \mathbb{V}\left(Y_k\right) +2\sum_{k=2}^K \alpha_k\;\text{Cov}\left(Y_1, Y_k\right),\\
    \label{eq:MFMC_variance}
    &=\frac{\sigma_1^2}{N_1} + \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\left(\alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k\right).
\end{align}
%
The normalized mean square error of the multi-fidelity Monte Carlo estimator, $\mathcal{E}_{A^{\text{MF}}}^2$, quantifies its accuracy and is decomposed into two components -- the bias error $\mathcal{E}_{\text{Bias}}^2$ and the statistical error $\mathcal{E}_{\text{Stat}}^2$, the decomposition is written as 
%
\[
\mathcal{E}_{A^{\text{MF}}}^2= \frac{\left\Vert\mathbb{E}(u)-\mathbb{E}(A^{\text{MF}}) \right\Vert_{U}^2+\mathbb E\left[\left\Vert\mathbb{E}(A^{\text{MF}})-A^{\text{MF}} \right\Vert_{U}^2\right]}{\left\Vert\mathbb{E}(u) \right\Vert_{U}^2} =\frac{\left\Vert\mathbb{E}(u)-\mathbb{E}(A^{\text{MF}}) \right\Vert_{U}^2}{\left\Vert\mathbb{E}(u) \right\Vert_{U}^2}+ \frac{\mathbb{V}\left[A^{\text{MF}}\right]}{\left\Vert\mathbb{E}(u) \right\Vert_{U}^2}=\mathcal{E}_{\text{Bias}}^2 + \mathcal{E}_{\text{Stat}}^2,
\]
%
where the variance term $\mathbb{V}\left(A^{\text{MF}}\right)$  can be explicitly expressed using \eqref{eq:MFMC_variance}. A splitting ratio $\theta$ is introduced as before to balance the contributions between these two components. The spatial resolution required to achieve the biased tolerance $\theta \epsilon^2$ is determined by estimating the number of spatial grid points $M_L$ at refinement level $L$, given by
%
\begin{equation}
    \label{eq:SLSGC_MLS_SpatialGridsNo}
    M_L = M_0s^{-L} \ge \left(\frac{\theta\epsilon}{C_m}\right)^{-\frac 1 {\alpha}} \qquad \text{ and } \qquad     L = \left\lceil \frac{1}{\alpha}\log_s \left(\frac{C_m M_0^\alpha}{\theta\epsilon}\right) \right\rceil,
\end{equation}
%
where $M_0$ is the number of grid points at the coarsest level, $s>1$ is the spatial refinement factor, $\alpha$ represents the convergence rate of the spatial discretization, and $C_m$ is a constant characterizing the discretization scheme. To determine the optimal sample sizes $N_k$ and control variate weights $\alpha_k$ in the MFMC estimator \eqref{eq:MFMC_estimator_independent}, we express the total computational cost for the MFMC estimator
%
\[
\mathcal{W}^{\text{MF}} = \sum_{k=1}^K C_kN_k,
\]
%
where $C_k$ is the cost of generating a single sample of model $u_{h,k}$, and $N_k$ is the corresponding sample count. Unlike previous formulations \cite{PeWiGu:2016} that derive sample sizes based on a fixed computational budget, our approach directly expresses the sample sizes and computational resources in terms of the desired accuracy $\epsilon$. This formulation offers greater flexibility in applications where accuracy targets are more relevant than rigid cost constraints. We formulate an optimization problem to determine the optimal sample sizes $N_k$ and weights $\alpha_k$ by minimizing the total sampling cost $\mathcal{W}^{\text{MF}}$, subject to three constraints. First, the normalized statistical error $\mathcal{E}_{\text{Stat}}^2$ enforces the desired estimator accuracy $(1-\theta)\epsilon^2$. Second,  the monotonicity constraints $N_{k-1}\le N_k$ for $k=2,\ldots, K$ ensures consistent sample reuse across fidelity levels. Third, all sample sizes must be non-negative. This leads to the following constrained optimization problem
%
\begin{equation}\label{eq:Optimization_pb_sample_size}
    \begin{array}{ll}
    \min \limits_{\begin{array}{c}\scriptstyle N_1,\ldots, N_K\in \mathbb{R} \\[-4pt]
\scriptstyle \alpha_2,\ldots,\alpha_K\in \mathbb{R}
\end{array}} &\displaystyle\sum\limits_{k=1}^K C_kN_k,\\
       \;\,\text{subject to} &\mathbb{V}\left[A^{\text{MF}}\right]- \epsilon_{\text{tar}}^2 = 0,\\[2pt]
       &\displaystyle -N_1\le 0,\quad \displaystyle N_{k-1}-N_k\le 0, \;\; k=2\ldots,K.
    \end{array}
\end{equation}
%
The solution to this problem, which yields explicit expressions for the optimal real-valued sample sizes and weights, is presented in Theorem~\ref{thm:Sample_size_est}. The proof of Theorem~\ref{thm:Sample_size_est} is provided in the appendix.




%
\begin{theorem}
\label{thm:Sample_size_est}
Consider a set of $K$ models, $u_k$ for $k=1,\ldots,K$, where each model is characterized by the standard deviation $\sigma_k$ of its output, the correlation coefficient $\rho_{1,k}$ between the highest-fidelity model $u_{h,1}$ and the $k$-th low-fidelity model, and the computational cost per sample $C_k$. Assume the following conditions hold
%
\begin{alignat*}{5}
    &(i)\;\; |\rho_{1,1}|>\ldots>|\rho_{1,K}|,& \qquad \qquad
    &(ii)\;\; \frac{C_{k-1}}{C_k}>\frac{\rho_{1,k-1}^2-\rho_{1,k}^2}{\rho_{1,k}^2-\rho_{1,k+1}^2},\quad \quad k=2,\ldots,K.
\end{alignat*}
%
Under these assumptions, the optimal sample sizes $N_k^*$ and weights $\alpha_k^*$, for $k=1,\ldots, K$, solving the optimization problem \eqref{eq:Optimization_pb_sample_size} are
%
\begin{align}
    % \label{eq:MFMC_coefficients}
    % &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\\
    \label{eq:MFMC_SampleSize}
    &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\qquad \;N_k^*=\frac{\sigma_1^2}{\epsilon_\text{tar}^2}\sqrt{\frac{\rho_{1,k}^2-\rho_{1,k+1}^2}{C_k}}\sum_{j=1}^K\sqrt{C_j\left(\rho_{1,j}^2-\rho_{1,j+1}^2\right)}, \quad \rho_{1,K+1}=0.
\end{align}
%
Using the optimal weights $\alpha_k^*$ and sample size estimates $N_k^*$, the variance of the MFMC estimator in \eqref{eq:MFMC_variance} can be expressed as
%
\begin{equation}
\label{eq:MFMC_variance_optimal}
\mathbb{V}\left[A^{\text{MF}}\right] =
% \frac{\sigma_1^2}{N_1^*} - \sum_{k=2}^K \left(\frac{1}{N_{k-1}^*} - \frac{1}{N_k^*}\right)\rho_{1,k}^2\sigma_1^2=
\sigma_1^2 \sum_{k=1}^K\frac{\rho_{1,k}^2 - \rho_{1,k+1}^2}{N_k^*},\quad \text{with}\;\;\rho_{K+1}=0.
\end{equation}
%
and the corresponding total sampling cost $\mathcal{W}^\text{MF}$ is
%
\begin{equation}\label{eq:MFMC_sampling_cost}
    \mathcal{W}^\text{MF} = \sum_{k=1}^K C_k N_k^* = \frac{\sigma_1^2}{\epsilon_{\text{tar}}^2}\left(\sum_{k=1}^K\sqrt{C_k\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)}\right)^2,\quad \text{with}\;\;\rho_{K+1}=0.
\end{equation}
%
\end{theorem}


In practice, however, the correlation parameters $\rho_{1,k}$ and the per-sample cost $C_k$ are typically unknown a priori and must be estimated from the sample data. Moreover, since the optimal sample sizes $N_k^*$ are real-valued, they must be rounded to integers for implementation. Departing from the rounding strategy in \cite{GrGuJuWa:2023, PeWiGu:2016}, which applies the floor function for values exceeding one and ceiling otherwise, we adopt a simpler and uniform approach: all sample sizes are rounded up to the nearest integer using the ceiling function, i.e., $\lceil N_k^* \rceil$. \JLcolor{An assumption for the rounding scheme to hold is that $\sum_{k=1}^K C_kN_k^*\ge \sum_{k=1}^K C_k$, which avoid that degenearte case when all real-valued sample size estimation are all below 1. }This rounding scheme guarantees that the realized variance remains below the target variance bound, as rounding up results in slightly smaller variance than predicted by \eqref{eq:MFMC_variance_optimal}. As a result, the total sampling cost under this rounding satisfies the inequality
%
\begin{equation}\label{eq:sampling_cost_bound}
    \sum_{k=1}^K C_k N_k^*\le \sum_{k=1}^K C_k \left\lceil N_k^*\right\rceil<\sum_{k=1}^K C_k N_k^* + \sum_{k=1}^K C_k,
\end{equation}
%
where the additional term $\sum_{k=1}^K C_k$ accounts for the fact that $N_k^*\le \lceil N_k^*\rceil< N_k^*+1$. This naturally leads to the consideration of whether the additive overhead introduced by rounding distorts the asymptotic behavior of the total sampling cost, particularly in regimes where some $N_k^*$ fall below one. To analyze this, define the auxiliary quantity $B_k = C_k(\rho_{1,k}^2 - \rho_{1,k+1}^2)$ in \eqref{eq:MFMC_sampling_cost} for all $k=1,\dots, K$, with the convention $\rho_{K+1}=0$. 
% Substituting into the sampling cost expression,  \eqref{eq:MFMC_sampling_cost} becomes
% %
% \begin{equation*}\label{eq:MFMC_sampling_cost_2}
%     \mathcal{W}^{\text{MF}} = \sum_{k=1}^K C_k N_k^* = \frac{\sigma_1^2}{\epsilon_{\text{tar}}^2}\left(\sum_{k=1}^K\sqrt{B_k} \right)^2.
% \end{equation*}
%
% The quantity $B_k$ depends on the product of the cost per sample $C_k$ and the difference between two successive correlations $(\rho_{1,k}^2 - \rho_{1,k+1}^2)$. Depending on how these components interact, $B_k$ may decay, grow, or remain constant as $k$ increases.
Under condition (ii) of Theorem \ref{thm:Sample_size_est}, the following inequality holds
%
\begin{equation}
\label{eq:Bk_Ck_decay_rate}
    \frac{\sqrt{B_{k}}}{\sqrt{B_{k-1}}}>\frac{C_{k}}{C_{k-1}}, \quad k=2,\ldots,K.
\end{equation}
%
This inequality implies that, in the asymptotic regime where $K$ is large,  the sequence $\sqrt{B_k}$ decays more slowly -- or grows more rapidly -- than the cost sequence $C_k$, regardless of the specific trend of $\sqrt{B_k}$. \JLcolor{Using this fact and the assumption that $\sum_{k=1}^K C_kN_k^*\ge \sum_{k=1}^K C_k$}, the additive overhead term
$\sum_{k=1}^K C_k$ in the cost bounds becomes asymptotically negligible relative to the leading-order term $\sum_{k=1}^K C_kN_k^*$. It follows that the total sampling cost with integer-rounded sample sizes retains the same asymptotic behavior as the cost computed using the real-valued optimal sizes in \eqref{eq:MFMC_sampling_cost}.


% Using the fact that $N_k$ increases and the value of $\alpha_k$, we observe that the MFMC estimator variance $\mathbb{V}\left(A^{\text{MFMC}}\right)$ in \eqref{eq:MFMC_variance2} always decreases as the model number $K$ increases. This reflects the fact that the low fidelity models are used as control variates to reduce the variance of the high fidelity model. However, this $K$ cannot be arbitrarily large, since the first summation term in \eqref{eq:MFMC_sampling_cost} grows, the second summation reflect the variance decay of the MFMC estimator. Thus this is a tie between these two terms. If $K$ is sufficiently large,  in order to achieve an optimal sampling cost, we need to study the decay and growth of these two terms. We will choose the $K$ such that the product of two summation terms in \eqref{eq:MFMC_sampling_cost} is minimum, i.e. If $K$ is sufficiently large, we need to find $K\in \mathbb{N}$ such that 
% \begin{equation}\label{eq:Optimal_K}
%    K = \text{argmin} \sum_{k=1}^K\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}\sum_{k=1}^K\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2. 
% \end{equation}

The efficiency of the multi-fidelity Monte Carlo estimator relative to the standard Monte Carlo estimator is quantified by the ratio
%
\begin{equation}\label{eq:MFMC_sampling_cost_efficiency}
    \xi = \frac{\mathcal{W}^\text{MF}}{\mathcal{W}^\text{MC}} = \frac{1}{C_1} \left(\sum_{k=1}^K\sqrt{C_k\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)}\right)^2,
\end{equation}
%
A smaller value of $\xi$ corresponds to greater computational savings, indicating a more effective MFMC estimator.

% Further more, we observe that
% \begin{align*}
%     \mathcal{W}_\text{MC}\mathbb{V}\left(A^{\text{MC}}\right) &=\frac{C_1\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{U}^2},\\
%  \mathcal{W}_\text{MFMC}\mathbb{V}\left(A^{\text{MFMC}}\right) &=  \frac{\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{U}^2}\sum_{k=1}^K\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}\sum_{k=1}^K\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2.
% \end{align*}
% This implies that if both Monte Carlo and multifidelity Monte Carlo have  a same sampling cost, then $\mu=  \mathbb{V}\left(A^{\text{MFMC}}\right)/\mathbb{V}\left(A^{\text{MC}}\right)$. Therefore, 

