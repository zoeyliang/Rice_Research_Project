\section{Pilot sample size for parameter estimation}\label{sec:Parameter_Estimation}
Accurate estimation of correlation coefficients constitutes a pivotal challenge in multi-fidelity Monte Carlo methods, with profound implications for estimator efficiency. While variance estimation for MFMC weights and sample sizes yields unbiased estimators converging at $\mathcal{O}(1/\sqrt{Q})$ for pilot sample size $Q$,  the nonlinear ratio estimator for correlation estimation
\[
\widehat{\rho}_{1,k} = \frac{\sum_{i=1}^Q\left\langle u_{1}^{(i)} - A_{1,Q}^{\text{MC}}, u_{k}^{(i)} - A_{k,Q}^{\text{MC}} \right\rangle_Z}{\sqrt{\sum_{i=1}^Q \left\|u_{1}^{(i)} - A_{1,Q}^{\text{MC}}\right\|_Z^2} \sqrt{\sum_{i=1}^Q \left\|u_{k}^{(i)} - A_{k,Q}^{\text{MC}}\right\|_Z^2}},
\]
where  $\widehat \cdot$ represent the sample estimate and $u_k^{(i)} := u_{h,k}(\cdot, \boldsymbol{\omega}^{(i)})$, is biased and exhibits pathological properties under bivariate normality: finite-sample bias decaying asymptotically as $\mathcal{O}(1/Q)$, variance proportional to $(1-\rho_{1,k}^2)^2/Q$ asymptotically, and severe distributional skewness near $|\rho_{1,k}| \approx 1$ \cite{Fi:1915,Ha:2007,Ri:1932}. These characteristics require substantially larger pilot samples than required for variance estimation, particularly in high-correlation regimes essential for effective multifidelity approximation.


Inadequate pilot sampling triggers dual failure modes that cascade through the MFMC framework. First, biased correlation estimates distort model selection, either excluding high-correlation models that enable significant cost reduction or incorporating weakly correlated surrogates that consume computational resources without proportional benefit. This induces suboptimal sample allocations that directly undermine theoretical efficiency. Second, variance in correlation estimates propagates nonlinearly through the efficiency ratio $\xi$, generating heavy-tailed distributions when the true correlation coefficients approach $\pm 1$. These distributional pathologies violate the variance guarantee $\mathbb{V}[A^{\mathrm{MF}}] \leq \epsilon_{\mathrm{tar}}^2$ with high probability due to extreme deviations in $\xi$. Crucially, these errors compound geometrically when multiple models exhibit similar correlation levels, as inaccuracies propagate through the weight allocation and amplify collective inefficiencies.


The core vulnerability stems from the nonlinear mapping $\xi(\boldsymbol{\rho})$, where errors $\Delta \boldsymbol{\rho}$ induce disproportionate perturbations $\Delta\xi = \xi(\widehat {\boldsymbol{\rho}}) - \xi(\boldsymbol{\rho})$ via its rational functional form. The gradient $\nabla_{\boldsymbol{\rho}} \xi$ quantifies this sensitivity, with components
%
\[
\frac{\partial \xi}{\partial \rho_{1,k}} = \frac{2S}{C_1}\rho_{1,k}\left(\sqrt{\frac{C_k}{\Delta_k }} - \sqrt{\frac{C_{k-1}}{\Delta_{k-1} }}\right), \quad S = \sum_{k=1}^K\sqrt{C_k\Delta_k }
\]
%
exhibit divergent magnitude $\|\nabla_{\boldsymbol{\rho}} \xi\|$ when $\Delta_k \to 0$, transforming small $\Delta \boldsymbol{\rho}$ into extreme $\Delta\xi$ values. Consequently, the sampling distribution of $\widehat{\xi}$ develops heavy tails.

% characterized by moment growth \JLcolor{$\mathbb{E}[|\widehat{\xi} - \xi|^p]^{1/p} \propto p \cdot (1-\boldsymbol{\rho}^2)^{-1}$ ($p>2$), invalidating variance reduction guarantees precisely where MFMC promises maximal gains.}


To mitigate these effects, we establish a pilot sampling framework anchored in dual criteria: (1) statistically confidence intervals for $\rho_{1,k}$ via Fisher transformation \cite{BiHi:2017,BoWr:1998, FiHaPe:1957,Fi:1915, Fi:1921} or Bootstrap \cite{BeDeToMeBaRo:2007,Ef:1979,EfTi:1993} to control estimation bias, and (2) bounded error propagation in $\xi$ enforced through gradient sensitivity analysis. Under bivariate normality of $u_{h,1}(\cdot, \boldsymbol{\omega})$ and $u_{h,k}(\cdot, \boldsymbol{\omega})$, the Fisher $z$-transformation $\widehat z_k = \tanh^{-1}(\widehat \rho_{1,k})$ yields variance-stabilized estimators with $\sigma_{\widehat z_k} = 1/\sqrt{Q-3}$ \cite{BiHi:2017,BoWr:1998, FiHaPe:1957,Fi:1915, Fi:1921}. The $(1-\alpha)$ confidence interval for $\rho_{1,k}$ is:
%
\begin{align}
    \label{eq:Confidence_Interval_rho}
    \text{CI}_{\rho_{1,k}}^{\text{fisher}} &= \text{tanh}\left(\widehat z_k \pm  z_{\alpha/2}\sigma_{\widehat z_k}\right)
    =\left[1-\frac{2}{R_k e^{-2z_{\alpha/2}\sigma_{\widehat z_k}}+1}, 1-\frac{2}{R_k e^{2z_{\alpha/2}\sigma_{\widehat z_k}}+1}\right] := [a_k,b_k].
    % = \left[\frac{e^{2(z_k - 1.96\sigma_{z_k})}-1}{e^{2(z_k - 1.96\sigma_{z_k})}+1},\; \frac{e^{2(z_k + 1.96\sigma_{z_k})}-1}{e^{2(z_k + 1.96\sigma_{z_k})}+1}\right].
\end{align}
%
where the z-score $z_{\alpha/2}$ is the $\alpha/2$-th quantile for the normal distribution and $R_k = (1+\widehat\rho_{1,k})/(1-\widehat\rho_{1,k})$.
The interval length $t_k=b_k - a_k$ satisfies $ \mathbb{P}(|\rho_{1,k}-\widehat \rho_{1,k}|\le t_k/2)\ge 1-\alpha$. When bivariate normality assumptions are violated, we use bootstrap methods to construct distribution-free confidence intervals through repeated sampling with replacement using the following procedure
%
\begin{enumerate}
    \item Generate $B$ bootstrap resamples by drawing $Q$ samples with replacement from pilot data
    \item Compute $\widehat{\rho}_{1,k}^{(b)}$ for each resample $b = 1,\dots,B$
    \item Construct the $(1-\alpha)$ confidence interval via the percentile method:
    \[
    \text{CI}_{\rho_{1,k}}^{\text{boot}} = \left[ \widehat{\rho}_{1,k}^{(\alpha/2)}, \widehat{\rho}_{1,k}^{(1-\alpha/2)} \right]
    \]
    where $\widehat{\rho}_{1,k}^{(\gamma)}$ denotes the $\gamma$-quantile of the bootstrap distribution
\end{enumerate}
%
This bootstrap approach provides three key advantages in our framework: (1) it makes no parametric assumptions about the joint distribution of model outputs, accommodating complex dependencies and heavy-tailed behaviors; (2) it automatically captures estimation bias through the empirical distribution of resampled statistics; and (3) it maintains validity even when Fisher transformation assumptions are violated. The bootstrap interval length $t_k^{\text{boot}}$ directly substitutes for $t_k$ in our error bound $\frac{|\Delta \xi|}{\xi} \leq \frac{E\sqrt{K-1}}{2} t_k^{\text{boot}}$, ensuring robustness to distributional misspecification.

Computationally, bootstrap resampling requires no additional model evaluations when pilot samples are stored. The dominant cost arises from computing correlation coefficients for each resample, primarily inner products for sample means and variances for $\widehat{\rho}_{1,k}^{(b)}$.


Linking statistical precision to computational efficiency, we bound relative efficiency errors via Cauchy-Schwarz
%
\begin{equation}\label{eq:delta_xi_bound}
    \frac{\left|\Delta \xi\right|}{\xi}\le \underbrace{\frac{1}{\xi}\sqrt{\sum_{k=2}^K \left(\frac{\partial \xi}{\partial \rho_{1,k}}\right)^2}}_{E(\boldsymbol{\rho})} \cdot \sqrt{\sum_{k=2}^K\left(\Delta\rho_{1,k}\right)^2}
    % =\underbrace{\frac{2}{S}\sqrt{\sum_{k=2}^K(S^\prime)^2} }_{E(\boldsymbol{\rho})}\cdot \sqrt{\sum_{k=2}^K\left(\Delta\rho_{1,k}\right)^2}
    \le \frac{E}{2} \sqrt{\sum_{k=2}^K t_k^2}\le \frac{E\sqrt{K-1}}{2}t_K,
\end{equation}
%
with probability $(1-\alpha)^K$  for independent estimators.  Setting $t_k \leq \delta$ controls the efficiency error via $E\sqrt{K-1}\,\delta/2$, while solving $t_k = \delta$ yields the minimum sample size
%
\begin{equation*}\label{eq:Pilot_sample_size_estimate}
Q \geq 3 + \left( \frac{2 z_{\alpha/2}}{\log m} \right)^2, \quad m = \frac{(R_k^2 + 1) \delta + \sqrt{(R_k^2 - 1)^2 \delta^2 + 16 R_k^2}}{2 R_k (2 - \delta)}.
\end{equation*}
%
% Since $\widehat \rho_{1,k} \in [-1,1]$, it follows that $t_k \in (0, 2)$. $t_k$ increases as $|\widehat \rho_{1,k}|$ approach 0 and decreases as $|\widehat \rho_{1,k}|$ approach to 1. 
This adapts sampling to correlation structures and model costs, preventing undersampling-induced pathologies while avoiding computational waste.

We propose a dynamic sampling strategy (also known as sequential analysis \cite{La:2001,Wa:1947}) using Welford's algorithm for incremental statistics and sequential confidence interval assessment, shown in Algorithm~\ref{algo:Parameter_Estimation}. A minimum of  $Q_{\min}=30$ to satisfy asymptotic requirements of Fisher-z transformation or Bootstrap, the algorithm expands the pilot sample until $\max_k t_k \leq \delta$ while monitoring gradient sensitivity $E(\boldsymbol{\rho})$. The algorithm adaptively increases $Q$ until either the Fisher or bootstrap-derived $t_k$ satisfies $\max_k t_k \leq \delta$, with the choice between methods determined by diagnostic checks for bivariate normality. This framework integrates with backtracking model selection (Algorithm~\ref{algo:enhanced_mfmc_selection}) to jointly optimize statistical precision and computational efficiency while maintaining robustness to distributional assumptions.


% $\mathbb{E}[|\widehat{\xi} - \xi|^p]^{1/p} \propto p \cdot (1-\rho^2)^{-1}$（$p>2$）
% Second, variance in correlation estimates introduces non-Gaussian tails in the sampling distribution of the cost efficiency $\xi$, leading to high-probability violations of the theoretical variance reduction bound. These effects compound when multiple correlated models are active, as errors propagate geometrically through the weight allocation system.










% Using these two estimates, we determine the optimal choice of $Q$ by ensuring that the mean square error does not exceed a prescribed threshold $\delta$, we allocate a fraction $\theta_1$ to bias and $1-\theta_1$ to variance. Using the error splitting in \eqref{eq:MSE_rho}, we obtain the required pilot sample size
% applying Chebyshev’s inequality $P(|\mathbb{E}(\rho_{1,k}^{(Q)})-\rho_{1,k}^{(Q)}|\ge \nu)\le \text{Var}(\rho_{1,k}^{(Q)})/\nu^2$ with $\nu = (1-\theta_1)\delta_1$ gives
% %
% \[
% P\left(\left|\mathbb{E}\left(\rho_{1,k}^{(Q)}\right)-\rho_{1,k}^{(Q)}\right|\ge \nu\right)\le \frac{\text{Var}\left(\rho_{1,k}^{(Q)}\right)}{\nu^2}
% \]
% %
% %
% \[
% \frac{(1-\rho_{1,k}^2)^2}{Q\nu^2} = \frac{(1-\rho_{1,k}^2)^2}{(1-\theta_1)^2Q\delta_1^2}\le 1\rightarrow Q\ge \frac{(1-\rho_{1,k}^2)^2}{(1-\theta_1)^2\delta_1^2}.
% \]
% %
% Combining these results, a lower bound on $Q$ can be determined as
%
% \begin{equation}
% \label{eq:Offline_Sample_Size}
%     Q\ge \max_{k} \left(\frac{\left|a_1\right|}{\sqrt{\theta_1\delta} }, \frac{a_2}{(1-\theta_1)\delta}\right).
% \end{equation}
% %
% Note in \eqref{eq:Offline_Sample_Size}, we still need to estimate the true correlation coefficients in order to estimate the lower bound of pilot sample size $Q$. However, the sample statistics also depends on $Q$,  we thus  iteratively update $Q$ until convergence is reached. % However, when sampling with a small sample size that does not rely on assumptions about the underlying data distribution, non-parametric method like  bootstrapping \cite{Wa:2006} and sequential analysis \cite{Wa:1947} provide alternative strategies for estimating $Q$. 







% \begin{align*}
% \frac{\partial  \xi}{\partial  \rho_{1,1}} &=\frac{2\sum_{j=1}^K\sqrt{C_j\left(\rho_{1,j}^2 - \rho_{1,j+1}^2\right)}}{C_1}\frac{C_1\rho_{1,1}}{\sqrt{C_1(\rho_{1,1}^2-\rho_{1,2}^2)  }}\\
%     \frac{\partial  \xi}{\partial  \rho_{1,k}} 
% &=\frac{2SS^\prime}{C_1}, \quad \forall\; k=2,\ldots, K.\\
% \frac{\partial  \mathbb{V}\left(A^{\text{MF}}\right)}{\partial  \rho_{1,k}} 
% &=\sigma_1^2\left[2\rho_{1,k}\left(\frac{1}{N_{k}} - \frac{1}{N_{k-1}}\right)-\left( \frac{\rho_{1,k-1}^2 -\rho_{1,k}^2 }{N_{k-1}^2}\frac{\partial N_{k-1}}{\partial  \rho_{1,k}}+\frac{\rho_{1,k}^2 -\rho_{1,k+1}^2 }{N_k^2}\frac{\partial N_k}{\partial  \rho_{1,k}}\right)\right]\\
% &=\epsilon^2(1-\theta)\left\Vert\mathbb{E}(u) \right\Vert_{Z}^2\frac{S^\prime \left(S-T\right)}{S^2},\\
% S& = \sum_{k=1}^K\sqrt{C_k\Delta_k },\quad
% S^\prime = \frac{\partial  S}{\partial  \rho_{1,k}} = \rho_{1,k}\left(\sqrt{\frac{C_k}{\Delta_k }} - \sqrt{\frac{C_{k-1}}{\Delta_{k-1} }}\right).\\
% T &=  \sqrt{C_{k-1}(\rho_{1,k-1}^2 - \rho_{1,k}^2)} - \sqrt{C_{k}(\rho_{1,k}^2 - \rho_{1,k+1}^2)}\\
% S^{\prime\prime}&= \frac{\partial^2  S}{\partial^2  \rho_{1,k}} = \frac{S^\prime}{\rho_{1,k}} - \rho_{1,k}^2\left(\frac{\sqrt{C_k}}{(\rho_{1,k}^2-\rho_{1,k+1}^2)^{3/2}}+\frac{\sqrt{C_{k-1}}}{(\rho_{1,k-1}^2-\rho_{1,k}^2)^{3/2}}\right).
% \end{align*}







% \begin{align}\label{eq:delta_var_bound}
%     \frac{\left|\Delta \mathbb{V}\left(A^{\text{MF}}\right)\right|}{\mathbb{V}\left(A^{\text{MF}}\right)}&\le
%     % \le \underbrace{\frac{1}{\mathbb{V}\left(A^{\text{MF}}\right)}\sqrt{\sum_{k=2}^K \left(\frac{\partial \mathbb{V}\left(A^{\text{MF}}\right)}{\partial \rho_{1,k}}\right)^2}}_{A_1}\cdot \sqrt{\sum_{k=2}^K\left(\Delta\rho_{1,k}\right)^2}=
%     \underbrace{\frac{1}{S^2}\sqrt{\sum_{k=2}^K\left(S^\prime \left(S-T\right)\right)^2}}_{A_1}\cdot \sqrt{\sum_{k=2}^K\left(\Delta\rho_{1,k}\right)^2}
% \end{align}

 
 
 % Note we dont know the exact value of $E$, since $[(S^\prime/S)^2]^{\prime} = 2S^\prime(SS^{\prime\prime} - (S^\prime)^2)/S^3$ is always positive,  an estimated upper bound $\widehat E$ for $E$ can be estimated as evaluating $E$ at the upper bound $b_k$ of each confidence interval $\text{CI}_{\rho_{1,k}}$. It follows that the relative error in $\xi$ is bounded by $\widehat E \sqrt{K - 1}\delta / 2$.




% \JLcolor{Given $\delta$, we first estimate $C^\prime$, then choose $\delta_2$ as $\delta/C^\prime$, $\delta_1=\delta_2/\sqrt{K-1}$, and select $N$ by \eqref{eq:Offline_Sample_Size} for all $k$.}


% The term $\left(1-\sqrt{\frac{C_{k-1}(\rho_{1,k}^2-\rho_{1,k+1}^2)}{C_k(\rho_{1,k-1}^2-\rho_{1,k}^2)}}\right)$ in $\partial \xi/\partial \rho_{1,k}$ encodes MFMC’s selection criteria, ensuring the derivative’s negativity when models are optimally ordered. This indicates that higher $\rho_{1,k}$ improves low-fidelity models’ variance reduction efficiency, reducing reliance on costly high-fidelity evaluations. This reinforces the idea that high-quality low-fidelity models—those that are more aligned with the high-fidelity results—can significantly lower the reliance on expensive high-fidelity evaluations, making the entire multi-fidelity approach more cost-effective.













%
\normalem
\begin{algorithm}[!ht]
\label{algo:Parameter_Estimation}
\DontPrintSemicolon
\DontPrintSemicolon

    
\KwIn{Max CI length $\delta$, tolerance $\vartheta$, number of models $K$, initial sample size $Q_0$, cost vector $\boldsymbol{C} = (C_1, \dots, C_K)$, stability threshold $\tau=3$, bootstrap sample count $B$ (default: 1000).}
\KwOut{Final sample size $Q$, estimated parameters $\widehat{\boldsymbol{\sigma}},\widehat{\boldsymbol{\alpha}}, \widehat{\boldsymbol{\rho}}$, efficiency $\widehat{\xi}$, stable index set $\mathcal{I}^{cur}$.}
\hrule

Initialization:
\begin{itemize}%[leftmargin=5pt]
    \item  $dQ \gets Q_0$, $p \gets 0$, $\text{converged} \gets \text{False}$, $\text{stable}\_\text{count} \gets 0$, $\mathcal{I}^{prev}\gets \emptyset$. %\tcp*{}
    
    \item Initialize Welford states: $m_k^{(0)} \gets 0$, $v_k^{(0)} \gets 0$ for $k=1,\dots,K$,  $r_k^{(0)} \gets 0$, for $k=2,\dots,K$. %\tcp*{Welford's algorithm}
\end{itemize}


\While{$\neg \text{converged}$ \& $\text{stable}\_\text{count}< \tau$}{
    
    \For{$k=2,\ldots, K$}{
    
        \For{$i = 1,\cdots, dQ $}
    {
    $Q=p+i$.

    Draw $dQ$ new samples $\{\boldsymbol{\omega}^{(i)}\}_{i=1}^{dQ}$. Compute sample realization $u_{1}^{(Q)}$ and $u_{k}^{(Q)}$.\\

    Update mean $m_1^{(Q)}$, $m_k^{(Q)}$ and proxy of variance $v_1^{(Q)}$, $v_k^{(Q)}$ and covariance $\text{Cov}_{1,k}^{(Q)}$ via Welford's algorithm.

    % Sort $\left\|u_{h,1}^{(j)}\right\|_U$ and $\left\|u_{h,k}^{(j)}\right\|_U$ into ordinal data for Spearman's correlation coefficient.
    }
    
    Compute correlations: $\widehat\rho_{1,k}^{(Q)}\gets r_k^{(Q)}/\sqrt{v_1^{(Q)}v_k^{(Q)}}$ for $k=2,\ldots, K$. $\widehat{\boldsymbol{\rho}}^{(Q)} \gets (1,\widehat{\rho}_{1,2}^{(Q)}, \dots, \widehat{\rho}_{1,K}^{(Q)})$.

    
    % Estimate Spearman's correlation coefficient $\widehat r_{1,k}$.
    \eIf{bivariate normality holds}{
        
            $\widehat{z}_k \gets \tanh^{-1}(\widehat{\rho}_{1,k}^{(Q)})$, $\sigma_{\widehat{z}_k} \gets 1/\sqrt{Q - 3}$. \\
            Compute $\text{CI}_{\rho_{1,k}}^{\text{fisher}}$ and $t_k$ via Fisher z-transform in \eqref{eq:Confidence_Interval_rho}.
        
    }{
        
            Generate $B$ bootstrap replicates of $\widehat{\rho}_{1,k}^{(Q)}$.  \\
            Compute $\text{CI}_{\rho_{1,k}}^{\text{boot}}$ and $t_k$ via bootstrap.
        
    }

    
    % \If{Bivariate normal}
    % {
    % Compute pilot sample size $Q_k^{(j)}$ using \eqref{eq:Pilot_sample_size_estimate}.
    % }
    % \Else {
    % Bootstrap construct confidence intervals.
    % }
    }
    % [$\text{index},\xi^{(p)}$] = Multi-fidelity Model Selection ($\boldsymbol{\rho}^{(p)},\boldsymbol{C}$).

    % $Q \geq \max\{30, Q_{\max \{\text{idx}^{current}\}} \}$ \text{as in } \eqref{eq:Pilot_sample_size_estimate} and
    [$\mathcal{I}^{cur},\widehat{\xi}^{(Q)}$] $\gets$ \textsc{ModelSelectionBacktrack} ($\widehat{\boldsymbol{\rho}}^{(Q)},\boldsymbol{C}$).\\

    \eIf{$\max_k t_k \leq \delta$, $\mathcal{I}^{prev}=\mathcal{I}^{cur}$ }{
            $\text{converged} \gets \text{True}$.

            $\text{stable}\_\text{count} = \text{stable}\_\text{count}+1$.
    }
    {
            $\text{stable}\_\text{count} = 0$.
    
        $p=p+dQ$. \tcp*{Adaptive sample size increase}
        
        $\widehat{\xi}^{(p)} = \widehat{\xi}^{(Q)}$.
    }
    
    % \eIf{bivariate normality holds}{
    %     \If{$\max_k t_k \leq \delta$, $\text{idx}^{prev}=\text{idx}^{current}$ }{
    %         $\text{converged} \gets \text{True}$.

    %         $\text{stable}\_\text{count} = \text{stable}\_\text{count}+1$.
    %     }
    % }{
    %     \If{$\max_k t_k \leq \delta$ and $\text{idx}^{prev}=\text{idx}^{current}$}{
    %         $\text{converged} \gets \text{True}$.

    %         $\text{stable}\_\text{count} = \text{stable}\_\text{count}+1$.
    %     }
    % }
    
    % \If{$\neg \text{converged}$}{
    %     $\text{stable}\_\text{count} = 0$.
    
    %     $p=p+dQ$. \tcp*{Adaptive sample size increase}
        
    %     $\xi^{(p)} = \xi^{(Q)}$.

    % }

    $\mathcal{I}^{prev} \gets \mathcal{I}^{cur}$.
    }
    % \If{ If bivariate: $\max_k\left|\left(Q_k^{(j)}-Q_k^{(p)}\right)/Q_k^{(j)}\right| \leq \vartheta$ as in \eqref{eq:delta_xi_bound} and $j\ge \max_k\left\{30,Q_k^{(j)}\right\}$, Else: Confidence interval length $<\delta$}
    % \text{AddSample = False}
    % }
    % \Else {
    % \text{AddSample = True}

    % $\xi^{(p)} = \xi^{(j)}$.

    % $Q_k^{(p)} = Q_k^{(j)}$.
    
    %  $p=p+dQ$}

    
    % $\left|\frac{\xi^{(j)}-\xi^{(p)}}{\xi^{(j)}}\right|<\delta$ and
    % $\&$ $\left|\frac{\sigma_{k}^{(j)}-\sigma_{k}^{(j-1)}}{\sigma_{k}^{(j)}}\right|<\delta$ $\&$ $\left|\frac{\widehat \sigma_{k}^{(j)}-\widehat \sigma_{k}^{(j-1)}}{\widehat \sigma_{k}^{(j)}}\right|<\delta$ for all $k=2,\ldots, K$}
    
    % \If{}
    % {
    
    % }
    % \Else {
    % \text{AddSample = True}
    % }
    
    
    % \If{$j<Q_t$}
    % {
    % AddSample = True
    % }
    
Return $\widehat{\sigma}_1 \gets \sqrt{v_1^{(Q)}}$, $\widehat{\sigma}_k \gets \sqrt{v_k^{(Q)}}$, $\widehat{\alpha}_k \gets \widehat{\rho}_{1,k}^{(Q)} \widehat{\sigma}_1^{(Q)} / \widehat{\sigma}_k^{(Q)}$, $\widehat{\boldsymbol{\rho}}^{(Q)}$, $\widehat{\xi}^{(Q)}$ for $k \in \mathcal{I}^{cur}$.
\caption{Dynamic Parameter Estimation}
\end{algorithm}
\ULforem
%


