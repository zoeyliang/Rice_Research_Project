%!TEX root = main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ====================================================
\section{Monte Carlo Method}\label{sec:MC}
% ====================================================

A standard approach to approximate the expectation in \eqref{eq:expectation_of_u} is the Monte Carlo (MC) method; 
see, e.g.,  \cite{MBGiles_2015a,MDGunzburger_CGWebster_GZhang_2014a}.
Because $u$ cannot be evaluated, but only a approximation $u_h \in   L_{\mathbb{P}}^2(W, \cU)$ can be 
computed, we estimate $\mathbb{E}[u_{h}]$, 
The MC estimator $A^{\text{MC}}_{N}$ of $\mathbb{E}[u_{h}]$ 
is the sample mean over $N$ independent and identically distributed (i.i.d.) realizations 
\begin{equation}\label{eq:MC_estimator}
    A^{\text{MC}}_{N} := \frac{1}{N}\sum_{i=1}^{N} u_{h}\big(\omega^{(i)} \big).
\end{equation}
%
This estimator is unbiased,  $\mathbb{E}[A^{\text{MC}}_{N}] = \mathbb{E}[u_{h}]$, 
and has variance $\mathbb{V}[A^{\text{MC}}_{N}] = N^{-1} \mathbb{V}[u_{h}]$, 
where the variance is defined as in \eqref{eq:variance_of_u}.
By the central limit theorem, the MC estimator $A^{\text{MC}}_{N}$ converges in distribution to $\mathbb{E}[u_h]$ as $N$ approaches infinity. 

To quantify the total approximation error of the estimator, we consider the  {\it normalized mean squared error (nMSE)}, defined as
%
 \[
\mathcal{E}_{A^{\text{MC}}_{N}}^2:= \mathbb E\left[ \big\| \mathbb{E}[u]-A^{\text{MC}}_{N}  \big\| _{U}^2\right]  \big/ \, \big\| \mathbb{E}[u]  \big\| _{U}^2.
\] 
%
The nMSE decomposes into two contributions: a {\it bias error} from spatial discretization, and a {\it statistical error} due to finite sampling
%
\[
\mathcal{E}_{A^{\text{MC}}_{N}}^2 
= \frac{ \big\| \mathbb{E}[u]-\mathbb{E}[u_{h}]  \big\| _{U}^2+\mathbb E\left[ \big\|  \mathbb{E}[u_{h}] -A^{\text{MC}}_{N}  \big\| _{U}^2\right]}{ \big\| \mathbb{E}[u]  \big\| _{U}^2} 
= \frac{ \big\| \mathbb{E}[u]-\mathbb{E}[u_{h}]  \big\| _{U}^2}{ \big\| \mathbb{E}[u]  \big\| _{U}^2}+\frac{\mathbb{V}\left[u_{h}\right]}{N \big\| \mathbb{E}[u]  \big\| _{U}^2}
=\mathcal{E}_{\text{Bias}}^2 + \mathcal{E}_{\text{Stat}}^2.
\]
%
Suppose the sample-wise discretization error satisfies
%
\begin{equation*} \label{eq:Assumption_uhA}
       \left\|u\left(\omega^{(i)}\right)-u_h\left(\omega^{(i)}\right)\right\|_U
       \leq C_m\left(\omega^{(i)}\right)M^{-\alpha}\,,
\end{equation*}
%
where $C_m(\omega^{(i)})$ is a constant depending only on the geometry of the spatial domain and the particular realization $\omega^{(i)}$, $\alpha>0$ is the convergence rate of spatial discretization, and $M$ denotes the number of spatial degrees of freedom. For simplicity and analytical tractability, we assume this constant is uniformly bounded across all realizations, i.e. $C_m(\omega^{(i)})\le C_m$ for some $C_m=\sup_{\omega \in \Omega} C_m(\omega)>0$ independent of the sample realization $\omega^{(i)}$ \cite{BaNoTe:2007,BaScZo:2011}.


Given a user-specified threshold $\epsilon^2$  for the nMSE, we introduce a {\it splitting ratio} $\theta \in (0,1)$ to allocate the total error budget between bias and statistical components
%
\begin{equation} \label{eq:error-budget}
%\textcolor{red}{\|u-u_h\|_{L^2(\boldsymbol W,U)}\le C_mM^{-\alpha}\le \theta_1\epsilon},\qquad\text{ and }\qquad \|u_h-\widehat u_{h}\|_{L^2(\boldsymbol W,U)} \le C_{p} P^{-\nu}\le \theta_2\epsilon\,.  
\mathcal{E}_{\text{Bias}}^2=\|u-u_h\|_{L^2(\boldsymbol \Omega,U)}\le C_mM^{-\alpha}= \theta\epsilon^2, \quad\quad \mathcal{E}_{\text{Stat}}^2 = \frac{\sigma_1^2}{N \big\| \mathbb{E}(u)  \big\| _{U}^2}=(1-\theta)\epsilon^2,
\end{equation}
where $C_m$ is independent of the sample and $\sigma_1^2 = \mathbb{V}\left( u_{h}\right)$. To meet these error constraints, the number of spatial nodes $M$ and sample size $N$ must obey
%
\begin{equation}
\label{eq:SLSGC_SL_SpatialGridsNo_n_SparseGridsNo}
M\ge \left(\frac{\theta\epsilon^2}{C_m}\right)^{-\frac 1 {\alpha}},\quad\quad  N \ge  \frac{\sigma_1^2}{\epsilon_{\text{tar}}^2},
\end{equation}
%
where $\epsilon_{\text{tar}}^2 = \epsilon^2(1-\theta) \big\| \mathbb{E}(u)  \big\| _{U}^2$.
Assuming each evaluation of $u_{h}$ incurs an average cost of $C$, the total cost to compute $A^{\text{MC}}_{N}$ is
%
\[
\mathcal{W}^\text{MC}  = CN=\frac{C\sigma_1^2}{\epsilon_{\text{tar}}^2}.
\]
%
In practice, both $M$ and $N$ are rounded up to the smallest integers satisfying \eqref{eq:SLSGC_SL_SpatialGridsNo_n_SparseGridsNo}.