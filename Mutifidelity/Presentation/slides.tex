\documentclass{beamer}
\mode<presentation>
\usepackage{algorithm,algorithmicx,amssymb,amsmath,amsthm,bbm,color,epstopdf,float,geometry,graphicx,hyperref,listings,mathrsfs,mathtools,multirow,subcaption,textgreek,xcolor}

\usepackage[noend]{algpseudocode}
\usepackage{algcompatible}
% \algnewcommand\algorithmicreturn{\textbf{return}}
% \algnewcommand\RETURN{\State \algorithmicreturn}%


\usepackage{float}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
    
% \usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[flushleft]{threeparttable}
\usepackage[shortlabels]{enumitem}
\newcommand{\JLcolor}[1]{{\textcolor{violet}{#1}}} %violet

% Create Background Pic
%\usepackage{tikz}
%\definecolor{bottomcolour}{rgb}{0.32,0.3,0.38}
%\definecolor{topcolour}{rgb}{0.08,0.08,0.16}
%\setbeamertemplate{background canvas}{%
%	\begin{tikzpicture}[remember picture,overlay]
%	\shade[top color=topcolour,bottom color=bottomcolour]
%(current page.north west) rectangle (current page.south east);
%	\end{tikzpicture}%     
%}


%=======================================================
% Set page number
\setbeamertemplate{footline}[page number]

%=======================================================
%\usetheme{Boadilla}
\setbeamerfont{title}{size=\fontsize{18pt}{18pt}\selectfont} % bold font: \bfseries
\setbeamerfont{author}{size=\fontsize{20pt}{20pt}\selectfont}
\setbeamerfont{institute}{size=\fontsize{13pt}{13pt}\selectfont}
%\setbeamercolor{itemize item}{fg=red}

%================================================
%Set font color
\definecolor{mygray1}{rgb}{0.7421875,0.7421875,0.7421875}
\definecolor{mygray2}{gray}{0.6}
\definecolor{mygray3}{rgb}{0.55, 0.52, 0.54}
\definecolor{mybrown1}{rgb}{0.28, 0.24, 0.2}
\definecolor{myred}{rgb}{0.68, 0.09, 0.13}
\definecolor{myginger}{rgb}{0.69, 0.4, 0.0}
\definecolor{myblue1}{rgb}{0.0, 0.2, 0.6}
\definecolor{myblue2}{rgb}{0.38, 0.51, 0.71}
\definecolor{myblue3}{rgb}{0.2,0.2,0.7}
\definecolor{mygreen}{rgb}{0.42, 0.56, 0.14}
\definecolor{myviolet}{rgb}{0.54, 0.17, 0.89}


\newcommand{\highlightA}[1]{\colorbox{myblue3!50!}{$\displaystyle #1$}}
\newcommand{\highlightB}[1]{\colorbox{myred!50!}{$\displaystyle #1$}}
\newcommand{\highlightC}[1]{\colorbox{mygreen!50!}{$\displaystyle #1$}}
%================================================
% set picture source
\usepackage[absolute,overlay]{textpos}

\setbeamercolor{framesource}{fg=gray}
\setbeamerfont{framesource}{size=\tiny}
\newcommand{\source}[1]{\begin{textblock*}{4cm}(8.7cm,8.6cm)
\begin{beamercolorbox}[ht=0.5cm,right]{framesource}
\usebeamerfont{framesource}\usebeamercolor[fg]{framesource} Source: {#1}
\end{beamercolorbox}
\end{textblock*}}

\setbeamercolor{frametitle}{fg=white,bg=black}


%================================================
% url color
\renewcommand\UrlFont{\color{myblue3}}
%================================================

\newtheorem{Thm}{{\bf Theorem}}


% Make first page
\makeatletter
\setbeamertemplate{navigation symbols}{}
\title[MFMC for Tokamak UQ]{Multi-fidelity Monte Carlo for Uncertainty Quantification in the Free-Boundary Grad–Shafranov Equation}
%\subtitle[short version]{}
%\date{}
\author[J.Liang]{\normalsize  Jiaxing Liang\\ Matthias Heinkenschloss}
\institute[Rice University]{\fontsize{8}{8} Department of Computational and Applied Mathematics, Rice University}
%\institute[UMD]{University of Maryland, College Park}
%\usetheme{Pittsburgh}
%\usecolortheme{owl}

%======Footline=================================================
%\makeatother
%\setbeamertemplate{footline}
%{
%	\leavevmode%
%	\hbox{%
%%		\begin{beamercolorbox}[wd=.4\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
%%			\usebeamerfont{author in head/foot}\insertshortauthor
%%		\end{beamercolorbox}%
%		\begin{beamercolorbox}[wd=\paperwidth,ht=2.25ex,dp=1ex,leftskip=4.3cm,rightskip=.3cm plus1fil]{topcolour}
%			\usebeamerfont{title in head/foot}\insertshorttitle\hfill%\hspace*{3em}
%			\insertframenumber{} / \inserttotalframenumber\hspace*{1ex}
%	\end{beamercolorbox}}%
%\begin{beamercolorbox}[wd=.1\paperwidth,ht=2.25ex,dp=1ex,right,rightskip=2ex]{page number in head/foot}%
%	%            \usebeamerfont{title in head/foot}\insertshorttitle\hspace*{3em}
%	\insertframenumber{} / \inserttotalframenumber
%\end{beamercolorbox}%    
%	\vskip0pt%
%}

%=======================================================
\begin{document}
\frame{\titlepage }


% --- Introduction and Motivation ---
\begin{frame}{Fusion Energy and Modeling Challenges}
    \begin{itemize}[leftmargin=5pt] 
        \item Tokamak fusion reactors confine plasma using strong magnetic fields.
        \item Plasma equilibrium governed by free-boundary Grad–Shafranov equation.
        \item Key engineering challenge: stability and shape of plasma boundary.
        \item Model predictions sensitive to uncertainties in coil currents and parameters.
    \end{itemize}
\end{frame}

% --- Mathematical Formulation ---
\begin{frame}{Free-Boundary Grad–Shafranov Equation}
    \begin{itemize}
        \item PDE for poloidal flux function $u(r,z)$:
        \[
        -\nabla \cdot \left( \frac{1}{\mu_0 r} \nabla u \right) = \text{source}(u;r,z,\omega),
        \]
        where $\omega$ encodes uncertain coil currents.
        \item Plasma boundary $\partial \Omega_p$ defined by last closed flux surface.
        \item Outputs of interest: expected flux $\mathbb{E}[u]$, boundary shape, X-points.
    \end{itemize}
\end{frame}


\begin{frame}{Uncertain Coil Currents (ITER Setup)}
    \begin{itemize}
        \item ITER geometry: 12 coils with reference currents $I_1,\ldots,I_{12}$.
        \item Uncertainty model: independent uniform perturbations $\pm 2\%$.
        \item Source profiles $p(\psi), g(\psi)$ fixed from experimental design.
        \item High-fidelity solver: finite element discretization, Newton iteration.
    \end{itemize}
\end{frame}

\begin{frame}{Uncertainty Quantification in Plasma Equilibria}
    \begin{itemize}
        \item Goal: quantify how parameter uncertainty affects plasma boundary and geometry.
        \item Standard tool: Monte Carlo (MC) sampling.
        \item Problem: each high-fidelity PDE solve is expensive ($10^3$--$10^6$ DOFs).
        \item Need variance-reduction strategies to achieve feasible computation.
    \end{itemize}
\end{frame}


\begin{frame}{High- and Low-Fidelity Models}
    \begin{itemize}
        \item High fidelity (HF): FE solver with fine mesh (up to $1.9 \times 10^6$ DOFs).
        \item Low fidelity (LF): sparse grid collocation + coarser meshes.
        \item LF models are cheaper but correlated with HF $\rightarrow$ useful as control variates.
    \end{itemize}
\end{frame}

\begin{frame}{Approach Overview}
    \begin{itemize}[leftmargin=5pt] 
        \item[$\triangleright$] Combine ideas from:
        \begin{itemize}[leftmargin=15pt] 
            \item[$\circ$]  Monte Carlo (baseline).
            \item[$\circ$] Multilevel Monte Carlo (hierarchical meshes).
            \item[$\circ$] Multi-fidelity Monte Carlo (use surrogates as control variates).
        \end{itemize}
        \item[$\triangleright$] New contributions:
        \begin{itemize}[leftmargin=15pt] 
            \item[$\circ$] Optimization-based formulation for MFMC sample allocation.
            \item[$\circ$] Dynamic pilot sampling strategy for correlation estimation.
        \end{itemize}
    \end{itemize}
\end{frame}
%------------------------------------------------------------
\begin{frame}[t]
    \frametitle{Monte Carlo Sampling}
    \begin{itemize}[leftmargin=5pt] 
        \item[$\triangleright$] {\fontsize{10}{10}\selectfont \textcolor{myblue3}{\bf Estimate $\mathbb{E}[u]$} by  a finite number of i.i.d. sample averages:
            \[
		A^{\text{MC}}_N:= \frac {1}{N}\sum_{i=1}^N u_h^{(i)}.
		\vspace{-2mm}
		\]
                }
        \item[$\triangleright$] Unbiased, but variance decays slowly: $\mathcal{O}(N^{-1/2})$. 
			{\fontsize{10}{10}\selectfont 
				
				\vspace{2mm}
				$u$: Solution of PDE. 
				
				$u_{h}$: Approximation of $u$ based on discretization parameter $h$. 
				
				$u_h^{(i)} = u_h(\omega^{(i)})$: realization of the $i$-th random sample $\omega^{(i)}$. 	 
				
				$N$: Number of samples. }
				\vspace{-3mm}
				{\fontsize{9}{10}\selectfont
			}
  {\fontsize{10}{10}\selectfont 
  $\mathbb{E}(A^{\text{MC}}_N) = \mathbb{E}(u_h)$, $\mathbb{V}(A^{\text{MC}}_N)= \mathbb{V}(u_h)/N$, where}
  {\fontsize{9}{10}\selectfont
  	
$
\mathbb{V}(u_h) = \frac{1}{N-1}\left(\sum_{i=1}^{N}\left\Vert u_h^{(i)}\right\Vert_{Z}^2-\frac{1}{N}\left\Vert\sum_{i=1}^{N}u_h^{(i)}\right\Vert_{Z}^2\right).
$
}
\end{itemize}
\end{frame}


%------------------------------------------------------------
\begin{frame}[t]
    \frametitle{Multi-fidelity Monte Carlo}
\begin{itemize}[leftmargin=5pt] 
\item[$\triangleright$] \textcolor{myblue3}{\bf Multi-fidelity Monte Carlo (MFMC)} 

{\footnotesize A variance reduction technique that efficiently estimates statistical quantities by fusing high-fidelity and low-fidelity models, maintaining accuracy while significantly lowering costs.}

%
\vspace{2mm}
%
{\fontsize{7}{7}\selectfont \textcolor{mygray2}{B. Peherstorfer, K. Willcox, and M. Gunzburger. Optimal Model Management for Multifidelity Monte Carlo Estimation. SIAM J. SCI. COMPUT, Vol. 38, No. 5, pp. A3163–A3194.}\par}
%

\item[$\triangleright$]\textcolor{myblue3}{\bf Unbiased estimator $A^{\text{MF}}$ for $\mathbb{E}(u)$} 
{\fontsize{7}{7}\selectfont 
\begin{align*}
A^{\text{MF}} &= A^{\text{MC}}_{1,N_1} + \sum_{k=2}^K \alpha_k\left(\overline{A}_{k,N_k} - \overline{A}_{k,N_{k-1}} \right), \quad N_k\ge N_{k-1}
    % &= A^{\text{MC}}_{1,N_1} +  \sum_{k=2}^K \alpha_k\left(\frac{N_{k-1}}{N_{k}}-1\right)\left(A_{k,N_{k-1}}^{\text{MC}}- A_{k,N_k\backslash N_{k-1}}^{\text{MC}}\right) 
\end{align*}
}
\vspace{-5mm}

{\footnotesize 
$\overline{A}_{k,N_{k-1}}$ reuses the first $N_{k-1}$ samples from $\overline{A}_{k,N_{k}}$.

$\mathbb{E}(A^{\text{MF}}) = \mathbb{E}(u_h)$, $\mathbb{V}(A^{\text{MF}})= \sigma_1^2\sum_{k=1}^K\frac{\Delta_{k}}{N_k}$.
}



\item[$\triangleright$] \textcolor{myblue3}{\bf High- and low fidelity modes} 

{\footnotesize 
\begin{itemize}[leftmargin=5pt] 
     \item[$\circ$] {\bf High fidelity model:} Expensive but accurate 
     
     (e.g., finite element solution of Grad-Shafranov equation).
     \item[$\circ$] {\bf Low fidelity models:} Cheap but less accurate 
     
     (e.g., sparse grid stochastic collocation with 25 nodes building on the meshes coarser than the one for high fidelity model).
\end{itemize}
 }
% \item[$\triangleright$] \textcolor{myblue3}{\bf Offline cost} 

% {\footnotesize 
% Construction of low fidelity models, parameter estimation, model selection.
% }
\end{itemize}
\end{frame}


% %------------------------------------------------------------
% \begin{frame}[t]
%     \frametitle{Multi-fidelity Monte Carlo}

%     \begin{itemize}[leftmargin=5pt] 
%      \vspace{3mm}
%         \item[$\triangleright$] How It Works:
%         \begin{itemize}[leftmargin=15pt]
%             \item[$\circ$] Multiple Fidelity Levels: It leverages models of different fidelities—e.g., simplified physics-based models, surrogate models, or coarse-grid simulations.
            
%             \item[$\circ$] Control Variate Approach: The low-fidelity models act as control variates to correct and improve estimates from the expensive high-fidelity model.

%             \item[$\circ$] Optimal Sampling Strategy: MFMC determines the optimal number of simulations to run at each fidelity level to achieve the best trade-off between accuracy and cost.
%         \end{itemize}

%     \end{itemize}


% \end{frame}


%------------------------------------------------------------
\begin{frame}[t]
    \frametitle{Multi-fidelity Monte Carlo}
    {\footnotesize Let $\Delta_k = \rho_{1,k}^2 - \rho_{1,k+1}^2$ for $k = 1, \dots, K$, with $\rho_{1,K+1} = 0$.}
    \begin{itemize}[leftmargin=5pt] 
     \vspace{3mm}
        \item[$\triangleright$] The original optimization problem
        {\footnotesize
                %
        \begin{equation*}\label{eq:Optimization_pb_sample_size}
            \begin{array}{ll}
            \min \limits_{\begin{array}{c}
        \scriptstyle \alpha_2,\ldots,\alpha_K\in \mathbb{R}
        \end{array}} &\mathbb{V}\left[A^{\text{MF}}\right]=\sigma_1^2\sum_{k=1}^K\frac{\Delta_{k}}{N_k},\\
               \;\,\text{subject to} &\displaystyle\sum\limits_{k=1}^K C_kN_k=p,\\[2pt]
               &\displaystyle -N_1\le 0,\quad \displaystyle N_{k-1}-N_k\le 0, \;\; k=2\ldots,K,\\
               &N_1,\ldots, N_K\in \mathbb{R}. 
            \end{array}
        \end{equation*}
        %
        }

        \item[$\triangleright$] Our optimization problem
        {\footnotesize
                %
        \begin{equation*}\label{eq:Optimization_pb_sample_size}
            \begin{array}{ll}
            \min \limits_{\begin{array}{c}
        \scriptstyle \alpha_2,\ldots,\alpha_K\in \mathbb{R}
        \end{array}} &\sum\limits_{k=1}^K C_kN_k,\\
               \;\,\text{subject to} &\displaystyle\mathbb{V}\left[A^{\text{MF}}\right]=\epsilon_{\text{tar}}^2,\\[2pt]
               &\displaystyle -N_1\le 0,\quad \displaystyle N_{k-1}-N_k\le 0, \;\; k=2\ldots,K,\\
               &N_1,\ldots, N_K\in \mathbb{R}. 
            \end{array}
        \end{equation*}
        %
        }
    \end{itemize}
\end{frame}


%------------------------------------------------------------
\begin{frame}[t]
    \frametitle{MFMC Optimal Sample Allocation}
    % \begin{itemize}[leftmargin=5pt] 
    %  \vspace{3mm}
    %     \item[$\triangleright$] The original optimization problem
        {\fontsize{8}{8}\selectfont 
        %
        \begin{theorem}[Original problem -- Simplified sample size]
        \label{thm:Sample_size_est}
        Consider an ensemble of $K$ models $\{u_{h,k}\}_{k=1}^K$ each characterized by the standard deviation $\sigma_k$ of its output, the correlation coefficient $\rho_{1,k}$ with the highest-fidelity model $u_{h,1}$, and the computational cost per sample evaluation $C_k$. Define $\Delta_k = \rho_{1,k}^2 - \rho_{1,k+1}^2$ for $k = 1, \dots, K$, with $\rho_{1,K+1} = 0$. Assume the following conditions hold
        \vspace{-5mm}
        
        %
        \begin{alignat*}{3}
        &(i)\;\; \textit{Correlation monotonicity}: \quad && |\rho_{1,1}| > \cdots > |\rho_{1,K}|, \\ 
        &(ii)\;\; \textit{Cost-correlation ratio}: \quad && \frac{\Delta_{k}}{C_k} > \frac{\Delta_{k-1}}{C_{k-1}}, \quad k=2,\ldots,K. 
        \end{alignat*}
        %
        
        \vspace{-3mm}
        Under these assumptions, the solution to the original optimization problem  yields optimal weights $\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k}$.
        % %
        % \begin{align}
        %     % \label{eq:MFMC_coefficients}
        %     % &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\\
        %     \label{eq:MFMC_SampleSize}
        %     &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},
        % \end{align}
        % %
        define an intermediate vector $\boldsymbol{r}^* = [r_1,\ldots,r_k]^T$, then $\boldsymbol{r}^*$ and the sample sizes $N_k^*$
        \vspace{-3mm}
        %
        \[
        r_k^* = \sqrt{\frac{C_1\Delta_k}{C_k\Delta_1}},\quad N_1^* = \frac{p}{\sum_{k=1}^K C_k r^*_k}, \quad N_k^*=N_1^*r_k^*,\quad \Rightarrow \quad \JLcolor{N_k^* = \sqrt{\frac{\Delta_k}{C_k}}\frac{p}{\sum_{j=1}^K \sqrt{C_j\Delta_j}}}.
        \] 
        \vspace{-3mm}
        %
        The resulting MFMC estimator achieves a variance of
        %
        \begin{equation*}
        \label{eq:MFMC_variance_optimal}
        \mathbb{V}\left[A^{\text{MF}}\right] =
        % \frac{\sigma_1^2}{N_1^*} - \sum_{k=2}^K \left(\frac{1}{N_{k-1}^*} - \frac{1}{N_k^*}\right)\rho_{1,k}^2\sigma_1^2=
        \frac{\sigma_1^2}{p}\left(\sum_{k=1}^K\sqrt{C_k\Delta_{k}}\right)^2.
        \end{equation*}
        %
        \end{theorem}
        }

\end{frame}



% %------------------------------------------------------------
% \begin{frame}[t]
%     \frametitle{MFMC Optimal Sample Allocation}
%     % \begin{itemize}[leftmargin=5pt] 
%     %  \vspace{3mm}
%     %     \item[$\triangleright$] The original optimization problem
%         {\fontsize{8}{8}\selectfont 
%         %
%         \begin{theorem}[Original problem -- Simplified sample size]
%         \label{thm:Sample_size_est}
%         Consider an ensemble of $K$ models $\{u_{h,k}\}_{k=1}^K$ each characterized by the standard deviation $\sigma_k$ of its output, the correlation coefficient $\rho_{1,k}$ with the highest-fidelity model $u_{h,1}$, and the computational cost per sample evaluation $C_k$. Define $\Delta_k = \rho_{1,k}^2 - \rho_{1,k+1}^2$ for $k = 1, \dots, K$, with $\rho_{1,K+1} = 0$. Assume the following conditions hold
%         \vspace{-5mm}
        
%         %
%         \begin{alignat*}{3}
%         &(i)\;\; \textit{Correlation monotonicity}: \quad && |\rho_{1,1}| > \cdots > |\rho_{1,K}|, \\ 
%         &(ii)\;\; \textit{Cost-correlation ratio}: \quad && \frac{\Delta_{k}}{C_k} > \frac{\Delta_{k-1}}{C_{k-1}}, \quad k=2,\ldots,K. 
%         \end{alignat*}
%         %
        
%         \vspace{-3mm}
%         Under these assumptions, the solution to the original optimization problem  yields optimal weights $\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k}$.
%         % %
%         % \begin{align}
%         %     % \label{eq:MFMC_coefficients}
%         %     % &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\\
%         %     \label{eq:MFMC_SampleSize}
%         %     &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},
%         % \end{align}
%         % %
%         The sample sizes $N_k^*$ and variance are
%         \vspace{-3mm}
%         %
%         \[
%         \JLcolor{N_k^* = \sqrt{\frac{\Delta_k}{C_k}}\frac{p}{\sum_{j=1}^K \sqrt{C_j\Delta_j}}},\quad \mathbb{V}\left[A^{\text{MF}}\right] =
%         % \frac{\sigma_1^2}{N_1^*} - \sum_{k=2}^K \left(\frac{1}{N_{k-1}^*} - \frac{1}{N_k^*}\right)\rho_{1,k}^2\sigma_1^2=
%         \frac{\sigma_1^2}{p}\left(\sum_{k=1}^K\sqrt{C_k\Delta_{k}}\right)^2.
%         \] 
%         % \vspace{-3mm}
%         % %
%         % The resulting MFMC estimator achieves a variance of
%         % %
%         % \begin{equation*}
%         % \label{eq:MFMC_variance_optimal}
%         % \mathbb{V}\left[A^{\text{MF}}\right] =
%         % % \frac{\sigma_1^2}{N_1^*} - \sum_{k=2}^K \left(\frac{1}{N_{k-1}^*} - \frac{1}{N_k^*}\right)\rho_{1,k}^2\sigma_1^2=
%         % \frac{\sigma_1^2}{p}\left(\sum_{k=1}^K\sqrt{C_k\Delta_{k}}\right)^2.
%         % \end{equation*}
%         % %
%         \end{theorem}
%         }

% \end{frame}




%------------------------------------------------------------
\begin{frame}[t]
    \frametitle{MFMC Optimal Sample Allocation}
    % \begin{itemize}[leftmargin=5pt] 
    %  \vspace{3mm}
    %     \item[$\triangleright$] The original optimization problem
        {\fontsize{8}{8}\selectfont 
        %
        \begin{theorem}[Our problem]
        \label{thm:Sample_size_est}
        Consider an ensemble of $K$ models $\{u_{h,k}\}_{k=1}^K$ each characterized by the standard deviation $\sigma_k$ of its output, the correlation coefficient $\rho_{1,k}$ with the highest-fidelity model $u_{h,1}$, and the computational cost per sample evaluation $C_k$. Define $\Delta_k = \rho_{1,k}^2 - \rho_{1,k+1}^2$ for $k = 1, \dots, K$, with $\rho_{1,K+1} = 0$. Assume the following conditions hold
        \vspace{-5mm}
        
        %
        \begin{alignat*}{3}
        &(i)\;\; \textit{Correlation monotonicity}: \quad && |\rho_{1,1}| > \cdots > |\rho_{1,K}|, \\ 
        &(ii)\;\; \textit{Cost-correlation ratio}: \quad && \frac{\Delta_{k}}{C_k} > \frac{\Delta_{k-1}}{C_{k-1}}, \quad k=2,\ldots,K. 
        \end{alignat*}
        %
        
        \vspace{-3mm}
        Under these assumptions, the solution to the original optimization problem  yields optimal weights $\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k}$.
        % %
        % \begin{align}
        %     % \label{eq:MFMC_coefficients}
        %     % &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\\
        %     \label{eq:MFMC_SampleSize}
        %     &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},
        % \end{align}
        % %
        The sample sizes $N_k^*$ and total computational cost are
        \vspace{-3mm}
        %
        \[
        N_k^*=\frac{\sigma_1^2}{\epsilon_\text{tar}^2}\sqrt{\frac{\Delta_{k}}{C_k}}\sum_{j=1}^K\sqrt{C_j\Delta_{j}}, \quad \mathcal{W}^\text{MF} = \frac{\sigma_1^2}{\epsilon_{\text{tar}}^2}\left(\sum_{k=1}^K\sqrt{C_k\Delta_{k}}\right)^2.
        \] 
        % \vspace{-3mm}
        % %
        % The total computational cost is
        % %
        % \begin{equation*}
        % \label{eq:MFMC_variance_optimal}
        % \mathcal{W}^\text{MF} = \frac{\sigma_1^2}{\epsilon_{\text{tar}}^2}\left(\sum_{k=1}^K\sqrt{C_k\Delta_{k}}\right)^2.
        % \end{equation*}
        % %
        \end{theorem}
        }

\end{frame}

%------------------------------------------------------------
\begin{frame}[t]
    \frametitle{Two MFMC formulations}
\renewcommand{\arraystretch}{2}
\begin{table}[ht]
\centering
\scalebox{1}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
&Original& Ours\\
\hline
$\mathbb{V}\left[A^{\text{MF}}\right]$ & $\frac{\sigma_1^2}{p}\left(\sum_{k=1}^K\sqrt{C_k\Delta_{k}}\right)^2$&$\epsilon_{\text{tar}}^2$\\
\hline
$\mathcal{W}^\text{MF}$ &$p$&$\frac{\sigma_1^2}{\epsilon_{\text{tar}}^2}\left(\sum_{k=1}^K\sqrt{C_k\Delta_{k}}\right)^2$\\
\hline
$N_k$ &$\sqrt{\frac{\Delta_k}{C_k}}\frac{p}{\sum_{j=1}^K \sqrt{C_j\Delta_j}}$&$\frac{\sigma_1^2}{\epsilon_\text{tar}^2}\sqrt{\frac{\Delta_{k}}{C_k}}\sum_{j=1}^K\sqrt{C_j\Delta_{j}}$\\
\hline
\end{tabular}
}
% \caption{Sample size for a second integer optimization problem.}
\label{Tab:Sample_Size2}
\end{table}
\end{frame}



%------------------------------------------------------------
\begin{frame}[t]
    \frametitle{One example}

    \begin{itemize}[leftmargin=5pt] 
     \vspace{3mm}
        \item[$\triangleright$] Parameters
\renewcommand{\arraystretch}{2}
\begin{table}[ht]
\centering
\scalebox{0.7}{
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Model index &1 &2 &3 &4 &5 \\
\hline
Correlation coeff $\rho$ &1     &9.9977e-01   &9.9925e-01  &9.9728e-01   &9.8390e-01\\
\hline
Standard deviation $\sigma_k$ &1.0840e-02    &1.0838e-02   &1.1001e-02  &1.1549e-02   &9.5720e-03\\
\hline
Cost &73&7.0318e-03 &1.4018e-03 &5.0613e-04 &2.6803e-04\\
\hline
\end{tabular}
}
% \caption{Parameters from plasma problem.}
\label{Tab:Parameters}
\end{table}

 \item[$\triangleright$] Sample size
%
\begin{table}[ht]
\centering
\scalebox{0.52}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
&Sample size &Total cost $p$ &$\mathbb{V}\left[A^{\text{MF}}\right]$\\
\hline
Real valued &[8.8130e-01,   1.3499e+02,   5.8811e+02,   2.5409e+03, 2.1100e+04]&73.05&6.9633e-08\\
\hline
Real valued, floor &[0,   134,   588,   2540,  21100]&8.7075&8.3150e-09\\
\hline
Modified MFMC &[1,   1,   3,    13,   114]&73.0484&1.5677e-06\\
% \hline
\hline
Integer program &[1, 2, 3, 11, 97]&73.0498&1.7250e-06\\
\hline
\hline
Real valued, ceil &[1,   135,   589,   2541,  21101]&81.7167&6.9633e-08\\
% CPU time for integer program [s] & 0.27\\
\hline
\end{tabular}
}
% \caption{Sample size for real-valued optimization and integer optimization for $p=73.05$.}
\label{Tab:Sample_Size_1}
\end{table}
%
\end{itemize}
\end{frame}


%------------------------------------------------------------
\begin{frame}[t]
    \frametitle{Modified MFMC}

    
    \begin{algorithm}[H]
    {\fontsize{8}{8}\selectfont 
    \begin{algorithmic}[1]

    \REQUIRE Parameters $\rho_{1,k}$ and $C_k$ for each $\widehat u_{h, k}$, total cost $p$. Ensure the $K^*$ models $\widehat u_{h, k}$ after the model selection satisfies $p\ge \sum_i C_i$.

    \ENSURE Sample sizes $N_k$ for $K^*$ models.

    \vspace{2mm}
    
    \STATE Set the weights $\alpha_k = \frac{\rho_{1,k}\sigma_1}{\sigma_k}, \quad 1\le k\le K^*.$

    \WHILE{there is $1\le k\le K^*-1$ such that $N_k<1$}
    
    \STATE  Set $k$ equal to the first such index.\\
            $N_k\gets 1$. \\
            Set the $j$-st component of $N_j$,
            \[
            N_{j} = \sqrt{\frac{\Delta_{j}}{C_{j}}}\frac{p-\sum_{i=1}^k C_i}{\sum_{i=k+1}^{K^*} \sqrt{C_i\Delta_i}}, \qquad k+1\le j\le K^*.
            \]
    \ENDWHILE
    
    \end{algorithmic}
    \caption{Modified MFMC}
    % \label{alg:seq}
    }
    \end{algorithm}
    
\end{frame}


%------------------------------------------------------------
\begin{frame}[t]
    \frametitle{Parameter estimation}
    \begin{itemize}[leftmargin=5pt] 
    \item[$\triangleright$] Parameters to estimate: Cost $C_k$, standard deviation $\sigma_k$, correlation coefficient $\rho_{1,k}$.
    \item[$\triangleright$] The sample Pearson correlation coefficient: a biased non-linear ratio estimator.
    \end{itemize}
    $Q$: number of pilot samples,
    
    $u_k^{(i)} = u_{h,k}(\cdot, \boldsymbol{\omega}^{(i)})$: $i$-th realization of the $k$-th model,

    
%
\[
\widehat{\rho}_{1,k} = \frac{\sum_{i=1}^Q\left\langle u_{1}^{(i)} - A_{1,Q}^{\text{MC}},\; u_{k}^{(i)} - A_{k,Q}^{\text{MC}} \right\rangle_U}{\sqrt{\sum_{i=1}^Q \left\|u_{1}^{(i)} - A_{1,Q}^{\text{MC}}\right\|_U^2} \sqrt{\sum_{i=1}^Q \left\|u_{k}^{(i)} - A_{k,Q}^{\text{MC}}\right\|_U^2}},
\]
%

Even under the idealized assumption of bivariate normality, it exhibits an asymptotic bias of $\frac{\rho_{1,k}^3 - \rho_{1,k}}{2Q}$ and variance decaying as $\frac{(1 - \rho_{1,k}^2)^2}{Q}$ \cite{Fi:1915, Ha:2007, Ri:1932}, with pronounced skewness as $|\rho_{1,k}| \to 1$.

\end{frame}


%------------------------------------------------------------
\begin{frame}[t]
    \frametitle{Dynamic strategy for parameter estimation}
    %
\begin{algorithm}[H]
    {\fontsize{8}{8}\selectfont 
    \begin{algorithmic}[1]

    \REQUIRE Relative error $\vartheta$ in $\xi$, number of models $K_c$, initial sample size $Q_0$, cost vector $\boldsymbol{C} = (C_1, \dots, C_{K_c})$, stability threshold $\tau=3$.
    \ENSURE Final sample size $Q$, estimated parameters $\widehat{\boldsymbol{\sigma}},\widehat{\boldsymbol{\alpha}}, \widehat{\boldsymbol{\rho}}$, efficiency $\widehat{\xi}$, model index set $\mathcal{I}$.
    
    \vspace{1mm}
    \hrule
    \vspace{1mm}
     
    \STATE Initialization:\\

    \STATE\hspace{3mm}\textbullet~ $dQ \gets Q_0$, $p \gets 0$, $\text{converged} \gets \texttt{False}$, $\text{stable}\_\text{count} \gets 0$, $\mathcal{I}^{prev}\gets \emptyset$. %\tcp*{}
    
    \STATE\hspace{3mm}\textbullet~  Welford's initialization: $m_k^{(0)} \gets 0$, $v_k^{(0)} \gets 0$ for $k=1,\dots,K_c$,  $r_k^{(0)} \gets 0$, for $k=2,\dots,K_c$. %\tcp*{Welford's algorithm}



\WHILE{$\neg \text{converged}$ \& $\text{stable}\_\text{count}< \tau$}
    
    \FOR{$k=2,\ldots, K_c$}
    
        \FOR{$i = 1,\cdots, dQ $}
    
        \STATE $Q \gets  p+i$.

        \STATE Draw $dQ$ new samples $\{\boldsymbol{\omega}^{(i)}\}_{i=1}^{dQ}$, compute sample realization $u_{1}^{(Q)}$ and $u_{k}^{(Q)}$.\\

        \STATE Update mean $m_1^{(Q)}$, $m_k^{(Q)}$, proxies $v_1^{(Q)}$, $v_k^{(Q)}$ and $r_{k}^{(Q)}$ via Welford's algorithm.
        
        \ENDFOR
    
    \STATE Compute correlations: $\widehat\rho_{1,k}^{(Q)}\gets r_k^{(Q)}/{\sqrt{v_1^{(Q)}v_k^{(Q)}}}$ for $k=2,\ldots, K_c$. 

    
    
    % \eIf{bivariate normality holds}{
        
    %         $\widehat{z}_k \gets \tanh^{-1}\left(\widehat{\rho}_{1,k}^{(Q)}\right)$, $\sigma_{\widehat{z}_k} \gets 1/\sqrt{Q - 3}$. \\
    %         Compute confidence interval $\text{CI}_{\rho_{1,k}}^{\text{fisher}}$ and its width $t_k$ via Fisher z-transform in \eqref{eq:Confidence_Interval_fisher}.
        
    % }{
        
    %         Generate $B$ bootstrap replicates of $\widehat{\rho}_{1,k}^{(Q)}$.  \\
    %         Compute  confidence interval $\text{CI}_{\rho_{1,k}}^{\text{boot}}$ and its width $t_k$ via bootstrap in \eqref{eq:Confidence_Interval_bootstrap}.
        
    % }
    }

    \STATE $\widehat{\boldsymbol{\rho}}^{(Q)} \gets (1,\widehat{\rho}_{1,2}^{(Q)}, \dots, \widehat{\rho}_{1,K_c}^{(Q)})$.

    % $\delta \gets \frac{2\vartheta}{E\sqrt{K-1}}$
    % [$\text{index},\xi^{(p)}$] = Multi-fidelity Model Selection ($\boldsymbol{\rho}^{(p)},\boldsymbol{C}$).

    % $Q \geq \max\{30, Q_{\max \{\text{idx}^{current}\}} \}$ \text{as in } \eqref{eq:Pilot_sample_size_estimate} and
    \STATE $\left[\mathcal{I}^{cur},\widehat{\xi}^{(Q)}\right]$ $\gets$ \textsc{ModelSelectionBacktrack} $\left(\widehat{\boldsymbol{\rho}}^{(Q)},\boldsymbol{C}\right)$.\\

    \eIf{$\frac{\widehat \xi^{(Q)}- \widehat \xi^{(p)}}{\widehat \xi^{(Q)}}\leq \vartheta$, $\mathcal{I}^{prev}=\mathcal{I}^{cur}$ }{ %$\max_k t_k \leq \delta$
            $\text{converged} \gets \texttt{True}$.

            $\text{stable}\_\text{count} \gets  \text{stable}\_\text{count}+1$.
    }
    {
            $\text{stable}\_\text{count} \gets  0$.
    
        $p \gets  p+dQ$. %\tcc*{Increase sample size.}
        
        $\widehat{\xi}^{(p)} \gets  \widehat{\xi}^{(Q)}$.
    }
    

    $\mathcal{I}^{prev} \gets \mathcal{I}^{cur}$.

    \ENDWHILE
    
$\widehat{\sigma}_1 \gets \sqrt{v_1^{(Q)}}$, $\widehat{\sigma}_k \gets \sqrt{v_k^{(Q)}}$, $\widehat{\alpha}_k \gets \widehat{\rho}_{1,k}^{(Q)} \widehat{\sigma}_1^{(Q)} / \widehat{\sigma}_k^{(Q)}$, for $k \in \mathcal{I}^{cur}$.

Return $\widehat{\boldsymbol{\sigma}} \gets [\widehat{\sigma}_1,\ldots,\widehat{\sigma}_{K}]$, $\widehat{\boldsymbol{\alpha}} \gets [1,\widehat{\sigma}_2\ldots,\widehat{\sigma}_{K}]$, $\widehat{\boldsymbol{\rho}} \gets \widehat{\boldsymbol{\rho}}^{(Q)}$, $\widehat{\xi} \gets \widehat{\xi}^{(Q)}$, $\mathcal{I} \gets \mathcal{I}^{cur}$.

    \end{algorithmic}
    \caption{Dynamic Parameter Estimation}
    }
    \end{algorithm}

\end{frame}





\begin{frame}
\frametitle{My Long Algorithm - Part 1}
\begin{algorithm}[H]
    \caption{Example Long Algorithm}
    % \label{alg:long_example}
    \begin{algorithmic}[1]
        \State Initialize variables
        \For{$i=1$ to $N$}
            \State Do something
            \State Perform calculations
            \State Store intermediate results
        \EndFor
        \State Check condition
    \end{algorithmic}
\end{algorithm}
\end{frame}

\begin{frame}
\frametitle{My Long Algorithm - Part 2}
\begin{algorithm}[H]
    \ContinuedFloat % Continues the numbering from the previous algorithm environment
    \caption{Example Long Algorithm (cont.)}
    \begin{algorithmic}[1]
        \State Continue processing
        \If{condition is met}
            \State Execute specific steps
        \Else
            \State Handle alternative case
        \EndIf
        \State Finalize results
        \State Return output
    \end{algorithmic}
\end{algorithm}
\end{frame}



%------------------------------------------------------------
\begin{frame}[t]
    \frametitle{Modified MFMC}

\end{frame}


%------------------------------------------------------------
\begin{frame}[t]
    \frametitle{Modified MFMC}
\end{frame}
% \begin{frame}[t]{stellar}
% \frametitle{Necessary conditions -- $t_f$ free}
% {\footnotesize
% \begin{itemize}[leftmargin=5pt] 
%     \item[] 
%     \begin{align*}
%     \text{Lagrangian} \quad &J = \varphi + \nu_b^Tb + \nu_p^T p + \nu_s^T s+\\ 
%     &\int_{t_0}^{t_f}  L + \lambda^t \left[f - \dot{x}\right] 
%     \;dt\\
%         \text{Hamiltonian} \quad &H(t, x, u, \lambda) = L(t, x, u) + \lambda(t)^T f(t, x, u),\\
%          \text{Control} \quad &u^* = \operatorname*{argmin}_{u\in \mathcal{U}}\, H(t, x^*, u,\lambda^*),\quad \forall t\in[t_0, t_f],\\
%         \text{State} \quad &\dot{x} = \frac{\partial H}{\partial \lambda},\\
%         \text{Adjoint equation} \quad &\dot{\lambda} = -\frac{\partial H}{\partial x},\\
%         \text{T ransversality cond} \quad &\lambda (t_f) = 
%     \end{align*}
% \end{itemize}
% }	
% \end{frame}


%}
%=================================================
% Imagine two positive nuclei in my hand, two forces acting on them: nuclear force (attract them) and coulub force(push them aways), when at big distance, nuclei repel each other, when find ways to conquer the coulomb force barrier, two neulei get closed to each other and merge into one heavier neulcei. mass defect release energy. example: sun. 

%Tokamak: russian word--toroidal chamber with magnetic coils. see below two figures. Inject fuel gas into the chamber and heat with microwave, becomes fluid of particles -- called plasma, high temperature, vibrate fast, high speed easy to overcome coulum repulsion and get closed to each other, and collide to produce energy.
%=================================================
% Q: the distance of repel & attract?


% %------------------------------------------------------------
% \begin{frame}[t]
% \frametitle{Plasma Confinement}
% %		\begin{columns}
% %			\begin{column}{0.8\textwidth}

% During fusion, the hot plasma tends to expand, to prevent it from contacting the reactor wall, it must be confined. 

% \vspace{4mm}
% {\fontsize{13}{13}\selectfont \textcolor{myblue3}{\bf Question: How to maintain confinement of hot plasma?} }

% \vspace{2mm}
% \begin{itemize} [leftmargin=5pt] 
% \vspace{-3mm}
% \item[$\triangleright$] Since plasmas are conductive and magnetically tractable -- use a \textcolor{myred}{\bf magnetic field}  generated by currents in  external coils around the fusion reactor.
% \vspace{1.8mm}

% %When heated to fusion temperatures, the electrons in atoms disassociate, resulting in a fluid of nuclei and electrons known as a plasma. Unlike electrically neutral atoms, a plasma is electrically conductive, and can, therefore, be manipulated by electrical or magnetic fields.
% \item[$\triangleright$] Our project focuses on studying the \textcolor{myred}{\bf impact of variations in current} on the physical properties of the confinement field.


% \begin{equation*}
% \hspace{-11mm}
% \text{Variations}
% \left\{ \begin{array}{l}
% \text{Power supply}\\ [3pt]
% \text{Temperature fluctuations}\\ [3pt]
% \text{Material impurities in the conduction wire}
% \end{array}\right.
% \end{equation*}


% \item []
% %\begin{figure}[H]	
% %	\centering
% %	\begin{tabular}{cc}
% %		\begin{subfigure}{0.45\textwidth}
% %			\includegraphics[height=0.55\linewidth]{../../../Project/Plot/BasicConfig}
% %%			\caption*{Divertor configuration}
% %		\end{subfigure}
% %		&
% %		\begin{subfigure}{0.45\textwidth}
% %			\includegraphics[height=0.55\linewidth]{../PictureFolder/PlasmaImage}
% %			\source{https://en.wikipedia.org/wiki/Mega{\_}Ampere{\_}Spherical{\_}Tokamak}
% %%			\caption*{Limiter configuration}
% %		\end{subfigure}
% %	\end{tabular} 
% %\end{figure}
% \end{itemize}
% \end{frame}



%=================================================
% We impose the bottom two conditions. if x is 0, then the equation becomes singular, so the first condition avoid singularity in the equation, and the second equaiton shows the decay of function at infinity, and ensure the uniqueness of the solution.
%=================================================

% %------------------------------------------------------------
% \begin{frame}[t]
%     \frametitle{Uncertainty Quantification (UQ)}
%     \begin{itemize}[leftmargin=5pt]
%         \item[$\triangleright$] \textcolor{myblue3}{\bf Source of uncertainty:} {\footnotesize Investigate the impact of \textcolor{myblue3}{uncertainties in the current intensities} on the confinement properties of a plasma.}
        
%         \item[$\triangleright$] \textcolor{myred}{\bf Baseline current intensities:} 
%         \[
%         \footnotesize
%         \begin{array}{lll}
%         I_1: -1.40\times 10^6\text{A} &I_5: -9.00\times 10^6\text{A} &I_9:  -6.43\times 10^6\text{A} \\
%         I_2: -9.50\times 10^6\text{A} &I_6: 3.56\times 10^6\text{A}  & I_{10}:  -4.82\times 10^6\text{A} \\
%         I_3: -2.04\times 10^7\text{A} &I_7:  5.47\times 10^6\text{A}  &I_{11}:  -7.50\times 10^6\text{A} \\
%         I_4: -2.04\times 10^7\text{A} &I_8: -2.27\times 10^6\text{A} &I_{12}:  1.72\times 10^7\text{A} 
%         \end{array}
%         \]
%         \item[$\triangleright$] \textcolor{myblue3}{\bf Uncertianty Quantification:} {\footnotesize Variations in the baseline current intensities. Each current in the array is considered to be a parameter, so we have 12-dimensional parameter space.}
        
        
%         \item[$\triangleright$] \textcolor{myblue3}{\bf Currents uniformly distributed:} 

%         {\footnotesize
%         \textcolor{mygray3}{$\tau$: perturbation level ($\tau= 1\%$ \& $2\%$). \quad $I_k$: current intensity in the $k$-th coil.}
        

%         \begin{itemize}[leftmargin=15pt]   
%             \item[$\circ$] Joint density function: $\displaystyle
%              \pi \left(\boldsymbol{\omega}\right)=\prod_{k=1}^{d} \pi_k\left(\omega_{k}\right)=\prod_{k=1}^{d} \frac{1}{2\tau |I_k|}$.
%             \item[$\circ$] 12-d parameter space: $\displaystyle W := \prod_{k=1}^{d}\left[I_k-\tau|I_k|,I_k+\tau|I_k|\right]$.
%         \end{itemize}
%         \par}
%     \end{itemize}
% \end{frame}	
%=================================================
% here I list an example of an array of currents, each current in the array is considered to be a parameter, so we have 12 dimensional parameter space, and if we use 1%...., we will use MC method to handle the uncertainty.
%=================================================











% %------------------------------------------------------------
% \begin{frame}[t]
%     \frametitle{Random space: from infinite to finite dimension}
    
%     \begin{itemize}[leftmargin=5pt] 
%             \item[$\triangleright$] \textcolor{myblue3}{\bf Finite dimensional noise assumption:}  
            
%         {\footnotesize
%         To solve the problem numerically, we need to reduce the infinite-dimensional space to a finite-dimensional space. This is often achieved via a certain type of decomposition which can approximate the target random process with desired accuracy.

%         Eg. Karhunen-Loève type expansion. 
%         }

%         \item[$\triangleright$] \textcolor{myblue3}{\bf Result:} 
        
%         {\footnotesize
%         The random inputs can be characterized by $d$ random variables.

%         \begin{equation*}
%             \left\{ \begin{array}{ll}
%             \mathcal{L}(t, x) = f(t, x, \omega) = f(t, x, Y^1(\omega), \cdots, Y^{d}(\omega)) & t \in D\\
%             \mathcal{B}(t, x) = g(t)& t \in \partial D
%             \end{array}\right.
%         \end{equation*}
%         where $\{Y^k\}_{k=1}^{d}$ are real-valued random variables with zero mean and unit variance. Each is mutually independent with probability density $\pi_k: \Gamma^k\rightarrow \mathbb{R}^+$ with image $\Gamma^k = Y^k(\omega)$ are bounded intervals in $\mathbb{R}$ for all $k$. Let $\boldsymbol{y}=(Y^1, \cdots Y^{d})$. The joint density and support of $\boldsymbol{y}$ are 
%         \begin{align*}
%              \pi \left(\boldsymbol{y}\right)=\prod_{k=1}^{d} \pi_k(Y^{k}), \quad W = \prod_{k=1}^{d} \Gamma^k.
%         \end{align*}

        
%         We assume this equation admits a unique solution $x^{d} = x^{d}(t, \omega) = x^d(t, Y^1(\omega), \cdots, Y^d(\omega))$. 
%         }
%     \end{itemize}
% \end{frame}

% %------------------------------------------------------------
% \begin{frame}[t]
%     \frametitle{Random space: from infinite to finite dimension}
    
%     \begin{itemize}[leftmargin=5pt] 

%         \item[$\triangleright$]
        
%         {\footnotesize
%         This allows to rewrite the problem as $(d+d_D)$ dimensional differential equation:

%         \begin{equation*}
%             \left\{ \begin{array}{ll}
%             \mathcal{L}(t, x) = f(t, x, \boldsymbol{y}) & (\boldsymbol{y}, t) \in W\times D\\
%             \mathcal{B}(t, x) = g(t)& (\boldsymbol{y}, t) \in W\times \partial D
%             \end{array}\right.
%         \end{equation*}
%         where $d$ and $d_D$ are the dimension of random space $W$ and physical space $D$ respectively.

%         Our goal is to approximate the function $x^d = x^d(t, \boldsymbol{y})$ for any $\boldsymbol{y}\in W$ and $t\in D$.
%         }
%     \end{itemize}
% \end{frame}



% %------------------------------------------------------------
% \begin{frame}[t]
%     \frametitle{Physical space: from infinite to finite dimension}
    
%     \begin{itemize}[leftmargin=5pt] 

%         \item[$\triangleright$]
        
%         {\footnotesize
%         Next, we approximate the problem in a finite element subspace.

%         \vspace{2mm}
%         Let $\widehat{\pi}$ be a finite element operator $\widehat{\pi}: Z\rightarrow Z_h$ with the optimality condition
%         \[
%         \|\varphi - \widehat{\pi}_h \varphi\|_Z\le C_{\widehat{\pi}} \min_{v\in Z_h}\|\varphi - v\|_Z, \quad \forall \varphi\in Z,
%         \]
%         where $C_{\widehat{\pi}}$ is independent of mesh size $h$.

%         \vspace{3mm}
%         Let $Z_h\subset Z$ be a standard FE space of dimension $N_h$. It contains continuous piecewise polynomials defined on regular triangulations $\mathcal{T}_h$ that have a maximum mesh parameter $h>0$. $Z_h$ has the following deterministic approximate property:

%         For a given function $\varphi \in Z$,
%         \[
%         \min_{v\in Z_h} \|\varphi - v\|_Z \le C(s;\varphi) h^s,
%         \]
%         where $s$ is a positive integer determined by the smoothness of $\varphi$ and the degree of the approximating finite element subspace.

%         $C(s;\varphi)$ is independent of $h$.
%         }
%     \end{itemize}
% \end{frame}







% %------------------------------------------------------------
% \begin{frame}[t]
%     \frametitle{Interpolation error}
    
%     \begin{itemize}[leftmargin=5pt] 
%     % \item[$\triangleright$] a 'function space' is a collection of functions with similar properties, where each function in the space is considered as an element, allowing operations like addition and scalar multiplication to be performed on them, often with restrictions based on the desired properties (like continuity or differentiability) of the functions within that space
    
%     \item[$\triangleright$] \textcolor{myblue3}{For functions in the space $F_d^k$}
%     \[
%     F_d^k = \left\{ f: [-1, 1]^d\rightarrow \mathbb{R}\big |\quad \partial^{|\boldsymbol{m}|}f \text{ continuous if } m_i\le k\; \forall i\right\}
%     \]
%     $\partial^{|\boldsymbol{m}|}$: d-variate partial derivative of order $|\boldsymbol{m}|$.
% \item[$\triangleright$]  For our problem, the fr\'echet derivative of x with respect to $\omega$ is twice continuously differentiable
    
%     \end{itemize}

    
%     \fbox{%
%     \parbox{1.01\textwidth}{
%     \textcolor{myblue3}{\bf Theorem:} {\footnotesize Interpolation error bound in $L_\infty$ norm
%     \vspace{-2mm}
%     \[\|(I_d-\mathscr{S})(f)\|_\infty = \mathscr{O}\left(N^{-k}\cdot \vert \log N\vert ^{(k+2)(d-1) +1}\right)\vspace{-2mm}\]
%     $N$: \# of collocation nodes, $d$: dimension, $k$: measure of smoothness of $f$ wrt $\boldsymbol{\xi}$.}}}

% \end{frame}


% %------------------------------------------------------------
% \begin{frame}[t]
%     \frametitle{Stochastic collocation}
%     {\footnotesize
%     \begin{itemize}[leftmargin=5pt] 
%             \item[$\triangleright$]  For our problem, to estimate trajectory, 
            
%             $x: D\rightarrow \mathbb{R}$, $Z = W_{1, \infty}(D)$. 

%             The fr\'echet derivative of x with respect to $\omega$ is twice continuously differentiable.
        
        
%     \end{itemize}
%     }
% \end{frame}


% %------------------------------------------------------------
% \begin{frame}[t]
%     \frametitle{Stochastic collocation}
%     {\footnotesize
%     \begin{itemize}[leftmargin=5pt] 
%             \item[$\triangleright$]  Sparse grid nodes: Chebyshev Gauss-Lobatto nodes $X^i = \{x_1^i, \cdots, x_{m_i}^i\}, i\in \mathbb{N}$.
%             {\footnotesize
%             \begin{itemize}[leftmargin=15pt]
%             \item[$\circ$] Nodes are nested $X^i\subset X^{i+1}$, include the end points of the interval.
%             \item[$\circ$] 
%             \begin{align*}
%                 m_i &= \left\{ \begin{array}{ll}
%                 1  &\text{if } i = 1,\\
%                 2^{i-1}+1 &\text{if } i > 1.
%                 \end{array}\right.
%                 \\
%                 X^i_j &= \left\{ \begin{array}{ll}
%                 \frac{1}{2}\left(-\cos\left(\frac{j-1}{m_i-1}\pi\right)+1\right)  &\text{for } j = 1, \cdots, m_i, \text{ if } m_i>1, \\
%                 \frac{1}{2} &\text{for } j=1, \text{ if } m_i = 1.
%                 \end{array}\right.
%             \end{align*}
%             \item[$\circ$] These are the roots of the Chebyshev polynomials of the second kind with degree $m_i$.
%             \end{itemize}
%             }
        
        
%     \end{itemize}
%     }
% \end{frame}




% %------------------------------------------------------------
% \begin{frame}[t]
%     \frametitle{Uncertainty quantification: Drag and Lift coefficient}
    
%     \begin{itemize}[leftmargin=5pt] 
    
%     \item[$\triangleright$] \textcolor{myblue3}{\bf Lift and drag forces:} {\footnotesize $\text{Lift: } L = \frac{1}{2}C_L \rho V^2S, \quad \text{Drag: } D=\frac{1}{2}C_D \rho V^2S $,}
    
%     \begin{align*}
%         \text{Atmospheric density:} \quad &\rho(y) = 1.225\times 10^9e^{-0.14y}\;\;[kg/m^3] \\
%         \text{Altitude:} \quad &y \;\; [m]\\
%         \text{Total velocity of vehicle:} \quad &V\;\; [m/s]\\
%         \text{Dynamic pressure:} \quad &\frac{1}{2} \rho V^2 \;\; [Pa]  \\
%         \text{Wing area:} \quad &S = 6\times 10^{-6} \;\; [m^2] \\
%         \text{Lift coefficient:} \quad &C_L = -0.04 + 0.8\alpha \\
%         \text{Drag coefficient:} \quad &C_D = 0.012 - 0.01\alpha + 0.6\alpha^2 \\
%         \text{Angle of attack:} \quad &\alpha \;\; [^\circ]
%     \end{align*}
    
%     % \item[$\triangleright$] \textcolor{myblue3}{\bf Drag:} {\footnotesize.}
    
%     % \item[$\triangleright$] \textcolor{myblue3}{\bf Efficiency:} {\footnotesize The use of surrogates reduces the CPU time required for the Monte Carlo .}
    
%     \end{itemize}
% Note: $C_L$ and $C_D$ only depend on the angle of attack.

% The drag coefficient is a measure of the drag force experienced by an object moving through a fluid. It is determined by factors such as the shape and size of the object and the properties of the fluid.

% \end{frame}





% %------------------------------------------------------------
\begin{frame}[t]
    \frametitle{Sparse Grid Stochastic Collocation}
\begin{itemize}[leftmargin=5pt] 

\item[$\triangleright$] Build surrogate via \textcolor{myblue3}{\bf Sparse Grid Stochastic Collocation}.
\vspace{2mm}
    \begin{itemize}[leftmargin=7pt] 
        \item [$\circ$] {\bf Idea:} {\footnotesize Select a special set of current  $\{\boldsymbol{\omega}^{(k)}\}_{1 \le k\le n_{sc}}$ in the parameter space. For each $\boldsymbol{\omega}^{(k)}$, evaluate the nonlinear solver to yield the realization $\{\psi_h^{(k)}\}$. An interpolation (surrogate) is then constructed to mimic the original nonlinear function with 
        \vspace{-3mm}
        $$\hspace{15mm}\widehat{\psi_h}(\cdot, \boldsymbol{\omega}) = \sum_k\psi_h^{(k)}(\cdot)L_{\boldsymbol{\omega}^{(k)}}(\boldsymbol{\omega}).$$
        \par}
    
        \vspace{2mm}
        \item [$\circ$] {\bf Sparse grids}: 
        \vspace{-8mm}
        \begin{figure}[H]	
        \centering
        \hspace{19mm}
        \begin{tabular}{ccc}
            \includegraphics[width=0.2\linewidth]{./figures_slides/full_grid_2d}
            &
            \includegraphics[width=0.2\linewidth]{./figures_slides/sparse_grid_2d}
            &
            \includegraphics[width=0.2\linewidth]{./figures_slides/sparse_grid_3d}
        \end{tabular}
        \vspace{-3mm}
        \caption*{{\fontsize{8}{8}\selectfont  Left to right: Full tensor grid 2d, level 4. Chebyshev sparse grids for 2d and 3d from level 0 to level 4. \par}}
        \end{figure}

        \vspace{-6mm}
        \fbox{%
        \parbox{1.01\textwidth}{
        \textcolor{myblue3}{\bf Theorem:} {\footnotesize Interpolation error bound in $L_\infty$ norm
        \vspace{-2mm}
        \[\|f-\mathscr{S}(f)\|_\infty = \mathscr{O}\left(N^{-k}\cdot \vert \log N\vert ^{(k+2)(d-1) +1}\right)\vspace{-2mm}\]
        $N$: \# of collocation nodes, $d$: dimension, $k$: measure of smoothness of $\psi$ wrt $\boldsymbol{\omega}$.}}}

    
        
        % {\fontsize{8}{8}\selectfont  \textcolor{mygray2}{V. Barthelmann, E. Novak, and K. Ritter. High dimensional polynomial interpolation on sparse grids. Advances in Computational Mathematics, 12:273–288, 2000.}
        % \par}

        \vspace{2mm}
        \item[$\circ$] {\bf Software:} {\footnotesize \textsc{Matlab} {\tt SPINTERP}  package -- A.Klimke.}
        
        \vspace{1mm}
        {\fontsize{8}{8}\selectfont \textcolor{mygray2}{SPINTERP, piecewise multilinear hierarchical sparse grid interpolation, http://people.math.sc.edu/burkardt/msrc/spinterp/spinterp.html, 2007.}
        \par}
        % \item [$\circ$] \textcolor{myblue3}{\bf Reference}: 
        % {\footnotesize V. Barthelmann and his collaborators.\par}
        
    
    \end{itemize}

\end{itemize}
\end{frame}





% %------------------------------------------------------------
% \begin{frame}[t]
%     \frametitle{Numerical Results}

%     {\footnotesize
%     \begin{itemize}[leftmargin=5pt] 
%             \item[$\triangleright$] {\bf Software:} {\footnotesize \textsc{Python} {\tt Chaospy}  toolbox -- J. Feinberg, \& H. Langtangen.}

%         Analyzing uncertainty using advanced Monte Carlo simulation and non-intrusive polynomial chaos expansions
        
%         \vspace{1mm}
%         {\fontsize{8}{8}\selectfont \textcolor{mygray2}{Chaospy: an open source tool for designing methods of uncertainty quantification. Journal of Computational Science, 11(2015).}
%         \par}
        
%         \item[$\triangleright$] Introduce two non-intrusive methods: {\bf Pseudo-spectral projection} \& {\bf point collocation}.
        
%         % \item[$\triangleright$] {\bf Pseudo-spectral projection:} apply a numerical integration scheme to estimate Fourier coefficients.

%         % Pros:  we can use sparse grid stochastic collocation to approximate the mean.
            
%         % \item[$\triangleright$] {\bf Point collocation:} collocation nodes can be arbitrary.

                
%         \item[$\triangleright$] Cons of this toolbox:

%         It does not allow secondary uncertainty analysis through MC simulation to yield more statistical metrics.
            
        
        
%     \end{itemize}
%     }
% \end{frame}









%------------------------------------------------------------
\begin{frame}[t]
    \frametitle{Numerical results}

\end{frame}



%------------------------------------------------------------
\begin{frame}[t]
    \frametitle{Numerical results}
\begin{itemize}[leftmargin=5pt] 
%

\item[$\triangleright$] \textcolor{myblue3}{\bf Efficiency:} {\footnotesize 
 Multi-fidelity Monte Carlo reduces the computational cost of Monte Carlo with direct non-linear solve by up to  $\boldsymbol{10^3}$ ($\sim \epsilon^{-1}$). However,  when accounting for {\bf upfront costs} such as surrogate construction and parameter estimation, the overall gain is limited to a factor of {\bf 50}.}
 \vspace{3mm}
\begin{figure}[ht!]\centering
\scalebox{0.9}{
\begin{tabular}{cc}
% \includegraphics[height=0.35\linewidth]{CPUtime_epsilon_L2norm.pdf}&
\includegraphics[height=0.4\linewidth]{./figures_slides/Cost_epsilon.pdf}
\end{tabular}
}
% \caption{Left: sampling cost (CPU time in seconds) of direct computation vs. tolerance $\epsilon$ for MC, MLMC with geometry-conforming uniform meshes, and MLMC with adaptive meshes.  Right: sampling cost vs. tolerance $\epsilon$ for MC with both direct computation and surrogate, MLMC with both direct computation and surrogate.} 
\label{fig:Experiment_result_plot}
\end{figure}

\end{itemize}
\end{frame}




%------------------------------------------------------------
\begin{frame}[t]
    \frametitle{Numerical results}
\begin{itemize}[leftmargin=5pt] 
% \item[$\triangleright$] \textcolor{myblue3}{\bf Surrogate-enhanced multilevel Monte Carlo:} {\footnotesize Integrate surrogate with multilevel Monte Carlo.}
\item[$\triangleright$] \textcolor{myblue3}{\bf Accuracy:} 
{\footnotesize Plasma boundary and geometric descriptors from MFMC sampling exhibit behavior consistent with those from MC and MLMC when interpolated onto a common mesh.}
% \item[$\triangleright$] \textcolor{myblue3}{\bf Efficiency:} {\footnotesize 
%  Su.}
%  \vspace{3mm}
\begin{figure}[ht!]\centering
\scalebox{0.15}{
\begin{tabular}{cccc}
\includegraphics[width=1\linewidth]{./figures_slides/QoI_MC_uniform.pdf}
&\includegraphics[width=1\linewidth]{./figures_slides/QoI_MLMC_DirectSolver_Interp2CommonGrid.pdf}
&\includegraphics[width=1\linewidth]{./figures_slides/QoI_MFMC.pdf} 
% &\includegraphics[width=1\linewidth]{QoI_MLMC_surrogate.pdf} 
% &\includegraphics[width=1\linewidth]{QoI_MLMC_DirectSolver_Interp2CommonGrid.pdf} 
% &\includegraphics[width=1\linewidth]{QoI_MLMC_surrogate_Interp2CommonGrid.pdf} 
\\
\includegraphics[width=1\linewidth]{./figures_slides/QoI_MC_uniform_xptRegion.pdf} 
&\includegraphics[width=1\linewidth]{./figures_slides/QoI_MLMC_DirectSolver_xptRegion_Interp2CommonGrid.pdf} 
&\includegraphics[width=1\linewidth]{./figures_slides/QoI_MFMC_xptRegion.pdf} 
% &\includegraphics[width=1\linewidth]{QoI_MLMC_surrogate_xptRegion.pdf} 
% &\includegraphics[width=1\linewidth]{QoI_MLMC_DirectSolver_xptRegion_Interp2CommonGrid.pdf}
% &\includegraphics[width=1\linewidth]{QoI_MLMC_surrogate_xptRegion_Interp2CommonGrid.pdf}
\\[1ex]
\quad {\fontsize{35}{35}\selectfont MC-FE} & {\fontsize{35}{35}\selectfont MLMC-FE (Interp)}  &{\fontsize{35}{35}\selectfont MFMC-FE (Interp)}
% & {\fontsize{35}{35}\selectfont MLMC + Direct + interp} 
% &{\fontsize{35}{35}\selectfont MLMC + Surrogate + interp} 
\\[-0.5ex]
\end{tabular}
}
\end{figure}


%
% \vspace{1mm}
% \item[$\triangleright$] {\bf This project is in preparation.}
% \vspace{1mm}
        
% {\fontsize{8}{8}\selectfont \textcolor{mygray2}{In preparation.}
% \par}
\end{itemize}
\end{frame}







%-----------------------------------------------------------
\begin{frame}
% \frametitle{$|\!\!\sim\! \underline{\hspace{0.25cm}}\!\sim\!\! |@ \underline{\hspace{0.25cm}} @|\!* \!\underline{\hspace{0.25cm}}\!*\!|. \underline{\hspace{0.25cm}} .|- \underline{\hspace{0.25cm}} -|G \underline{\hspace{0.25cm}} G|x\underline{\hspace{0.25cm}} x|/\underline{\hspace{0.25cm}}\backslash|w_w|Q\underline{\hspace{0.25cm}}Q|>\underline{\hspace{0.25cm}}>|=\underline{\hspace{0.25cm}}=|+\underline{\hspace{0.25cm}}+|\#\underline{\hspace{0.25cm}}\#|z\underline{\hspace{0.25cm}}z|>\underline{\hspace{0.25cm}}<|6\underline{\hspace{0.25cm}}9|$}


	\begin{center}
	{\fontsize{35}{70}\selectfont  Thank You $|\neg\underline{\hspace{0.4cm}}\neg|$}
	\end{center}
\end{frame}
%----------------------------------------------------------------
\end{document}