\documentclass[final,3p,times,11pt]{elsarticle}
\usepackage[USenglish]{babel}
\usepackage{amsmath,amssymb,amsthm, mathrsfs}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{stmaryrd}
\usepackage[dvipsnames]{xcolor}
\usepackage{cancel}
\usepackage{ulem}
\usepackage{tabularx}
\usepackage{comment}
%\usepackage{subcaption}
%\usepackage[show]{ed}
%\usepackage{showkeys}
%\usepackage{showlabels}
%\usepackage[notcite,notref]{showkeys}
%\usepackage{refcheck}
% \usepackage[ruled,vlined]{algorithm2e}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\definecolor{Myblue}{rgb}{.2 0.4 1}

\usepackage{hyperref}
\hypersetup{
    %bookmarks=true,         % show bookmarks bar?
    colorlinks = true,       % false: boxed links; true: colored links
    % linkcolor=green
     %linkcolor=red,          % color of internal links (change box color with linkbordercolor)
     %citecolor=green,        % color of links to bibliography
    %filecolor=magenta,      % color of file links
    %urlcolor=cyan           % color of external links
}
%\usepackage{wrapfig}
%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%\usepackage{lineno}


% ==============   Macros  ====================
\newcommand{\mynabla}{\widetilde{\nabla}} 
\newcommand{\jump}[1]{[\![#1]\!]}
\newcommand{\HEcolor}[1]{{\textcolor{blue}{#1}}}
\newcommand{\TSVcolor}[1]{{\textcolor{orange}{#1}}}
\newcommand{\JLcolor}[1]{{\textcolor{violet}{#1}}} %violet
\newcommand{\Grids}{\boldsymbol{\chi}}

\newtheorem{theorem}{Theorem}%[section]
\newtheorem{VariationalForm}[theorem]{Variational Formulation}
% =============================================

\journal{}
\makeatletter
\def\ps@pprintTitle{%
 \let\@oddhead\@empty
 \let\@evenhead\@empty
 \def\@oddfoot{}%
 \let\@evenfoot\@oddfoot}
\makeatother




\begin{document}
\begin{frontmatter}
\title{Multifidelity write-up}
\begin{abstract}
We derived the explicit sample size estimation in terms of the desired accuracy requirement.
\end{abstract}
\end{frontmatter}



\section{Model problem}\label{sec:Problem_setup}
We have a high fidelity model denoted as $f_1: \Omega \rightarrow Z$ we desire, and several low fidelity models (surrogates) $f_k$ for $k\in \mathbb{N}$. Our objective is to approximate 
\[
\mathbb{E}\left(f_1(\boldsymbol{\omega})\right).
\]
Note for each $f_i(\boldsymbol{\omega})$, its variance and Pearson product-moment correlation coefficient are 
\begin{equation*}
    \sigma_i^2 = \mathbb{V}\left(f_i(\boldsymbol{\omega})\right),\qquad \rho_{i,j} = \frac{\text{Cov}\left(f_i(\boldsymbol{\omega}),f_j(\boldsymbol{\omega})\right)}{\sigma_i\sigma_j}, \quad i,j=1,\dots, K,
\end{equation*}
where $\mathbb{V}(f) := \mathbb{E}\left(\left\Vert f - \mathbb{E}(f)\right\Vert_Z^2\right)$.


\section{Monte Carlo estimator}
The Monte Carlo estimator for the expectation of each $f_k$ is defined as the sample mean of $N_k$ i.i.d realizations $\boldsymbol{\omega}_1,\ldots,\boldsymbol{\omega}_N$
\begin{equation}\label{eq:MC_estimator}
    A^{\text{MC}}_{k,N_k} := \frac{1}{N_k}\sum_{i=1}^{N_k} f_k(\boldsymbol{\omega}_i),\quad \forall k=1,\ldots, K,
\end{equation}
where $\mathbb{E}(A^{\text{MC}}_{k,N_k}) = \mathbb{E}(f_k)$, $\mathbb{V}(A^{\text{MC}}_{k,N_k}) = \mathbb{V}(f_k)/{N_k}$. Let $C_k$ denote the average evaluation cost per sample for $f_k$, then the total sampling cost for each Monte Carlo estimator $A^{\text{MC}}_{k,N_k}$ is 
\[
\mathcal{W}_\text{MC}^k  = C_kN_k.
\]
We define the \textit{normalized mean squared error}  (nMSE), denoted as $\mathcal{E}_{A}^2$, with normalizing factor $\left\Vert\mathbb{E}(f) \right\Vert_{Z}^2$ for estimator $A$ as
 \[
\mathcal{E}_{A}^2:=\frac{\mathbb E\left[\left\Vert\mathbb{E}(f)-A \right\Vert_{Z}^2\right]}{\left\Vert\mathbb{E}(f) \right\Vert_{Z}^2}.
\] 
If we use a Monte Carlo estimator to estimate $\mathbb{E}\left(f_1(\boldsymbol{\omega})\right)$, 
\[
\mathcal{E}_{A^{\text{MC}}_{1,N_1}}^2=\frac{\mathbb E\left[\left\Vert\mathbb{E}(f_1)-A^{\text{MC}}_{1,N_1} \right\Vert_{Z}^2\right]}{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2} = \frac{\left\Vert\mathbb{E}(f_1)-\mathbb{E}(A^{\text{MC}}_{1,N_1}) \right\Vert_{Z}^2+\mathbb E\left[\left\Vert\mathbb{E}(A^{\text{MC}}_{1,N_1})-A^{\text{MC}}_{1,N_1} \right\Vert_{Z}^2\right]}{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2} = \frac{\mathbb{V}\left(f_1\right)}{N_1\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2}.
\]
Given a target tolerance $\epsilon^2$ for the nMSE, the sample size $N_1$ is estimated as 
\[
N_1 = \left\lceil \frac{\mathbb{V}\left(f_k\right)}{\epsilon^2\left\Vert\mathbb{E}(f_k) \right\Vert_{Z}^2}\right\rceil\simeq \epsilon^{-2}.
\]
So the total expense of sampling with $N_1$ samples using Monte Carlo method for $\mathbb{E}\left(f_1(\boldsymbol{\omega})\right)$ is 
\[
\mathcal{W}_\text{MC}^1  = C_1N_1.
\]

\section{Multifidelity Monte Carlo}
The Multifidelity Monte Carlo (MFMC) estimator is defined as
\begin{equation}\label{eq:MFMC_estimator}
    A_{\text{MFMC}} := A^{\text{MC}}_{1,N_1} + \sum_{k=2}^K \alpha_k\left(A^{\text{MC}}_{k,N_k} - A^{\text{MC}}_{k,N_{k-1}} \right),
\end{equation}
where $\alpha_k$ are the coefficients to weight the correction term. In each correction term, the two Monte Carlo estimators are dependent in the sense that $A^{\text{MC}}_{k,N_{k-1}}$ recycles the first $N_{k-1}$ samples of $A^{\text{MC}}_{k,N_{k}}$ so we require $N_{k-1}\le N_k$ for $k=2,\ldots,K$. Using this property and \eqref{eq:MC_estimator}, we can remove the dependent samples and rewrite \eqref{eq:MFMC_estimator} as
\[
A^{\text{MFMC}} = A^{\text{MC}}_{1,N_1} +  \sum_{k=2}^K \alpha_k\left[\left(\frac{N_{k-1}}{N_{k}}-1\right)A_{k,N_{k-1}}^{\text{MC}}+\left(1-\frac{N_{k-1}}{N_{k}}\right) A_{k,N_k-N_{k-1}}^{\text{MC}}\right],
\]
Now, the samples in $A_{k,N_{k-1}}^{\text{MC}}$ and $A_{k,N_k-N_{k-1}}^{\text{MC}}$ are independent. Define $Y_1 :=A^{\text{MC}}_{1,N_1},\; Y_k:=\left(\frac{N_{k-1}}{N_{k}}-1\right)A_{k,N_{k-1}}^{\text{MC}}+\left(1-\frac{N_{k-1}}{N_{k}}\right) A_{k,N_k-N_{k-1}}^{\text{MC}}$ for $k=2\ldots, K$.  Note that $Y_k$ are independent with each other for $k=2,\ldots, K$. Then 
\[
A^{\text{MFMC}} = Y_1 + \sum_{k=2}^K \alpha_k Y_k.
\]
So $\mathbb{E}(Y_k) = 0$ for $k\ge 2$ and $\mathbb{E}(A^{\text{MFMC}}) = \mathbb{E}(f_1) $. For independent random variable, since each realization is uncorrelated to each other, this indicates that the sum of sample realizations and variance is interchangeable, therefore
\[
\mathbb{V}\left(Y_1\right) = \frac{\sigma_1^2}{N_1}, \quad \mathbb{V}\left(Y_k\right) = \left(\frac{N_{k-1}}{N_{k}}-1\right)^2\frac{\sigma_k^2}{N_{k-1}}+\left(\frac{N_{k-1}}{N_{k}}-1\right)^2\frac{\sigma_k^2}{N_k-N_{k-1}} = \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\sigma_k^2.
\]
So,
\begin{align}
    \nonumber
    \mathbb{V}\left(A^{\text{MFMC}}\right) &= \mathbb{V}\left(Y_1\right) + \mathbb{V}\left(\sum_{k=2}^K \alpha_kY_k\right)+2\;\text{Cov}\left(Y_1,\sum_{k=2}^K \alpha_k Y_k \right),\\
    \nonumber
    &=\mathbb{V}\left(Y_1\right) + \sum_{k=2}^K \alpha_k^2 \mathbb{V}\left(Y_k\right)+2\sum_{2\le k<j\le K} \alpha_k\alpha_j\; \text{Cov}(Y_k,Y_j) +2\sum_{k=2}^K \alpha_k\;\text{Cov}\left(Y_1, Y_k\right),\\
    &=\frac{\sigma_1^2}{N_1} + \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\left(\alpha_k^2\sigma_k^2 - 2\alpha_i\rho_{1,i}\sigma_1\sigma_i\right),
\end{align}
where we use the fact that $Y_k$ and $Y_j$ are independent with each other and \cite[Lemma~3.2]{PeWiGu:2016}
\[
\text{Cov}(Y_1,Y_k) = \text{Cov}\left(A^{\text{MC}}_{1,N_1},A^{\text{MC}}_{k,N_k}\right) - \text{Cov}\left(A^{\text{MC}}_{1,N_1},A^{\text{MC}}_{k,N_{k-1}}\right) = -2 \sum_{k=2}^K \alpha_k \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\rho_{1,i}\sigma_1\sigma_i.
\]
The nMSE error for the multifidelity Monte Carlo estimator is
\[
\mathcal{E}_{A^{\text{MFMC}}}^2=\frac{\mathbb E\left[\left\Vert\mathbb{E}(f_1)-A^{\text{MFMC}} \right\Vert_{Z}^2\right]}{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2} = \frac{\left\Vert\mathbb{E}(f_1)-\mathbb{E}(A^{\text{MFMC}}) \right\Vert_{Z}^2+\mathbb E\left[\left\Vert\mathbb{E}(A^{\text{MFMC}})-A^{\text{MFMC}} \right\Vert_{Z}^2\right]}{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2} = \frac{\mathbb{V}\left(A^{\text{MFMC}}\right)}{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2}.
\]
The total sampling cost for MFMC estimator is 
\[
\sum_{k=1}^K C_kN_k.
\]
Our next goal is to determine the sample size $N_k$ such that the MFMC estimation satisfy the accuracy threshold $\mathcal{E}_{A^{\text{MFMC}}}^2\le \epsilon^2$. We formulate an following optimization problem to minimize the sampling cost subject to a bounded variance of MFMC estimator, and solve for sample size $N_k\in \mathbb{R}$ for $k=1\dots, K$ and $\alpha_k\in \mathbb{R}$ for $k=2\dots, K$ as
\begin{equation}\label{eq:Optimization_pb}
    \begin{array}{lll}
    \displaystyle\min_{N_1,\ldots N_K\in \mathbb{R}, \alpha_2,\ldots,\alpha_K\in \mathbb{R}} &\sum_{k=1}^K C_kN_k,\\
       \text{s.t.} &\displaystyle -N_1\le 0, \\%[6pt]
        &\displaystyle N_{k-1}-N_k\le 0 &k=2\ldots,K,\\
        &\mathbb{V}\left(A^{\text{MFMC}}\right)\le \left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2\epsilon^2.
    \end{array}
\end{equation}

\begin{theorem}
\label{thm:Sample_size_est}
Let $f_k$ be $K$ models that satisfy the following conditions
%
\begin{alignat*}{8}
    &(i)\;\; |\rho_{1,1}|>\ldots>|\rho_{1,k}|& \qquad \qquad
    &(ii)\;\; \frac{C_{k-1}}{C_k}>\frac{\rho_{1,k-1}^2-\rho_{1,k}^2}{\rho_{1,k}^2-\rho_{1,k+1}^2},\;\;k=2,\ldots,K.
\end{alignat*}
%
Then the global minimizer to \eqref{eq:Optimization_pb} is 
\begin{align*}
    &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k}, \quad k=2\ldots, K,\\
    &N_k^*=\frac{\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2\epsilon^2}\sqrt{\frac{\rho_{1,k}^2 - \rho_{1,k+1}^2}{C_k}}\sum_{j=1}^K\left(\sqrt{\frac{C_j}{\rho_{1,j}^2 - \rho_{1,j+1}^2}} - \sqrt{\frac{C_{j-1}}{\rho_{1,{j-1}}^2 - \rho_{1,j}^2}}\right)\rho_{1,j}^2, \quad k=1\ldots, K,
\end{align*}
with $\rho_{1,0}=\infty$ and $\rho_{1,K+1}=0$.

\end{theorem}
\begin{proof}
First note that $(ii)$ indicates a strict increasing sequence of sample size, the minimizer $\alpha_k^*$ and $N_k^*$ satisfy the constraints $(i)$ and $(ii)$.

Our objective function
\[
J^* = \sum_{k=1}^K C_kN_k^* = \frac{\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2\epsilon^2}\sum_{k=1}^K\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}\sum_{k=1}^K\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2
\]
Consider another local minimum $\alpha_k^+$ and $N_k^+$ of \eqref{eq:Optimization_pb} such that $N_{k-1}^+ = N_{k}^+$ for some $k\ge 2$.
\[
N_{k_i}
\]
\end{proof}



% ========================================
% \section{Numerical experiments}\label{sec:Num-Exp}
% ========================================


\section{Appendix}\label{sec:Appendix}



\bibliographystyle{abbrv}
\bibliography{references}
\end{document}


