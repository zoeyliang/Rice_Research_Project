% ====================================================
\section{Multi-fidelity Monte Carlo}\label{sec:MFMC}
% ====================================================
% examine two main sampling methods: Monte Carlo Finite-Element (MC-FE) and multifidelity Monte Carlo Finite-Element sampling methods (MFMC-FE).

% Let $u_h$ be a finite element discretized solution to \eqref{eq:FreeBoundary} as our high fidelity model. 
The Monte Carlo finite element estimator is often computationally expensive due to the extensive sampling on a fixed finite element discretization. This can become prohibitively costly, particularly under stringent accuracy requirements. To mitigate this challenge, we turn to multi-fidelity Monte Carlo sampling \cite{PeWiGu:2016}, an approach that combines high-fidelity models -- characterized by their accurate approximation of system behavior -- and low-fidelity models, which are computationally efficient albeit less detailed. The core of this approach is rooted in the {\it control variate} technique, which reduces the variance of the estimator by exploiting the statistical correlations between high- and low-fidelity models. By optimally allocating computational effort across these models, the multi-fidelity framework significantly decreases the reliance on expensive high-fidelity evaluations, ultimately lowering computational costs. Importantly, this strategy preserves the accuracy and robustness of the resulting estimator, offering a practical and efficient alternative to traditional high-fidelity-only Monte Carlo sampling. Below, we provide a brief overview of this method.

To approximate \eqref{eq:QoI}, the multi-fidelity Monte Carlo combines the high-fidelity model $\widehat u_{h,1}=u_{h}$ with a sequence of progressively less detailed but computationally cheaper low-fidelity models $\widehat u_{h,k}: W \rightarrow Z$ for $k=2,\ldots,K$. The fidelity of the models decreases as $k$ increases, with $\widehat u_{h,1}$ being the high fidelity model. The variance and Pearson correlation coefficient between any two models $\widehat u_{h,k}$ and $\widehat u_{h,j}$ are
%
\begin{equation*}
    \sigma_k^2 = \mathbb{V}\left(\widehat u_{h,k}\right),\qquad \rho_{k,j} = \frac{\text{Cov}\left(\widehat u_{h,k},\widehat u_{h,j}\right)}{\sigma_k\sigma_j}, \quad k,j=1,\dots, K,
\end{equation*}
%
where $\text{Cov}(\widehat u_{h,k},\widehat u_{h,j}) := \mathbb{E}[\langle \widehat u_{h,k} - \mathbb{E}(\widehat u_{h,k}), \widehat u_{h,j} - \mathbb{E}(\widehat u_{h,j})\rangle_Z]$ and $\rho_{k,k}=1$.
% The Multilevel Monte Carlo estimator is defined as
% \begin{equation}\label{eq:MLMC_estimator}
%     A^{\text{ML}} := A^{\text{MC}}_{L,N_L} + \sum_{k=2}^K \left(A^{\text{MC}}_{k-1,N_{k-1}} - A^{\text{MC}}_{k,N_{k-1}} \right),
% \end{equation}
The multi-fidelity Monte Carlo Finite-Element estimator $A^{\text{MF}}$ incorporates weighted corrections derived from low-fidelity models into the  Monte Carlo estimator of the high-fidelity model, defined as
%
\begin{equation}\label{eq:MFMC_estimator}
    A^{\text{MF}} := A^{\text{MC}}_{1,N_1} + \sum_{k=2}^K \alpha_k\left(\overline{A}_{k,N_k} - \overline{A}_{k,N_{k-1}} \right),
\end{equation}
%
where $A^{\text{MC}}_{1,N_1} $ is the Monte Carlo estimator for the high-fidelity model using $N_1$ samples, $\alpha_k$ are the weights for the correction terms, and $\overline{A}_{k,N_k}$ denotes the sample average of the $k$-th model based on $N_k$ samples. The sample average term $\overline{A}_{k,N_{k-1}}$ in each correction reuses the first $N_{k}$ samples from $\overline{A}_{k,N_{k}}$, requiring the condition $N_{k-1}\le N_k$ for $k=2,\ldots, K$. By partitioning the $N_k$ samples into two disjoint sets, one of size $N_{k-1}$ and the other of size $N_k - N_{k-1}$, we can reformulate the MFMC estimator \eqref{eq:MFMC_estimator} as
%
\begin{equation}\label{eq:MFMC_estimator_independent}
    A^{\text{MF}} = A^{\text{MC}}_{1,N_1} +  \sum_{k=2}^K \alpha_k\left(\frac{N_{k-1}}{N_{k}}-1\right)\left(A_{k,N_{k-1}}^{\text{MC}}- A_{k,N_k\backslash N_{k-1}}^{\text{MC}}\right),
\end{equation}
%
The weights in the correction terms of this reformulation  are now functions of the sample ratio between successive fidelity levels. This reformulation is crucial because it ensures that the samples used in  the two components $A_{k,N_{k-1}}^{\text{MC}}$ and $A_{k,N_k\backslash N_{k-1}}^{\text{MC}}$ of the low-fidelity corrections are independent. The MFMC estimator can be compactly written as
%
\[
A^{\text{MF}} = Y_1 + \sum_{k=2}^K \alpha_k Y_k,
\]
%
where the correction terms $Y_k$ are defined as
%
\[
Y_1 :=A^{\text{MC}}_{1,N_1},\quad Y_k:=\left(\frac{N_{k-1}}{N_{k}}-1\right)\left(A_{k,N_{k-1}}^{\text{MC}}- A_{k,N_k\backslash N_{k-1}}^{\text{MC}}\right), k=2\ldots, K.
\]
%
Since $Y_k$ is the difference between two independent Monte Carlo estimators of the same model, $\mathbb{E}(Y_k) = 0$ for $k\ge 2$, which implies $\mathbb{E}(A^{\text{MF}}) = \mathbb{E}(u_{h,1})$, ensuring the unbiasedness of the estimator. Since the samples in two disjoint components of $Y_k$ are independent, the variances of $Y_k$ can be computed as
%
\[
\mathbb{V}\left(Y_1\right) = \frac{\sigma_1^2}{N_1}, \quad \mathbb{V}\left(Y_k\right) = \left(\frac{N_{k-1}}{N_{k}}-1\right)^2\left(\frac{\sigma_k^2}{N_{k-1}}+\frac{\sigma_k^2}{N_k-N_{k-1}}\right) = \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\sigma_k^2.
\]
%
While $Y_k$ and $Y_j$ for $k\neq j, k,j=2,\cdots, K$ are dependent due to overlapping sample sets, it can be shown that $Y_k$ and $Y_j$ are uncorrelated. However, $Y_k$  is correlated with $Y_1$ since the samples used in $Y_k$ reuse those of $Y_1$. Using the covariance relation from \cite[Lemma~3.2]{PeWiGu:2016}, we express the covariance between $Y_1$ and $Y_k$ as
%
\[
\text{Cov}(Y_1,Y_k) = - \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\rho_{1,k}\sigma_1\sigma_k.
\]
%
Using the variances of k-th correction terms and covariances between the first and k-th correction terms, the variance of the multi-fidelity Monte Carlo estimator can be expressed as
%
\begin{align}
    \nonumber
    \mathbb{V}\left(A^{\text{MF}}\right) &= \mathbb{V}\left(Y_1\right) + \mathbb{V}\left(\sum_{k=2}^K \alpha_kY_k\right)+2\;\text{Cov}\left(Y_1,\sum_{k=2}^K \alpha_k Y_k \right),\\
    \nonumber
    &=\mathbb{V}\left(Y_1\right) + \sum_{k=2}^K \alpha_k^2 \mathbb{V}\left(Y_k\right)+2\sum_{2\le k<j\le K} \alpha_k\alpha_j\; \text{Cov}(Y_k,Y_j) +2\sum_{k=2}^K \alpha_k\;\text{Cov}\left(Y_1, Y_k\right),\\
    % \nonumber
    % &=\mathbb{V}\left(Y_1\right) + \sum_{k=2}^K \alpha_k^2 \mathbb{V}\left(Y_k\right) +2\sum_{k=2}^K \alpha_k\;\text{Cov}\left(Y_1, Y_k\right),\\
    \label{eq:MFMC_variance}
    &=\frac{\sigma_1^2}{N_1} + \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\left(\alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k\right).
\end{align}
%
The normalized mean square error of the multi-fidelity Monte Carlo estimator, $\mathcal{E}_{A^{\text{MF}}}^2$ quantifies its accuracy and can be decomposed into two components -- the bias error $\mathcal{E}_{\text{Bias}}^2$ and the statistical error $\mathcal{E}_{\text{Stat}}^2$, the decomposition is written as 
%
\[
\mathcal{E}_{A^{\text{MF}}}^2= \frac{\left\Vert\mathbb{E}(u)-\mathbb{E}(A^{\text{MF}}) \right\Vert_{Z}^2+\mathbb E\left[\left\Vert\mathbb{E}(A^{\text{MF}})-A^{\text{MF}} \right\Vert_{Z}^2\right]}{\left\Vert\mathbb{E}(u) \right\Vert_{Z}^2} =\frac{\left\Vert\mathbb{E}(u)-\mathbb{E}(A^{\text{MF}}) \right\Vert_{Z}^2}{\left\Vert\mathbb{E}(u) \right\Vert_{Z}^2}+ \frac{\mathbb{V}\left(A^{\text{MF}}\right)}{\left\Vert\mathbb{E}(u) \right\Vert_{Z}^2}=\mathcal{E}_{\text{Bias}}^2 + \mathcal{E}_{\text{Stat}}^2,
\]
%
where the variance term $\mathbb{V}\left(A^{\text{MF}}\right)$  can be explicitly expressed using \eqref{eq:MFMC_variance}. A splitting ratio $\theta$ is introduced as before to balance the contributions between these two components. The spatial resolution required to achieve the accuracy bound $\theta \epsilon^2$ for the discretization error can be determined by estimating the number of spatial grid points $M_L$ and the corresponding grid level $L$, given by
%
\begin{equation}
    \label{eq:SLSGC_MLS_SpatialGridsNo}
    M_L = M_0s^{-L} \ge \left(\frac{\theta\epsilon}{C_m}\right)^{-\frac 1 {\alpha}} \qquad \text{ and } \qquad     L = \left\lceil \frac{1}{\alpha}\log_s \left(\frac{C_m M_0^\alpha}{\theta\epsilon}\right) \right\rceil,
\end{equation}
%
where $M_0$ is the number of grid points at the coarsest level, $s>1$ is the refinement factor, $\alpha$ represents the convergence rate of the spatial discretization, and $C_m$ is a constant characterizing the discretization scheme. In order to determine the optimal sample sizes $N_k$ and weights $\alpha_k$ for the MFMC estimator \eqref{eq:MFMC_estimator_independent}, we first express the total computational cost for the MFMC estimator
%
\[
\mathcal{W}^{\text{MF}} = \sum_{k=1}^K C_kN_k,
\]
%
where $C_k$ denotes the cost of generating a single sample from the $k$-th low-fidelity model, and $N_k$ represent the number of samples taken from that model. Unlike previous approaches \cite{PeWiGu:2016} that derive sample sizes based on a fixed computational budget, our approach explicitly express the sample sizes and computational resources in terms of the desired accuracy $\epsilon$. While both methodologies yield equivalent results, our proposed framework provides flexibility in scenarios where accuracy requirements are prioritized over predefined cost constraints. To determine the optimal sample sizes $N_k$ and weights $\alpha_k$,  we formulate an optimization problem that minimizes the total sampling cost $\mathcal{W}^{\text{MF}}$, subject to three constraints: first, the normalized statistical error $\mathcal{E}_{\text{Stat}}^2$ must meet the desired accuracy threshold $(1-\theta)\epsilon^2$; second,  the hierarchical ordering condition $N_{k-1}\le N_k$ for $k=2,\ldots, K$ ensures a logical allocation of samples across fidelity levels; third, all sample sizes must be non-negative. The resulting optimization problem is
%
\begin{equation}\label{eq:Optimization_pb_sample_size}
    \begin{array}{ll}
    \min \limits_{\begin{array}{c}\scriptstyle N_1,\ldots, N_K\in \mathbb{R} \\[-4pt]
\scriptstyle \alpha_2,\ldots,\alpha_K\in \mathbb{R}
\end{array}} &\displaystyle\sum\limits_{k=1}^K C_kN_k,\\
       \;\,\text{subject to} &\mathbb{V}\left(A^{\text{MF}}\right)- \left\Vert\mathbb{E}(u) \right\Vert_{Z}^2(1-\theta)\epsilon^2 = 0,\\[2pt]
       &\displaystyle N_{k-1}-N_k\le 0, \quad\quad\, k=2\ldots,K.\\[1pt]
        &\displaystyle -N_1\le 0, \\
    \end{array}
\end{equation}
%
The solution to this optimization problem \eqref{eq:Optimization_pb_sample_size}, which explicitly provides the optimal real-valued sample sizes and weights, is presented in Theorem \ref{thm:Sample_size_est}.
%
\begin{theorem}
\label{thm:Sample_size_est}
Consider a set of $K$ models, $u_k$ for $k=1,\ldots,K$, where each model is characterized by the standard deviation $\sigma_k$ of its output, the correlation coefficient $\rho_{1,k}$ between the highest-fidelity model $u_{h,1}$ and the $k$-th low-fidelity model, and the computational cost per sample $C_k$. Assume the following conditions hold
%
\begin{alignat*}{8}
    &(i)\;\; |\rho_{1,1}|>\ldots>|\rho_{1,K}|,& \qquad \qquad
    &(ii)\;\; \frac{C_{k-1}}{C_k}>\frac{\rho_{1,k-1}^2-\rho_{1,k}^2}{\rho_{1,k}^2-\rho_{1,k+1}^2},\quad \quad k=2,\ldots,K.
\end{alignat*}
%
Under these assumptions, the optimal sample sizes $N_k^*$ and weights $\alpha_k^*$, for $k=1,\ldots, K$, solving the optimization problem \eqref{eq:Optimization_pb_sample_size} are
%
\begin{align}
    \label{eq:MFMC_coefficients}
    &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\\
    \label{eq:MFMC_SampleSize}
    &N_k^*=\frac{\sigma_1^2}{\epsilon^2(1-\theta)\left\Vert\mathbb{E}(u) \right\Vert_{Z}^2}\sqrt{\frac{\rho_{1,k}^2-\rho_{1,k+1}^2}{C_k}}\sum_{j=1}^K\sqrt{C_j\left(\rho_{1,j}^2-\rho_{1,j+1}^2\right)}, \quad\quad \rho_{1,K+1}=0.
\end{align}
%
\end{theorem}
%

\begin{proof}
The optimization problem is approached with the method of Lagrange multipliers, where the auxiliary Lagrangian function $L$ incorporates multipliers $\lambda_0,\ldots, \lambda_K$ to enforce the constraints on the variance and sample sizes
%
\[
L = \sum_{k=1}^K C_kN_k +\lambda_0 \left(\frac{\sigma_1^2}{N_1} + \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\left(\alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k\right)\right)-\lambda_1 N_1+\sum_{k=2}^K\lambda_k(N_{k-1} - N_k),
\]
%
To solve this, we apply the Karush-Kuhn-Tucker (KKT) conditions for \eqref{eq:Optimization_pb_sample_size}, leading to a system of equations for the partial derivatives of the Lagrangian with respect to the optimization variables $\alpha_k$ and $N_k$,  in addition to  primal feasibility, dual feasibility, and complementary slackness conditions
%
\begin{align*}
\frac{\partial L}{\partial \alpha_j}=0,\quad \frac{\partial L}{\partial N_k}&=0,\quad j=2\ldots,K, \quad k=1\ldots,K,\\
\mathbb{V}\left(A^{\text{MF}}\right)- \epsilon^2(1-\theta)\left\Vert\mathbb{E}(u) \right\Vert_{Z}^2 &= 0,\\
    N_{k-1}-N_k&\le 0, \quad k=2\ldots,K,\\
    -N_1&\le 0,\\[6pt]
    \lambda_1,\ldots,\lambda_K &\ge 0,\\
    \lambda_k(N_{k-1}-N_k)&=0,\quad k=2\ldots,K,\\
    \lambda_1 N_1&=0.
\end{align*}
%
The partial derivatives of the Lagrangian with respect to $\alpha_k$ and $N_k$ are computed as
%
\begin{align*}
    \frac{\partial L}{\partial \alpha_k}&=\lambda_0\left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\left(2\alpha_k\sigma_k^2 - 2\rho_{1,k}\sigma_1\sigma_k\right),\quad k=2,\dots,K,\\
    \frac{\partial L}{\partial N_1}&=C_1 + \lambda_0\left(-\frac{\sigma_1^2}{N_1^2} - \frac{\alpha_2^2\sigma_2^2-2\alpha2\rho_{1,2}\sigma_1\sigma_2}{N_1^2}\right)-\lambda_1+\lambda_2,\\
    \frac{\partial L}{\partial N_k}&=C_k+\lambda_0\left(\frac{\alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k}{N_k^2}-\frac{\alpha_{k+1}^2\sigma_{k+1}^2 - 2\alpha_{k+1}\rho_{1,k+1}\sigma_1\sigma_{k+1}}{N_k^2}\right)-\lambda_k+\lambda_{k+1}, \quad k=2,\dots,K-1,\\
    \frac{\partial L}{\partial N_K}&=C_K + \lambda_0\left(\frac{\alpha_K^2\sigma_K^2 - 2\alpha_K\rho_{1,K}\sigma_1\sigma_K}{N_K^2}\right)-\lambda_K.
\end{align*}
%
By solving the equation $\partial L/\partial \alpha_k=0$, the optimal weights $\alpha_k^*$ are determined as
%
\[
\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k}.
\]
%
Substituting $\alpha_k^*$ into $\partial L/\partial N_k=0$ yields a representation for $C_k$ in terms of the sample sizes $N_k$, variance contributions, correlation coefficients, and the Lagrangian multipliers
%
\begin{equation*}
    C_k=\left\{ \begin{array}{ll}
\frac{\lambda_0\sigma_1^2}{N_k^2}\left(\rho_{1,k}^2-\rho_{1,k+1}^2\right)+\lambda_k-\lambda_{k+1}, & \text{ for }\; k=1,\ldots,K-1, \\
\frac{\lambda_0\sigma_1^2}{N_k^2}\rho_{1,k}^2+\lambda_k, & \text{ for }\; k=K.
\end{array}\right.
\end{equation*}
%
 As shown by  \cite{PeWiGu:2016}, the global minimizer is achieved when the inequality constraints are inactive ($\lambda_k$=0, $k=1,\dots, K$) in the complementary slackness conditions, indicating that the sample sizes strictly increase with $k$ ($N_{k-1}< N_k$ for $k=2,\ldots, K$). This results in the expression for the optimal sample sizes
% \[
% N_1 = \sigma_1\sqrt{\lambda_0}\sqrt{\frac{1-\rho_{1,2}^2}{C_1}}, \quad N_k = \sigma_1\sqrt{\lambda_0}\sqrt{\frac{\rho_{1,k}^2-\rho_{1,k+1}^2}{C_k}}, \quad N_K = \sigma_1\sqrt{\lambda_0}\sqrt{\frac{\rho_{1,K}^2}{C_K}},
% \]
% or we can simplify the notation as
\begin{equation}
\label{eq:sample_size_1}
    N_k = \sigma_1\sqrt{\lambda_0}\sqrt{\frac{\rho_{1,k}^2-\rho_{1,k+1}^2}{C_k}},\quad \text{with}\quad  \rho_{1,K+1}=0 \quad k=1,\ldots,K.
\end{equation}
% \frac{1}{N_k} = \frac{1}{\sigma_1\sqrt{\lambda_0}}\sqrt{\frac{C_k}{\rho_{1,k}^2-\rho_{1,k+1}^2}},
Using the optimal coefficients $\alpha_k^*$ and sample size estimations $N_k$ in \eqref{eq:sample_size_1}, the variance \eqref{eq:MFMC_variance} of the multi-fidelity Monte Carlo estimator becomes
%
\begin{equation*} \label{eq:MFMC_variance2}
    \mathbb{V}\left(A^{\text{MF}}\right) = \frac{\sigma_1}{\sqrt{\lambda_0}}\sum_{k=1}^K\sqrt{C_k\left(\rho_{1,k}^2-\rho_{1,k+1}^2\right)},\quad \text{with}\;\;\rho_{K+1}=0.
\end{equation*}
%
Using the equality constraint that variance satisfies,  we solve for the value of $\sqrt{\lambda_0}$ as
%
\[
\sqrt{\lambda_0} = \frac{\sigma_1}{\epsilon^2(1-\theta)\left\Vert\mathbb{E}(u) \right\Vert_{Z}^2}\sum_{k=1}^K\sqrt{C_k\left(\rho_{1,k}^2-\rho_{1,k+1}^2\right)}.
\]
%
Substituting $\sqrt{\lambda_0}$ into \eqref{eq:sample_size_1}, we derive the optimal sample sizes for $k=1,\ldots, K$
%
\[
N_k^* = \frac{\sigma_1^2}{\epsilon^2(1-\theta)\left\Vert\mathbb{E}(u) \right\Vert_{Z}^2}\sqrt{\frac{\rho_{1,k}^2-\rho_{1,k+1}^2}{C_k}}\sum_{j=1}^K\sqrt{C_j\left(\rho_{1,j}^2-\rho_{1,j+1}^2\right)},\quad \text{with}\;\;\rho_{K+1}=0.
\]
%
Note that by ensuring the condition $(ii)$ is satisfied, we can guarantee that $N_k^*$ increases strictly as $k$ grows. 
\end{proof}
%
Using the sample size estimates $N_k^*$ as established in Theorem \ref{thm:Sample_size_est}, the total sampling cost $\mathcal{W}^\text{MF}$ can be estimated as
%
\begin{equation}\label{eq:MFMC_sampling_cost}
    \mathcal{W}^\text{MF} = \sum_{k=1}^K C_k N_k^* = \frac{\sigma_1^2}{\epsilon^2(1-\theta)\left\Vert\mathbb{E}(u) \right\Vert_{Z}^2}\left(\sum_{k=1}^K\sqrt{C_k\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)}\right)^2,\quad \text{with}\;\;\rho_{K+1}=0.
\end{equation}
%
In practical applications, however, the correlation parameters $\rho_{1,k}$ and the costs per single sample $C_k$ are usually not known a priori and must be estimated from the sample data. Furthermore, since the theoretical sample sizes $N_k^*$ are real numbers, they need to be rounded to the nearest integer values for implementation purposes. To address this, we use the ceiling function $\lceil N_k^* \rceil$ to represent the rounded sample sizes, leading to the lower and upper bounds for the total sampling cost
%
\begin{equation}\label{eq:sampling_cost_bound}
    \sum_{k=1}^K C_k N_k^*\le \sum_{k=1}^K C_k \left\lceil N_k^*\right\rceil<\sum_{k=1}^K C_k N_k^* + \sum_{k=1}^K C_k,
\end{equation}
%
where the additional term $\sum_{k=1}^K C_k$ arises from the fact that $N_k^*\le \lceil N_k^*\rceil< N_k^*+1$. To better understand the behavior of the sampling cost, define $B_k = C_k(\rho_{1,k}^2 - \rho_{1,k+1}^2)$ with $\rho_{K+1}=0$ for $k=1,\dots, K$. Substituting $B_k$ into the sampling cost, we rewrite \eqref{eq:MFMC_sampling_cost} as
%
\begin{equation*}\label{eq:MFMC_sampling_cost_2}
    \mathcal{W}^{\text{MF}} = \sum_{k=1}^K C_k N_k^* = \frac{\sigma_1^2}{\epsilon^2(1-\theta)\left\Vert\mathbb{E}(u) \right\Vert_{Z}^2}\left(\sum_{k=1}^K\sqrt{B_k} \right)^2.
\end{equation*}
%
The quantity $B_k$ depends on the product of the cost per sample $C_k$ and the difference between two successive correlations $(\rho_{1,k}^2 - \rho_{1,k+1}^2)$. Depending on how these components interact, $B_k$ may decay, grow, or remain constant as $k$ increases.
Under condition (ii) of Theorem \ref{thm:Sample_size_est}, the following inequality holds
%
\begin{equation}
\label{eq:Bk_Ck_decay_rate}
    \frac{\sqrt{B_{k}}}{\sqrt{B_{k-1}}}>\frac{C_{k}}{C_{k-1}}, \quad k=2,\ldots,K.
\end{equation}
%
If $\sqrt{B_k}$ decays as $k$ increases, \eqref{eq:Bk_Ck_decay_rate} indicates that its decay is slower than that of $C_k$. In the asymptotic regime where $K$ is large, whether $\sqrt{B_k}$ decay or grow or stay the same, \eqref{eq:Bk_Ck_decay_rate} indicates that the contribution of the term $\sum_{k=1}^K C_k$ in the cost bounds becomes negligible compared to $\sum_{k=1}^K C_kN_k$. Consequently, this implies that the sampling cost of integer-rounded sample size has the same asymptotic cost behavior as the real-valued sample size as $\mathcal{W}^\text{MF}$ in \eqref{eq:MFMC_sampling_cost}.




% Using the fact that $N_k$ increases and the value of $\alpha_k$, we observe that the MFMC estimator variance $\mathbb{V}\left(A^{\text{MFMC}}\right)$ in \eqref{eq:MFMC_variance2} always decreases as the model number $K$ increases. This reflects the fact that the low fidelity models are used as control variates to reduce the variance of the high fidelity model. However, this $K$ cannot be arbitrarily large, since the first summation term in \eqref{eq:MFMC_sampling_cost} grows, the second summation reflect the variance decay of the MFMC estimator. Thus this is a tie between these two terms. If $K$ is sufficiently large,  in order to achieve an optimal sampling cost, we need to study the decay and growth of these two terms. We will choose the $K$ such that the product of two summation terms in \eqref{eq:MFMC_sampling_cost} is minimum, i.e. If $K$ is sufficiently large, we need to find $K\in \mathbb{N}$ such that 
% \begin{equation}\label{eq:Optimal_K}
%    K = \text{argmin} \sum_{k=1}^K\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}\sum_{k=1}^K\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2. 
% \end{equation}


The total sampling cost efficiency of the multi-fidelity Monte Carlo estimator relative to the standard Monte Carlo estimator is
%
\begin{equation}\label{eq:MFMC_sampling_cost_efficiency}
    \xi = \frac{\mathcal{W}^\text{MF}}{\mathcal{W}^\text{MC}} = \frac{1}{C_1} \left(\sum_{k=1}^K\sqrt{C_k\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)}\right)^2,
\end{equation}
%

% Further more, we observe that
% \begin{align*}
%     \mathcal{W}_\text{MC}\mathbb{V}\left(A^{\text{MC}}\right) &=\frac{C_1\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2},\\
%  \mathcal{W}_\text{MFMC}\mathbb{V}\left(A^{\text{MFMC}}\right) &=  \frac{\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2}\sum_{k=1}^K\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}\sum_{k=1}^K\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2.
% \end{align*}
% This implies that if both Monte Carlo and multifidelity Monte Carlo have  a same sampling cost, then $\mu=  \mathbb{V}\left(A^{\text{MFMC}}\right)/\mathbb{V}\left(A^{\text{MC}}\right)$. Therefore, 
This ratio also quantifies reduction of computational budget achieved by the MFMC estimator given the same variance. The quantity of $\xi$ is determined by the cost per sample for various models and the correlation parameters. The lower the value of $\xi$, the more effective the MFMC estimator.


\subsection{Sensitivity analysis}

For Welford's algorithm, the sample mean, variance and standard deviation for both high- and low- fidelity models are unbiased, $\mathbb{E}(m_w^{(N)})=\mu_1$ and $\mathbb{E}(\widehat m_w^{(N)})=\mu_k$. The variance $\mathbb{E}(v_w^{(N)})=(N-1)\sigma_1^2$, the standard deviation $\mathbb{E}(\sigma_w^{(N)})=\sqrt{\mathbb{E}(v_w^{(N)})/(N-1)}=\sigma_1$ and $\mathbb{E}(\widehat \sigma_w^{(N)})=\sigma_k$. $\mathbb{E}( r_w^{(N)})=(N-1)\rho_{1,k}\sigma_1\sigma_k$ and $\mathbb{E}(\text{Cov}_w^{(N)}) = \mathbb{E}( r_w^{(N)})/(N-1) = \rho_{1,k}\sigma_1\sigma_k$. 

Let $\rho_{1,k}^{(N)} = \frac{\text{Cov}_w^{(N)}}{\sigma_w^{(N)}\widehat\sigma_w^{(N)}}$. Let $q=(a,b,c)=(\rho_{1,k}\sigma_1\sigma_k,\sigma_1,\sigma_k), q_N=(\check{a},\check{b},\check{c})=(\text{Cov}_w^{(N)},\sigma_w^{(N)},\widehat\sigma_w^{(N)})$, then $\rho_{1,k} =f(a,b,c) = a/(bc)$, then $ \nabla f =(\frac{1}{bc}, -\frac{a}{b^2c}, -\frac{a}{bc^2})$. Then we use Taylor expansion to express $\rho_{1,k}^{(N)}$ around $q$ we have
\begin{align*}
    \rho_{1,k}^{(N)} &\approx \rho_{1,k} + \frac{\partial \rho_{1,k}}{\partial \text{Cov}_w}\left(\text{Cov}_w^{(N)} - \rho_{1,k}\sigma_1\sigma_k\right) + \frac{\partial \rho_{1,k}}{\partial \sigma_1}\left(\sigma_w^{(N)} - \sigma_1\right) + \frac{\partial \rho_{1,k}}{\partial \sigma_k}\left(\widehat \sigma_w^{(N)} - \sigma_k\right)\\
    &+ \frac 1 2 \begin{bmatrix}
        \check{a}-a\\
        \check{b}-b\\
        \check{c}-c\\
    \end{bmatrix}^T
    \begin{bmatrix}
        0&-\frac{1}{b^2c}&-\frac{1}{bc^2}\\
        -\frac{1}{b^2c}&\frac{2a}{b^3c}&\frac{a}{b^2c^2}\\
        -\frac{1}{bc^2}&\frac{a}{b^2c^2}&\frac{2a}{bc^3}\\
    \end{bmatrix}
    \begin{bmatrix}
        \check{a}-a\\
        \check{b}-b\\
        \check{c}-c\\
    \end{bmatrix}
\end{align*}.


% \[
% \rho_{1,k}^{(N)} \approx \rho_{1,k} + \frac{\partial \rho_{1,k}}{\partial \text{Cov}_w}\left(\text{Cov}_w^{(N)} - \rho_{1,k}\sigma_1\sigma_k\right) + \frac{\partial \rho_{1,k}}{\partial \sigma_1}\left(\sigma_w^{(N)} - \sigma_1\right) + \frac{\partial \rho_{1,k}}{\partial \sigma_k}\left(\widehat \sigma_w^{(N)} - \sigma_k\right)+
% \]
where $\frac{\partial f}{\partial a} = \frac{1}{\sigma_1\sigma_k}$, $\frac{\partial f}{\partial b} = -\frac{\rho_{1,k}}{\sigma_1}$, $\frac{\partial f}{\partial c} = -\frac{\rho_{1,k}}{\sigma_k}$. $\nabla f|_{q} = (\frac{1}{\sigma_1\sigma_k},-\frac{\rho_{1,k}}{\sigma_1},-\frac{\rho_{1,k}}{\sigma_k} )^T$. Since the error for $\check{a},\check{b}, \check{c}$ are $\mathcal{O}(\frac{1}{\sqrt{N}})$, Therefore,
\begin{equation}
\label{eq:Expectation_rho}
    \mathbb{E}\left(\rho_{1,k}^{(N)}\right) = \rho_{1,k} + \mathcal{O}\left(\frac 1 N\right).
\end{equation}



According to the central limit theorem, $q_N$ asymptotically follows a normal distribution
\[
\sqrt{N}(q_N-q)\sim \mathcal{N}(0,\Sigma)
\]
covariance matrix $\Sigma$ can be calculated by the multivariable delta method, therefore
\begin{equation}
    \label{eq:Var_rho}
    \text{Var}\left(\rho_{1,k}^{(N)}\right)\approx \frac{1}{N}\nabla f^T \Sigma \nabla f\approx \frac{(1-\rho_{1,k}^2)^2}{N} + \mathcal{O}\left(\frac{1}{N^2}\right).
\end{equation}


We can decompose the relative error of calculating the correlated coefficients as 
\begin{equation*}
    \left|\rho_{1,k} - \rho_{1,k}^{(N)}\right|\le \underbrace{\left|\rho_{1,k} - \mathbb{E}\left(\rho_{1,k}^{(N)}\right)\right|}_{\text{Bias}}+\underbrace{\left|\mathbb{E}\left(\rho_{1,k}^{(N)}\right)-\rho_{1,k}^{(N)}\right|}_{\text{Variance}}
\end{equation*}
If we bound the relative error by $\delta_1$, and split the error equally between the bias and variance with splitting ratio $\theta_1$. For the bias term, using \eqref{eq:Expectation_rho}, we have $N\ge \frac{C_\rho}{\theta_1\delta_1}$. The variance term uses Chebyshev inequality
\[
P\left(\left|\mathbb{E}\left(\rho_{1,k}^{(N)}\right)-\rho_{1,k}^{(N)}\right|\ge \nu\right)\le \frac{\text{Var}\left(\rho_{1,k}^{(N)}\right)}{\nu^2}
\]

Take $\nu = (1-\theta_1)\delta_1$, by \eqref{eq:Var_rho}, we have
\[
\frac{(1-\rho_{1,k}^2)^2}{N\nu^2} = \frac{(1-\rho_{1,k}^2)^2}{(1-\theta_1)^2N\delta_1^2}\le 1\rightarrow N\ge \frac{(1-\rho_{1,k}^2)^2}{(1-\theta_1)^2\delta_1^2}.
\]
So if $N\ge \frac{(1-\rho_{1,k}^2)^2}{(1-\theta_1)^2\delta_1^2}$, the variance has high probability to be less than $(1-\theta_1)\delta_1$. Therefore, the sample size $N$ should satisfy
\begin{equation}
\label{eq:Offline_Sample_Size}
    N\ge \max \left(\frac{C_\rho}{\theta_1\delta_1 }, \frac{(1-\rho_{1,k}^2)^2}{(1-\theta_1)^2\delta_1^2}\right), \quad \text{ for each }k,
\end{equation}
Note to estimate $C_\rho$, we can use samples of size $Q_{i+1},Q_i$, then 
\[
C_\rho\approx \frac{\rho_{1,k}^{(Q_{i+1})} - \rho_{1,k}^{(Q_i)}}{\frac{1}{Q_i\left|\rho_{1,k}^{(Q_i)}\right|} - \frac{1}{Q_{i+1}\left|\rho_{1,k}^{(Q_{i+1})}\right|} }
\]
% Note that
% \[
% \frac{d}{d\rho_{1,k}}\sqrt{\rho_{1,k}^2 - \rho_{1,k+1}^2} = \frac{\rho_{1,k}}{\sqrt{\rho_{1,k}^2 - \rho_{1,k+1}^2}}
% \]
% \[
% \frac{d }{d \rho_{1,k}} \sum_{j=1}^K\sqrt{C_j\left(\rho_{1,j}^2 - \rho_{1,j+1}^2\right)} = \rho_{1,k}\sqrt{\frac{C_k}{\rho_{1,k}^2-\rho_{1,k+1}^2}}\left(1-\sqrt{\frac{C_{k-1}(\rho_{1,k}^2-\rho_{1,k+1}^2)}{C_k(\rho_{1,k-1}^2-\rho_{1,k}^2)}}\right)
% \]

Let $\boldsymbol{\rho}$ be the true value, the approximated value is $\boldsymbol{\rho}+\Delta \boldsymbol{\rho}$
\[
\Delta\xi=\xi(\boldsymbol{\rho}+\Delta \boldsymbol{\rho}) - \xi(\boldsymbol{\rho}) \approx \sum_{k=2}^K \frac{\partial \xi}{\partial \rho_{1,k}} \Delta\rho_{1,k},
\]
where
%
\begin{align*}
% \frac{\partial  \xi}{\partial  \rho_{1,1}} &=\frac{2\sum_{j=1}^K\sqrt{C_j\left(\rho_{1,j}^2 - \rho_{1,j+1}^2\right)}}{C_1}\frac{C_1\rho_{1,1}}{\sqrt{C_1(\rho_{1,1}^2-\rho_{1,2}^2)  }}\\
    \frac{\partial  \xi}{\partial  \rho_{1,k}} 
&=\frac{2\sum_{j=1}^K\sqrt{C_j\left(\rho_{1,j}^2 - \rho_{1,j+1}^2\right)}}{C_1}\left(\frac{C_k\rho_{1,k}}{\sqrt{C_k(\rho_{1,k}^2-\rho_{1,k+1}^2)}}-\frac{C_{k-1}\rho_{1,k}}{\sqrt{C_{k-1}(\rho_{1,k-1}^2-\rho_{1,k}^2)}}\right), \quad \forall\; k=2,\ldots, K.
\end{align*}
%
Note The term $\left(\frac{C_k\rho_{1,k}}{\sqrt{C_k(\rho_{1,k}^2-\rho_{1,k+1}^2)}}-\frac{C_{k-1}\rho_{1,k}}{\sqrt{C_{k-1}(\rho_{1,k-1}^2-\rho_{1,k}^2)}}\right)$ in $\partial \xi/\partial \rho_{1,k}$ is always negative due to the conditions required for MFMC. In this case, by Cauchy Schwartz's inequality, we can estimate
\[
\left|\frac{\Delta \xi}{\xi}\right|\le \frac{1}{\xi}\sqrt{\sum_{k=2}^K \left(\frac{\partial \xi}{\partial \rho_{1,k}}\right)^2}\cdot \sqrt{\sum_{k=2}^K\left(\Delta\rho_{1,k}\right)^2}\le C^\prime \delta_2<\delta
\]
where 

\begin{equation}
    C^\prime  = \frac{1}{\xi}\sqrt{\sum_{k=2}^K \left(\frac{\partial \xi}{\partial \rho_{1,k}}\right)^2}
\end{equation}


Given $\delta$, we first estimate $C^\prime$, then choose $\delta_2$ as $\delta/C^\prime$, $\delta_1=\delta_2/\sqrt{K-1}$, and select $N$ by \eqref{eq:Offline_Sample_Size} for all $k$.


% The term $\left(1-\sqrt{\frac{C_{k-1}(\rho_{1,k}^2-\rho_{1,k+1}^2)}{C_k(\rho_{1,k-1}^2-\rho_{1,k}^2)}}\right)$ in $\partial \xi/\partial \rho_{1,k}$ encodes MFMC’s selection criteria, ensuring the derivative’s negativity when models are optimally ordered. This indicates that higher $\rho_{1,k}$ improves low-fidelity models’ variance reduction efficiency, reducing reliance on costly high-fidelity evaluations. This reinforces the idea that high-quality low-fidelity models—those that are more aligned with the high-fidelity results—can significantly lower the reliance on expensive high-fidelity evaluations, making the entire multi-fidelity approach more cost-effective.


\subsection{Model selection}
To maximize the capability of the multi-fidelity Monte Carlo estimator relative to the Monte Carlo estimator, we aim to select a subset of models from the available set such that the cost efficiency ratio $\xi$ (or equivalently, the sampling cost $\mathcal{W}^\text{MF}$) is minimized. Let $\mathcal{S}=\{\widehat u_{h, k}\}_{k=1}^K$ denote the $K$ available models. Our goal is to identify a subset  $\mathcal{S}^*=\{ \widehat u_{h, i}\}_{i=1}^{K^*} \subseteq S ( \text{ with } K^*\le K)$ that minimizes $\xi$  subject to the conditions of Theorem \ref{thm:Sample_size_est} -- noting that $\mathcal{S}^*$ is non-empty since the high-fidelity model $u_{h,1}$ is always included. Unlike the exhaustive algorithm in \cite[Algorithm~1]{PeWiGu:2016}, which incurs a computational cost of $\mathcal{O}(2^K)$, our method uses a backtracking strategy that incrementally selects the subsets while pruning branches that cannot lead to valid complete solutions. Although the worst-case complexity of our approach remains exponential, effective pruning significantly reduces the search space in practice, yielding sub-exponential complexity--achieving best-case performance of $\mathcal{O}(K)$ and typically $\mathcal{O}(K^2)$, depending on the branching factor at each recursion level. Furthermore, in our MFMC workflow, the models are pre-sorted by correlation, which facilitates early termination of unpromising branches and minimizes unnecessary computations for large $K$. The resulting algorithm, presented in Algorithm \ref{algo:enhanced_mfmc_selection}, outputs the selected $K^*$ low-fidelity models along with their corresponding correlation coefficients $\boldsymbol{\rho}$, costs $\boldsymbol{C}$, the minimal cost efficiency ratio $\epsilon_{\text{min}}$, and weights $\alpha_i$ for $i=2\ldots, K^*$. 

\begin{equation*}\label{eq:Optimization_pb_model_selection}
    \begin{array}{lll}
    \displaystyle\min_{S^*} &\displaystyle \xi,\\
       \text{s.t.} &\displaystyle |\rho_{1,1}|>\ldots>|\rho_{1,K^*}|,\\
       &\displaystyle \frac{C_{i-1}}{C_i}>\frac{\rho_{1,i-1}^2-\rho_{1,i}^2}{\rho_{1,i}^2-\rho_{1,i+1}^2}, \quad i=1,\ldots,{K^*}, \quad \rho_{1,K^*+1}=0,\\
    \end{array}
\end{equation*}


\normalem
\begin{algorithm}[!ht]
\label{algo:MFMC_Algo_model_selection}
\DontPrintSemicolon    
   \KwIn{$K$ candidate models $\widehat  u_{h, k}$ with coefficients $\rho_{1,k}$, $\sigma_1$, $\sigma_k$ and cost per sample $C_k$.}\vspace{1ex}
    
    \KwOut{Selected $K^*$ models $\widehat u_{h, i}$ in $\mathcal{S}^*$, with coefficients $\rho_{1,i}$, $\alpha_i$ and $C_i$ for each model $\widehat u_{h, i}$.}\vspace{1ex}
    \hrule \vspace{1ex}

   % Estimate $\rho_{1,k}$ and $C_k$ for each model $u_{h, k}$ using $N_0$ samples.
   
   
   Sort $u_{h, k}$ by decreasing $\rho_{1,k}$ to create $\mathcal{S}=\{\widehat u_{h, k}\}_{k=1}^K$. 
   
   Initialize $w^*=C_1$, $\mathcal{S}^*=\{\widehat u_{h, 1}\}$. Let $ \mathcal{\widehat S}$ be all $2^{K-1}$ ordered subsets of $\mathcal{S}$, each containing $\widehat u_{h, 1}$. 
   % Set $ \mathcal{\widehat S}_1=\mathcal{S}^*$.

    % $(2 \le j \le 2^{K-1})$
    \For{each subset $\mathcal{\widehat S}_j$\,}{

    {
    \If{ condition $(ii)$ from Theorem \ref{thm:Sample_size_est} is satisfied}{
    Compute the objective function value $w$ using \eqref{eq:MFMC_sampling_cost_efficiency}.
    
    \If{$w<w^*$}{
    {
    Update $\mathcal{S}^* = \mathcal{\widehat S}_j$ and $w^* = w$.
    }
    } 
    }
    }
    $j=j+1$.
    }
    Compute $\alpha_i$ for $\mathcal{S}^*$, $i=2,\dots, K^*$ by \eqref{eq:MFMC_coefficients}.
\caption{Multi-fidelity Model Selection--\JLcolor{\cite[Algorithm~1]{PeWiGu:2016}}}
\end{algorithm}
\ULforem


\normalem
\begin{algorithm}[!ht]
\label{algo:enhanced_mfmc_selection}
\DontPrintSemicolon
\SetAlgoVlined
\SetKwProg{Fn}{Function}{}{}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}

\Input{%
$K$ candidate models $\{\widehat u_{h,k}\}_{k=1}^K$ with correlation coefficients $\rho_{1,k}$, standard deviations $\sigma_1$ and $\sigma_k$, and costs per sample $C_k$. 
}
\Output{%
  Optimal subset of models $\mathcal{S}^*=\{\widehat u_{h,i}\}_{i=1}^{K^*} \subseteq \{\widehat u_{h,k}\}_{k=1}^K$, correlations of selected models $\boldsymbol{\rho}^*$, costs of selected models $\boldsymbol{C}^*$, minimal cost efficiency ratio $\xi_{\text{min}}$, weights $\alpha_i$ for $i=2,\ldots, K^*$.
}
\hrule

\Fn{[ind\textunderscore for\textunderscore model, $\xi_{\text{min}}$] = MFMC\textunderscore model\textunderscore selection\textunderscore backtrack ($\boldsymbol{\rho}, \boldsymbol{C}$)}{
Sort models by non-increasing $|\rho_{1,k}|$, relabel $\rho_{1,k}, C_k$ for $k=1,\ldots,K$ and $\{\widehat u_{h,k}\}_{k=1}^K$.


Initialize global variables: 
$\mathcal{S} = \{\widehat u_{h,1}\}$, $\boldsymbol{\rho}=[1]$, $\boldsymbol{C}=[C_1]$, $\xi_{\text{min}}=1$.

\vspace{3mm}
\textbf{Backtrack} $(1,\, \xi_{\text{min}},\, 2)$. 

ind\textunderscore for\textunderscore model = order(index).
\vspace{3mm}

\Fn{ $[\mathcal{S}^*,\, \boldsymbol{\rho}^*, \,\boldsymbol{C}^*, \xi_{\text{min}}]$ = \textbf{Backtrack} $\left(\text{cur}\_\text{index}, \, \xi, \,k_{\text{last}}\right)$}{


  \If{$\xi \leq \xi_{\text{min}}\,$ }{
    $\xi_{\text{min}}=\xi$

    index = cur\textunderscore index
  }
  % \Else {
  %   $\mathcal{S}^* = \mathcal{S}$, $\boldsymbol{\rho}^* = \boldsymbol{\rho}$, $\boldsymbol{C}^* = \boldsymbol{C}$, $\xi_{\text{min}}=\xi$.
  % }
  
  \If{$k_{\text{last}} > K$}{ 
    \Return
  }
  
  \For{$k = k_{\text{last}}$ \textbf{to} $K$}{ 
     % $\rho_{1,\text{last}} = \boldsymbol{\rho}_{\text{end}}$, $C_{\text{last}} = \boldsymbol{C}_{\text{end}}$.
     prev\textunderscore idx = cur\textunderscore index(end)
    
    \If{% 
      $\frac{\boldsymbol{C}_{\text{prev}\_\text{idx}}}{C_k} > \frac{\boldsymbol{\rho}_{\text{prev}\_\text{idx}}^2 - \rho_{1,k}^2}{\rho_{1,k}^2}$ 
    }{
        Continue to next iteration.
    }

        Compute $\xi$ via \eqref{eq:MFMC_sampling_cost_efficiency} for  
      [cur\textunderscore index, $k$].

      \If{$\xi\ge \xi_{\text{min}}$ or $\,\xi>1$}{ 
    \Return
    }
      
      \textbf{Backtrack} $(\, [\text{cur}\_\text{index},k],\xi, k+1)$.
  }
}
}
\vspace{3mm}



For $\mathcal{S}^*=\{\widehat u_{h,i}\}_{i=1}^{K^*}$, compute weights $\alpha_i$ for $i=2,...,K^*$ via \eqref{eq:MFMC_coefficients}. 

\caption{Multi-fidelity Model Selection with Backtracking Pruning--\JLcolor{Our version}}
\end{algorithm}
\ULforem




\normalem
\begin{algorithm}[!ht]
\label{algo:MFMC_Algo}
\DontPrintSemicolon

    
   \KwIn{Selected $K^*$ models $\widehat u_{h, k}$ in $\mathcal{S}^*$, parameters $\rho_{1,k}$, $\alpha_k$ and $C_k$ for each $\widehat u_{h, k}$,  tolerance $\epsilon$. }\vspace{1ex}
    
    \KwOut{Sample sizes $N_k$ for $K^*$ models, expectation 
    estimate $A^{\text{MF}}$.}\vspace{1ex}
    \hrule \vspace{1ex}
    

    Compute the sample size $N_k$ for $1\leq k\leq K^*$ by \eqref{eq:MFMC_SampleSize} and generate i.i.d. $N_1$ and $N_k-N_{k-1}$ samples for $k=2,\ldots, K^*$.

    Evaluate $\widehat u_{h, 1}$ to obtain $\widehat u_{h, 1}(\boldsymbol{\omega}^i)$ for $i = 1,\ldots,N_1$ and compute $A_{1,N_1}^{\text{MC}}$ by \eqref{eq:MC_estimator}.
    
    \For{$k = 2,\ldots,K^* $\,}{

    Evaluate $\widehat u_{h, k}$ to obtain $\widehat u_{h, k}(\boldsymbol{\omega}^i)$ for $i = 1,\ldots,N_{k-1}$ and compute $A_{k,N_{k-1}}^{\text{MC}}$ by \eqref{eq:MC_estimator}.

    Evaluate $\widehat u_{h, k}$ to obtain $\widehat u_{h, k}(\boldsymbol{\omega}^i)$ for $i = 1,\ldots,N_k-N_{k-1}$ and compute $A_{k,N_k\backslash N_{k-1}}^{\text{MC}}$ by \eqref{eq:MC_estimator}.

    % Store $N_{k-1}$ and $N_{k}-N_{k-1}$ samples as $N_k$ samples.
    }

    Compute $A^{\text{MF}}$ by \eqref{eq:MFMC_estimator_independent}.
    
\caption{Multifidelity Monte Carlo}
\end{algorithm}
\ULforem

% \normalem
% \begin{algorithm}[!ht]
% \label{algo:MFMC_Algo}
% \DontPrintSemicolon

    
%    \KwIn{Models $f_k$ in $\mathcal{S}^*$, parameters $\rho_k$, $\alpha_k$ and $C_k$ for each $f_k$ in $\mathcal{S}^*$,  tolerance $\epsilon$. }\vspace{1ex}
    
%     \KwOut{Sample sizes $N_k$ for $K^*$ models, expectation 
%     estimate $A^{\text{MFMC}}$.}\vspace{1ex}
%     \hrule \vspace{1ex}
%     Compute initial sample sizes $\boldsymbol{N}=[N_1,\ldots, N_{K^*}]$ using \eqref{eq:MFMC_SampleSize}. Set $\boldsymbol{N}_{\text{old}} = \boldsymbol{0}$ and $\boldsymbol{dN} = \boldsymbol{N}$. 
    
%     Initialize sample means $A_{1,N_1}^{\text{MC}}, A_{k,N_{k-1}}^{\text{MC}}, A_{k,N_k\backslash N_{k-1}}^{\text{MC}}=0. $
    
%     \While{$\sum_k dN_k>0$\,}{

%     Evaluate $dN_{1}$ samples for $f_1$ to obtain $f_1(\boldsymbol{\omega}^i)$. Update $A_{1,N_1}^{\text{MC}} = \frac{\boldsymbol{N}_{\text{old}}^1 A_{1,N_1}^{\text{MC}}+\sum_i f_1(\boldsymbol{\omega}^i)}{\boldsymbol{N}_{\text{old}}^1+dN_1}$ and $\sigma_1$.

%     Store $dN_1$ samples.
    
%     \For{$2\le k\le K^*$\,}{
    
%         % \For{$i = 1,\ldots,dN_k $\,}
%     % {
%     Evaluate previously stored $dN_{k-1}$ samples for $f_k$ to obtain $f_k(\boldsymbol{\omega}^i)$. Update $A_{k,N_{k-1}}^{\text{MC}} = \frac{\boldsymbol{N}_{\text{old}}^k A_{k,N_{k-1}}^{\text{MC}}+\sum_i f_k(\boldsymbol{\omega}^i)}{\boldsymbol{N}_{\text{old}}^k+dN_{k-1}}$. 
    
%     Collect new $dN_{k}-dN_{k-1}$ samples. Evaluate $f_k$ to obtain $f_k(\boldsymbol{\omega}^i)$. Update $A_{k,N_k\backslash N_{k-1}}^{\text{MC}} = \frac{\boldsymbol{N}_{\text{old}}^k A_{k,N_k\backslash N_{k-1}}^{\text{MC}}+\sum_i f_k(\boldsymbol{\omega}^i)}{\boldsymbol{N}_{\text{old}}^k+dN_{k}-dN_{k-1}}$. 

    
%     Compute $\sigma_k, \rho_{1,k}$.

%     Store $dN_{k-1}$ and $dN_{k}-dN_{k-1}$ samples as $dN_k$ samples.
    
%     \If{Condition (i) \& (ii) in Theorem \ref{thm:Sample_size_est} is not satisfied \,}{
%     Reselect models via Algorithm \ref{algo:MFMC_Algo_model_selection} with a larger sample size and restart.

%     Break. 
%     }

%     }
    
    
    
%     \vspace{4mm}
%     $\boldsymbol{N}_{\text{old}} \leftarrow \boldsymbol{N}$
    
%     Update $\alpha_k$ and the sample size $\boldsymbol{N}$ by \eqref{eq:MFMC_coefficients} 
%  and \eqref{eq:MFMC_SampleSize}.

%     $\boldsymbol{dN} \leftarrow \max \left\{\boldsymbol N-\boldsymbol N_{\text{old}}, \boldsymbol{0}\right\}.$

    
%     }
%     Compute $A^{\text{MFMC}}$ using $A_{1,N_1}^{\text{MC}}, A_{k,N_{k-1}}^{\text{MC}}, A_{k,N_k\backslash N_{k-1}}^{\text{MC}}$ and $\alpha_k$ from step 4, 7, 8, 15, by \eqref{eq:MFMC_estimator_independent}.
% \caption{Multi-fidelity Monte Carlo}
% \end{algorithm}
% \ULforem


% \begin{theorem}
% \label{thm:Sampling_cost_est}
% Let $f_k$ be $K$ models that satisfy the following conditions
% %
% \begin{alignat*}{8}
%     &(i)\;\; |\rho_{1,1}|>\ldots>|\rho_{1,K}|& \qquad \qquad
%     &(ii)\;\; \frac{C_{k-1}}{C_k}>\frac{\rho_{1,k-1}^2-\rho_{1,k}^2}{\rho_{1,k}^2-\rho_{1,k+1}^2},\;\;k=2,\ldots,K.
% \end{alignat*}
% %
% Suppose there exists $0<s<q<1$ such that 
% $C_k = c_s s^{k}$, $\rho_{1,k}^2 = q^{ k-1}$, then 
% \begin{equation*}
%     \mathcal{W}_\text{MFMC} = 
% \end{equation*}

% \end{theorem}
% \begin{proof}
% Since $q>s$, condition (ii) is satisfied.
% \begin{align*}
% \rho_{1,k}^2 - \rho_{1,k+1}^2&=q^k\left(\frac1 q-1\right),\quad \rho_{1,k-1}^2 - \rho_{1,k}^2=q^k\frac 1 q\left(\frac1 q-1\right)\\
%     \mathcal{W}_\text{MFMC} &= \frac{\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2\epsilon^2}\sum_{k=1}^K\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}\sum_{k=1}^K\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2,\\
%     &=\frac{\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2\epsilon^2} \sum_{k=1}^K\sqrt{q^{k}s^{ k}}\left(\sqrt{\frac{s(1-q)}{1-\rho_{1,2}^2}} + \sum_{k=2}^K\left(\sqrt{\frac{s^{k}}{q^{ k}}} - \sqrt{\frac{q s^{ k}}{s q^{ k}}}\right)q^{k} + \left(\sqrt{\frac{s^{ K}(1-q)}{q^{K}}}-\sqrt{\frac{q s^{ K}}{s q^{K}}}\right)q^{K}\right)\\
%     &\propto \frac{1}{\epsilon^2} \sum_{k=1}^K\left(q^{\frac{1}{2}}s^{\frac{1}{2}}\right)^k
% \end{align*}
    
% \end{proof}








\begin{theorem}
\label{thm:Sample_cost_est}
Let $\ell$ be the spatial grid nodes starting from 0 to $L$.
Let $u_{h,k}$ be $K$ models such that $u_{h,1}$ is the high fidelity model, $k\ge 2$ are the low fidelity models constructed using the sparse grid stochastic collocation method. Suppose there exist positive constants $\alpha, \beta, \gamma$ such that the following conditions are satisfied
%
\begin{alignat*}{8}
    &(i)\;\; |\rho_{1,1}|>\ldots>|\rho_{1,K}|,& \qquad \qquad
    &(ii)\;\; \frac{C_{k-1}}{C_k}>\frac{\rho_{1,k-1}^2-\rho_{1,k}^2}{\rho_{1,k}^2-\rho_{1,k+1}^2},\;\;k=2,\ldots,K, \quad \rho_{1,K+1}=0,\\
    &(iii)\;\; \left\Vert\mathbb{E}\left(u-u_{h,k}\right)\right\Vert_Z=\mathcal{O}\left( M_{L-k+1}^{-\alpha}\right),& \qquad \qquad
    &(iv)\;\; C_k=
    \left\{\begin{array}{ll}
    \mathcal{O}\left( M_{L}^{\gamma}\right), & k=1,\\
    \mathcal{O}\left( M_{L-k+1}^{\gamma_1}\right), & k\ge 2,\\
    \end{array}
\right.\\
    &(v)\;\; \rho_{1,k}^2-\rho_{1,k+1}^2=
    \left\{\begin{array}{ll}
    \mathcal{O}\left( M_{L}^{-\beta}\right), & k=1,\\
    \mathcal{O}\left( M_{L-k+1}^{-\beta_1}\right), & k\ge 2,\\
    \end{array}
\right.
\end{alignat*}
%
%
Then for $0<\epsilon$ there exist spatial grid level $K$ and sample size $N_\ell$ for which the multi-fidelity estimator $A^{\text{MF}}$ satisfies
\[
\left\Vert\mathbb{E}(u)-A^{\text{MF}} \right\Vert_{L^2(\boldsymbol W,Z)}<\epsilon\,\left\Vert\mathbb{E}(u) \right\Vert_{L^2( \boldsymbol W,Z)},
\]
with total sampling work bounded as
\begin{equation*}
    \mathcal{W}^{\text{MF}} \simeq \left\{\begin{array}{ll}
\epsilon^{-2}\left(c_1\epsilon^{\frac{\beta-\gamma}{2\alpha}}+c_2\mathcal{O}(1)\right)^2, & \beta_1>\gamma_1,\\
\epsilon^{-2}\left(c_1\epsilon^{\frac{\beta-\gamma}{2\alpha}}+c_2|\log\epsilon|\right)^2, & \beta_1=\gamma_1,\\
\epsilon^{-2}\left(c_1\epsilon^{\frac{\beta-\gamma}{2\alpha}}+c_2\epsilon^{-\frac{\gamma_1-\beta_1}{2\alpha}}\right)^2, & \beta_1<\gamma_1,
\end{array}
\right.
\end{equation*}

If $c_1\gg c_2$, then $\mathcal{W}^\text{MF} \simeq \epsilon^{-2+\frac{\beta-\gamma}{\alpha}}$.
\end{theorem}
\begin{proof}\label{eq:Sample_cost_est}

$C_1=W_L, C_k = W_{L-k+1}\sim M_{L-k+1}^\gamma\sim s^{(L-k+1)\gamma}$, K=L.

%
\begin{equation}
\label{eq:Geo_sum_for_s}
\sum_{k=0}^L s^{\eta k}=\left\{\begin{array}{ll}
\frac{1}{1-s^{\eta}}=\mathcal{O}\left(1\right), & \eta<0,\\
L+1 = \mathcal{O}\left(|\log \epsilon|\right), & \eta = 0,\\
\mathcal{O}\left(\epsilon^{-\frac{\eta}{\alpha}}\right), & \eta>0,
\end{array}
\right.
\end{equation}
%

Note that if all candidate models are selected as low fidelity models, then $K=L+1$. In general, $K\sim L$.
%
\begin{align*}
    \mathcal{W}^\text{MF} &\sim \epsilon^{-2} \left(\sum_{k=1}^K\sqrt{C_k\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)}\right)^2 \sim \epsilon^{-2}\left(\sqrt{C_1\left(\rho_{1,1}^2 - \rho_{1,2}^2\right)}+\sum_{k=2}^K \sqrt{C_k\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)} \right)^2\\
    &\sim \epsilon^{-2} \left(M_L^{\frac{\gamma-\beta}{2}}+\sum_{k=2}^{L+1}M_{L-k+1}^\frac{\gamma_1-\beta_1}{2}\right)^2=\epsilon^{-2} \left(M_L^{\frac{\gamma-\beta}{2}}+\sum_{p=0}^{L-1}M_{p}^\frac{\gamma_1-\beta_1}{2}\right)^2\sim \epsilon^{-2}\left(s^{\frac{(\gamma-\beta)}{2}L}+\sum_{p=0}^{L-1}s^{\frac{(\gamma_1-\beta_1)}{2}p}\right)^2,
\end{align*}
%
From \eqref{eq:SLSGC_MLS_SpatialGridsNo}, $s^{\frac{(\gamma-\beta)}{2}L}\sim \epsilon^{\frac{\beta-\gamma}{2\alpha}}$. The term $\sum_{p=0}^{L-1}s^{\frac{(\gamma_1-\beta_1)}{2}p}$ can be determined from \eqref{eq:Geo_sum_for_s}. Therefore, 
\[
\mathcal{W}^\text{MF} \simeq \left\{\begin{array}{ll}
\epsilon^{-2}\left(c_1\epsilon^{\frac{\beta-\gamma}{2\alpha}}+c_2\mathcal{O}(1)\right)^2, & \beta_1>\gamma_1,\\
\epsilon^{-2}\left(c_1\epsilon^{\frac{\beta-\gamma}{2\alpha}}+c_2|\log\epsilon|\right)^2, & \beta_1=\gamma_1,\\
\epsilon^{-2}\left(c_1\epsilon^{\frac{\beta-\gamma}{2\alpha}}+c_2\epsilon^{-\frac{\gamma_1-\beta_1}{2\alpha}}\right)^2, & \beta_1<\gamma_1,
\end{array}
\right.
\]
If $c_1\gg c_2$, then $\mathcal{W}^\text{MF} \sim \epsilon^{-2+\frac{\beta-\gamma}{\alpha}}$.


% The cancellation of the two successive terms in the ratio representation on both sides of \eqref{eq:Theorem_cond_ii} indicates that 
% \[
% \frac{B_1}{C_1^2}C_k^2\le B_k\le \frac{B_K}{C_K^2}C_k^2, \quad k=1,\ldots,K.
% \]
% This indicates that
% \[
% \sqrt{\frac{1-\rho_{1,2}^2}{C_1}}\sum_{k=1}^K C_k\le \sum_{k=1}^K\sqrt{B_k}\le \sqrt{\frac{\rho_{1,K}^2}{C_K}}\sum_{k=1}^K C_k, \quad \text{or} \quad \sum_{k=1}^K
% \sqrt{B_k}\simeq \sum_{k=1}^K C_k,
% \]
% where $A\simeq B$ represents $a_1 B\le A\le a_2 B$ for positive $A$ and $B$, with constants $a_1, a_2$ independent of sample size $N_k$ but depends on model number $K$. This indicates that the sum in \eqref{eq:sampling_cost_bound} behaves in a similar style as $\sum_{k=1}^K C_k$. If $\epsilon$ is chosen sufficiently small, we can ignore the small term $\sum_{k=1}^K C_k$ in  \eqref{eq:sampling_cost_bound}. 





% Since $K$ is independent of $\epsilon$,  the total sampling cost behaves like $\epsilon^{-2}$. 

% Moreover, the inequality \eqref{eq:Theorem_cond_ii} also indicates that
% \[
% 0<\frac{C_L}{\sqrt{B_L}}\le \frac{C_k}{\sqrt{B_k}}<\frac{C_{k-1}}{\sqrt{B_{k-1}}}\le\frac{C_1}{\sqrt{B_1}},\quad k=2,\dots, L
% \]
% Therefore, the sequence $\frac{C_k}{\sqrt{B_k}} - \frac{C_{k-1}}{\sqrt{B_{k-1}}}\in (\frac{C_L}{\sqrt{B_L}} - \frac{C_{1}}{\sqrt{B_{1}}},0)$  is bounded. 

% Note that
% \[
% H_1:=\sum_{k=1}^L\sqrt{B_k}, \quad \sqrt{\frac{1-\rho_{1,2}^2}{C_1}}\sum_{k=1}^L C_k\le \sum_{k=1}^L\sqrt{B_k}\le \sqrt{\frac{\rho_{1,L}^2}{C_L}}\sum_{k=1}^L C_k, \quad H_1\uparrow \sqrt{\frac{\rho_{1,L}^2}{C_L}}\sum_{k=1}^L C_k \;\;\text{as}\;\; L\rightarrow \infty
% \]
% \[
% H_2:=\sum_{j=1}^L\left(\frac{C_j}{\sqrt{B_j}} - \frac{C_{j-1}}{\sqrt{B_{j-1}}}\right)\rho_{1,j}^2, \quad  0<H_2\le \sqrt{\frac{C_1}{1-\rho_{1,2}^2}}, \quad H_2\downarrow 0 \;\;\text{as}\;\; L\rightarrow \infty.
% \]


\end{proof}