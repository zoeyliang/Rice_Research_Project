% ====================================================
\section{Appendix}\label{sec:Appendix}
% ====================================================
\subsection{Proof of Theorem \ref{thm:Sample_size_est}}
\begin{proof}
The optimization problem is approached with the method of Lagrange multipliers, where the auxiliary Lagrangian function $L$ incorporates multipliers $\lambda_0,\ldots, \lambda_K$ to enforce the constraints on the variance and sample sizes
%
\[
L = \sum_{k=1}^K C_kN_k +\lambda_0 \left(\frac{\sigma_1^2}{N_1} + \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\left(\alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k\right)\right)-\lambda_1 N_1+\sum_{k=2}^K\lambda_k(N_{k-1} - N_k),
\]
%
To solve this, we apply the Karush-Kuhn-Tucker (KKT) conditions for \eqref{eq:Optimization_pb_sample_size}, leading to a system of equations for the partial derivatives of the Lagrangian with respect to the optimization variables $\alpha_k$ and $N_k$,  in addition to  primal feasibility, dual feasibility, and complementary slackness conditions
%
\begin{align*}
\frac{\partial L}{\partial \alpha_j}=0,\quad \frac{\partial L}{\partial N_k}&=0,\quad j=2\ldots,K, \quad k=1\ldots,K,\\
\mathbb{V}\left(A^{\text{MF}}\right)- \epsilon_{\text{tar}}^2 &= 0,\\
   -N_1\le 0,\qquad N_{k-1}-N_k&\le 0, \quad k=2\ldots,K,\\
    \lambda_k &\ge 0,\quad k=1\ldots,K,\\ 
    \lambda_1 N_1=0,\qquad\lambda_k(N_{k-1}-N_k)&=0,\quad k=2\ldots,K.
\end{align*}
%
The partial derivatives of the Lagrangian with respect to $\alpha_k$ and $N_k$ are computed as
%
\begin{align*}
    \frac{\partial L}{\partial \alpha_k}&=\lambda_0\left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\left(2\alpha_k\sigma_k^2 - 2\rho_{1,k}\sigma_1\sigma_k\right),\quad k=2,\dots,K,\\
    % \frac{\partial L}{\partial N_1}&=C_1 + \lambda_0\left(-\frac{\sigma_1^2}{N_1^2} - \frac{\alpha_2^2\sigma_2^2-2\alpha2\rho_{1,2}\sigma_1\sigma_2}{N_1^2}\right)-\lambda_1+\lambda_2,\\
    \frac{\partial L}{\partial N_k}&=C_k+\lambda_0\left(\frac{\alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k}{N_k^2}-\frac{\alpha_{k+1}^2\sigma_{k+1}^2 - 2\alpha_{k+1}\rho_{1,k+1}\sigma_1\sigma_{k+1}}{N_k^2}\right)-\lambda_k+\lambda_{k+1}, \quad k=1,\dots,K,
    % \frac{\partial L}{\partial N_K}&=C_K + \lambda_0\left(\frac{\alpha_K^2\sigma_K^2 - 2\alpha_K\rho_{1,K}\sigma_1\sigma_K}{N_K^2}\right)-\lambda_K.
\end{align*}
%
where $\alpha_1 = 1, \alpha_{K+1} = 0$ and $\lambda_{K+1} = 0$. By solving the equation $\partial L/\partial \alpha_k=0$, the optimal weights $\alpha_k^*$ are determined as
%
\[
\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k}.
\]
%
Substituting $\alpha_k^*$ into $\partial L/\partial N_k=0$ yields a representation for $C_k$ in terms of the sample sizes $N_k$, variance contributions, correlation coefficients, and the Lagrangian multipliers
% %
% \begin{equation*}
%     C_k=\left\{ \begin{array}{ll}
% \frac{\lambda_0\sigma_1^2}{N_k^2}\left(\rho_{1,k}^2-\rho_{1,k+1}^2\right)+\lambda_k-\lambda_{k+1}, & \text{ for }\; k=1,\ldots,K-1, \\
% \frac{\lambda_0\sigma_1^2}{N_k^2}\rho_{1,k}^2+\lambda_k, & \text{ for }\; k=K.
% \end{array}\right.
% \end{equation*}
% %
\begin{equation*}
    C_k=\frac{\lambda_0\sigma_1^2}{N_k^2}\left(\rho_{1,k}^2-\rho_{1,k+1}^2\right)+\lambda_k-\lambda_{k+1}, \;\text{ for }\; k=1,\ldots,K, 
\end{equation*}
where $\rho_{1,K+1} = 0$. As shown by  \cite{PeWiGu:2016}, the global minimizer is achieved when the inequality constraints are inactive ($\lambda_k$=0, $k=1,\dots, K$) in the complementary slackness conditions, indicating that the sample sizes strictly increase with $k$ ($N_{k-1}< N_k$ for $k=2,\ldots, K$). This results in the expression for the optimal sample sizes
% \[
% N_1 = \sigma_1\sqrt{\lambda_0}\sqrt{\frac{1-\rho_{1,2}^2}{C_1}}, \quad N_k = \sigma_1\sqrt{\lambda_0}\sqrt{\frac{\rho_{1,k}^2-\rho_{1,k+1}^2}{C_k}}, \quad N_K = \sigma_1\sqrt{\lambda_0}\sqrt{\frac{\rho_{1,K}^2}{C_K}},
% \]
% or we can simplify the notation as
\begin{equation}
\label{eq:sample_size_1}
    N_k = \sigma_1\sqrt{\lambda_0}\sqrt{\frac{\rho_{1,k}^2-\rho_{1,k+1}^2}{C_k}},\quad \text{for}\quad  k=1,\ldots,K.
\end{equation}
% \frac{1}{N_k} = \frac{1}{\sigma_1\sqrt{\lambda_0}}\sqrt{\frac{C_k}{\rho_{1,k}^2-\rho_{1,k+1}^2}},
Using the optimal coefficients $\alpha_k^*$ and sample size estimations $N_k$ in \eqref{eq:sample_size_1}, the variance \eqref{eq:MFMC_variance} of the multi-fidelity Monte Carlo estimator becomes
%
\begin{equation*} \label{eq:MFMC_variance2}
    \mathbb{V}\left(A^{\text{MF}}\right) = \frac{\sigma_1}{\sqrt{\lambda_0}}\sum_{k=1}^K\sqrt{C_k\left(\rho_{1,k}^2-\rho_{1,k+1}^2\right)}, \quad \text{for}\quad  k=1,\ldots,K.
\end{equation*}
%
Using the constraint that variance satisfies, we solve for the value of 
% $\sqrt{\lambda_0}=\sigma_1/(\epsilon^2(1-\theta)\left\Vert\mathbb{E}(u) \right\Vert_{Z}^2)\sum_{k=1}^K\sqrt{C_k(\rho_{1,k}^2-\rho_{1,k+1}^2)}$ 
%
\[
\sqrt{\lambda_0} = \frac{\sigma_1}{\epsilon_{\text{tar}}^2}\sum_{k=1}^K\sqrt{C_k\left(\rho_{1,k}^2-\rho_{1,k+1}^2\right)}.
\]
%
Substituting $\sqrt{\lambda_0}$ into \eqref{eq:sample_size_1}, we derive the optimal sample sizes as
%
\[
N_k^* = \frac{\sigma_1^2}{\epsilon_{\text{tar}}^2}\sqrt{\frac{\rho_{1,k}^2-\rho_{1,k+1}^2}{C_k}}\sum_{j=1}^K\sqrt{C_j\left(\rho_{1,j}^2-\rho_{1,j+1}^2\right)},\quad \text{for}\quad  k=1,\ldots,K.
\]
%
Note that by ensuring the condition $(ii)$ is satisfied, we can guarantee that $N_k^*$ increases strictly as $k$ grows. 
\end{proof}
%

\begin{lemma}\label{lemma:Y_k_Y_j}
$Y_k$ and $Y_j$ are uncorrelated  for $2\le k<j\le K$.


\begin{proof}
For $2\le k<j\le K$, we have $N_{k-1}\le N_k\le N_{j-1}$, we partition $N_{j-1}$ samples into three subsets of sizes $N_{k-1}$, $N_{k}-N_{k-1}$, and $N_{j-1} - N_{k}$, the samples in these three subsets are independent. From the definition of $Y_k$ in \eqref{eq:MFMC_Yk}
\begin{align*}
    \text{Cov}\left[Y_k,Y_j\right] &= \left(\frac{N_{k-1}}{N_k}-1\right) \left(\frac{N_{j-1}}{N_j}-1\right)\text{Cov}\left[A_{k, N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}} - A_{j,N_{j}\backslash N_{j-1}}^{\text{MC}}\right] \\
    & = M \left(\text{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}}\right] - \text{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j}\backslash N_{j-1}}^{\text{MC}}\right] \right)\\
    & = M \text{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}}\right]
\end{align*}
where $M = (N_{k-1}/N_k-1) (N_{j-1}/N_j-1)$. The second term disappears since samples in $A_{j,N_{j}\backslash N_{j-1}}^{\text{MC}}$ are independent with the samples in $A_{k,N_{k-1}}^{\text{MC}}$ and $A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}$. Moreover, we can express $A_{j,N_{j-1}}^{\text{MC}}$ as
%
\begin{equation*}
    A_{j,N_{j-1}}^{\text{MC}} = \frac{N_{k-1}}{N_{j-1}}A_{j,N_{k-1}}^{\text{MC}} + \frac{N_k - N_{k-1}}{N_{j-1}} A_{j,N_{k}\backslash N_{k-1}}^{\text{MC}} + \frac{N_{j-1} - N_k}{N_{j-1}} A_{j,N_{j-1}\backslash N_{k}}^{\text{MC}}.
\end{equation*}
%

\begin{align*}
    \frac{\text{Cov}\left[Y_k,Y_j\right]}{M} &= \text{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}}\right]\\
    % &= \text{Cov}\left[A_{N_{k-1}}^k, \frac{N_{k-1}}{N_{j-1}}A_{N_{k-1}}^j + \frac{N_k - N_{k-1}}{N_{j-1}} A_{N_{k}\backslash N_{k-1}}^j + \frac{N_{j-1} - N_k}{N_{j-1}} A_{N_{j-1}\backslash N_{k}}^j\right] \\
    % &- \text{Cov}\left[ A_{N_{k}\backslash N_{k-1}}^k, \frac{N_{k-1}}{N_{j-1}}A_{N_{k-1}}^j + \frac{N_k - N_{k-1}}{N_{j-1}} A_{N_{k}\backslash N_{k-1}}^j + \frac{N_{j-1} - N_k}{N_{j-1}} A_{N_{j-1}\backslash N_{k}}^j\right]\\
    &=\text{Cov}\left[A_{k,N_{k-1}}^{\text{MC}}, \frac{N_{k-1}}{N_{j-1}}A_{j,N_{k-1}}^{\text{MC}}\right]-\text{Cov}\left[ A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, \frac{N_k - N_{k-1}}{N_{j-1}} A_{j,N_{k}\backslash N_{k-1}}^{\text{MC}} \right]\\
    &=\frac{\text{Cov}\left[N_{k-1}A_{k,N_{k-1}}^{\text{MC}}, N_{k-1} A_{j,N_{k-1}}^{\text{MC}}\right]}{N_{j-1}N_{k-1}}-\frac{\text{Cov}\left[(N_k-N_{k-1}) A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, (N_k - N_{k-1}) A_{j,N_{k}\backslash N_{k-1}}^{\text{MC}} \right]}{N_{j-1}(N_k-N_{k-1})}\\
    &=\frac{\text{Cov}\left[\sum_{i=1}^{N_{k-1}}u_{h,k}\left(\cdot, \boldsymbol{\omega}^{(i)}\right),\sum_{i=1}^{N_{k-1}}u_{h,j}\left(\cdot, \boldsymbol{\omega}^{(i)}\right)\right]}{N_{j-1}N_{k-1}}\\
    &-\frac{\text{Cov}\left[\sum_{i=1}^{N_k-N_{k-1}}u_{h,k}\left(\cdot, \boldsymbol{\omega}^{(i)}\right), \sum_{i=1}^{N_k-N_{k-1}}u_{h,j}\left(\cdot, \boldsymbol{\omega}^{(i)}\right)\right]}{N_{j-1}(N_k-N_{k-1})}\\
    &=\frac{\sum_{i=1}^{N_{k-1}}\text{Cov}\left[u_{h,k}\left(\cdot, \boldsymbol{\omega}^{(i)}\right),u_{h,j}\left(\cdot, \boldsymbol{\omega}^{(i)}\right)\right]}{N_{j-1}N_{k-1}} -\frac{\sum_{i=1}^{N_k-N_{k-1}}\text{Cov}\left[u_{h,k}\left(\cdot, \boldsymbol{\omega}^{(i)}\right), u_{h,j}\left(\cdot, \boldsymbol{\omega}^{(i)}\right)\right]}{N_{j-1}(N_k-N_{k-1})}\\
    &=\frac{N_{k-1}\rho_{k,j}\sigma_k\sigma_j}{N_{j-1}N_{k-1}}-\frac{(N_k-N_{k-1})\rho_{k,j}\sigma_k\sigma_j}{N_{j-1}(N_k-N_{k-1})}=0
\end{align*}
\end{proof}


\end{lemma}