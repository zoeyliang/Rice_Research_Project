% ====================================================
\section{Iterative sample size estimation for MFMC}\label{sec:Iterative_IntegerValued_Sample_Size}
% ====================================================

To address this challenge, we develop an iterative scheme for MFMC sample size estimation grounded in dynamic programming principles, specifically using Bellman's principle of optimality \cite{Be:1957}. This approach ensures consistency with the continuous optimal allocation while enabling sequential decision-making that naturally accommodates integer constraints in practical implementations. 

\subsection{Theoretical Justification for Dynamic Programming Decomposition}

The decomposition of the original optimization problem \eqref{eq:Optimization_pb_sample_size_reduced} into sequential subproblems is justified by the problem's mathematical structure and fundamental optimization principles:


\begin{enumerate}

    \item \textbf{Optimal Substructure and Separability}: The objective function $\sum_{k=1}^K \frac{\Delta_k}{N_k}$ exhibits complete separability across fidelity levels, while the budget constraint $\sum_{k=1}^K C_kN_k = p$ admits additive decomposition. This separable structure ensures that the optimal solution possesses the key property that any subsequence of decisions must be optimal for the corresponding subproblem with the remaining budget.

    \item \textbf{Bellman's Principle of Optimality}: If $\{N_1^*, N_2^*, \ldots, N_K^*\}$ constitutes an optimal solution to the global problem, then for any $k \in \{1,\ldots,K\}$, the truncated sequence $\{N_k^*, N_{k+1}^*, \ldots, N_K^*\}$ must be an optimal solution to the subproblem defined on levels $k$ through $K$ with residual budget $R_k = p - \sum_{j=1}^{k-1} C_j N_j^*$. This principle follows directly from the contradiction that would arise if a superior partial solution existed for any subproblem.

    \item \textbf{State Variable Sufficiency}: The recursive definition of the state variable $R_k$ 
    \[
    R_1 := p, \qquad R_k := p - \sum_{j=1}^{k-1} C_j H_j, \quad k=2,\ldots,K.
    \]
    as the remaining computational budget captures all essential information from previous decisions. The Markovian property holds because future optimization depends only on the current remaining budget $R_k$ and not on the specific history of allocations that led to this state.


    \item \textbf{Sequential Decision Framework}: At each stage $k$, given the fixed decisions $\{H_1, \ldots, H_{k-1}\}$ and current state $R_k$, the optimization over remaining levels $\{H_k, \ldots, H_K\}$ constitutes a well-defined subproblem that preserves the original problem's structure:
    \begin{equation}\label{eq:Sequential_Optimization}
        \begin{array}{ll}
        \displaystyle 
        \min_{H_k,\ldots,H_K\in \mathbb{R}} & 
            \displaystyle 
            \sum_{j=k}^K \frac{\Delta_j}{H_{j}}, \\
        \text{subject to} &
            \displaystyle \sum_{j=k}^K C_j H_j = R_k,\\
            &H_k\ge 0,\quad H_{j-1}-H_j\le 0,\;\; j=k+1,\ldots,K.
        \end{array}
    \end{equation}

    \item \textbf{Constraint Preservation}: The sequential formulation inherently maintains all original constraints. The budget constraint is preserved through the recursive budget update, while the monotonicity constraints $H_{j-1} \leq H_j$ are enforced locally at each stage, ensuring global consistency with the MFMC hierarchy requirements.
\end{enumerate}

\subsection{Convexity and Global Optimality}

The convexity of each subproblem in \eqref{eq:Sequential_Optimization} is fundamental to guaranteeing global optimality. Since $\Delta_j > 0$ and $H_j\ge 0$, the objective function $\sum_{j=k}^K \frac{\Delta_j}{H_j}$ is strictly convex on the positive orthant, while the budget constraint is linear and the monotonicity constraints form a convex polyhedron. This convexity ensures that:
\begin{itemize}
    \item Local optimal solutions are globally optimal
    \item The sequential dynamic programming approach cannot be trapped in suboptimal local minima
    \item The KKT conditions provide both necessary and sufficient conditions for optimality
\end{itemize}


Applying the optimality conditions derived in Theorem~\ref{thm:Sample_size_est} to each subproblem yields the recursive solution:
\begin{equation*}
    H_k^* = \sqrt{\frac{\Delta_k}{C_k}} \frac{R_k}{\sum_{j=k}^K\sqrt{C_j\Delta_j}},
    \qquad 
    R_{k+1} = R_k - C_k H_k^*.
\end{equation*}
Unfolding this recursion provides the explicit forward iterative process:
\begin{equation}\label{eq:MFMC_New_RealValued_Sample_Size}
    H_1^* = \sqrt{\frac{\Delta_1}{C_1}} \frac{p}{\sum_{j=1}^K\sqrt{C_j\Delta_j}}, 
    \qquad 
    H_k^* = \sqrt{\frac{\Delta_k}{C_k}} \frac{p-\sum_{j=1}^{k-1}C_jH_j^*}{\sum_{j=k}^K\sqrt{C_j\Delta_j}}, 
    \quad k = 2,\ldots, K.
\end{equation}


This iterative scheme possesses several desirable properties: it maintains feasibility at every step, enables progressive budget allocation, provides computational advantages by decomposing a high-dimensional constrained optimization into a sequence of lower-dimensional problems, and maintains guaranties of global optimality through the dynamic programming framework.

%
\begin{theorem}[Monotonicity of the iterative formulation]\label{thm:Monotonicity_H_k}
Under the assumptions of Theorem \ref{thm:Sample_size_est}, the iteratively defined sample sizes $H_k^*$ in \eqref{eq:MFMC_New_RealValued_Sample_Size} is monotonically decreasing for all $K$.
\end{theorem}
%
\begin{proof}
Let $S_k = \sum_{j=k}^K \sqrt{C_j\Delta_j}$. Then
\[
S_{k-1} = \sqrt{C_{k-1}\Delta_{k-1}}+S_k, \quad H_k^* = \sqrt{\frac{\Delta_k}{C_k}}\frac{R_k}{S_k}
\]
\[
R_k = R_{k-1}-C_{k-1}H_{k-1}^* = R_{k-1}\left(1-\frac{\sqrt{C_{k-1}\Delta_{k-1}}}{S_{k-1}}\right) = R_{k-1}\frac{S_k}{S_{k-1}}
\]
Therefore, 
\[
\frac{R_k}{S_k} = \frac{R_{k-1}}{S_{k-1}}
\]
Therefore,
\[
H_k^* = \sqrt{\frac{\Delta_k}{C_k}}\frac{R_k}{S_k}>\sqrt{\frac{\Delta_{k-1}}{C_{k-1}}}\frac{R_{k-1}}{S_{k-1}} = H_{k-1}^*, \quad k=2,\ldots,K
\]

\end{proof}


\subsection{Equivalence with Continuous MFMC Solution}

The following theorem establishes the fundamental equivalence between the iterative formulation and the standard MFMC allocation, ensuring that the sequential approach preserves all optimality properties of the original continuous solution.

%
\begin{theorem}[Equivalence of iterative and standard MFMC formulations]\label{thm:MFMC_Iterative_RealValued_Sample_Size}
\JLcolor{Under the assumptions of Theorem \ref{thm:Sample_size_est}},
The iteratively defined sample sizes $H_k^*$ in \eqref{eq:MFMC_New_RealValued_Sample_Size} are identical to the standard real-valued MFMC sample sizes $N_k^*$ in \eqref{eq:MFMC_RealValued_Sample_Size}, i.e.,
\[
H_k^* = N_k^*
    = \sqrt{\frac{\Delta_k}{C_k}}\,
      \frac{p}{\sum_{j=1}^K \sqrt{C_j\Delta_j}}.
\]
Moreover, this iterative scheme preserves both the total computational budget and the optimal variance reduction:
\[
\sum_{k=1}^K C_k H_k^* = p, 
\qquad  
f(H_k^*) = \sum_{k=1}^K \frac{\Delta_k}{H_k^*} = \frac{1}{p} \left(\sum_{k=1}^K \sqrt{C_k\Delta_k}\right)^2.
\]
\end{theorem}
%


\begin{proof}
Define the cumulative cost and remaining budget by
\[
    T_k = \sum_{j=1}^k C_j H_j^*, 
    \qquad 
    R_k = p - T_k,
\]
so that $R_0 = p$ and $T_0 = 0$. For $k=1$, from \eqref{eq:MFMC_New_RealValued_Sample_Size} we obtain
\[
    T_1 = C_1H_1^* 
    = p\,\frac{\sqrt{C_1\Delta_1}}{\sum_{j=1}^K \sqrt{C_j\Delta_j}},
    \qquad
    R_1 = p - T_1
    = p\,\frac{\sum_{j=2}^K \sqrt{C_j\Delta_j}}{\sum_{j=1}^K \sqrt{C_j\Delta_j}}.
\]

For general $k\ge 1$, the iterative definition \eqref{eq:MFMC_New_RealValued_Sample_Size} gives
\[
    H_k^*
    = \sqrt{\frac{\Delta_k}{C_k}}\,
      \frac{R_{k-1}}{\sum_{j=k}^K \sqrt{C_j \Delta_j}},
\]
which implies that the remaining budget satisfies
\[
    R_k 
    = R_{k-1} - C_k H_k^*
    = \frac{\sum_{j=k+1}^K \sqrt{C_j \Delta_j}}
           {\sum_{j=k}^K \sqrt{C_j \Delta_j}} \, R_{k-1}.
\]
Hence, $\{R_k\}_{k=1}^K$ forms a geometric sequence. Introducing the normalization factor
\begin{equation}\label{eq:aggregate_cost_variance_weight_S}
    S := \sum_{j=1}^K \sqrt{C_j \Delta_j},
\end{equation}
we obtain by recursion that
\[
    R_k = \frac{p}{S}\sum_{j=k+1}^K\sqrt{C_j\Delta_j}.
\]
Substituting this expression into the formula for $H_k^*$ yields
\[
    H_k^*
    = \frac{p}{S}\sqrt{\frac{\Delta_k}{C_k}},
\]
which coincides with the closed-form MFMC sample size in \eqref{eq:MFMC_RealValued_Sample_Size}.  
In particular, for $k=K$, we have $R_K=0$, and thus $T_K = p - R_K = p$, verifying that the total cost constraint $\sum_{k=1}^K C_k H_k^* = p$ is satisfied.  
Finally,
\[
    f(H_k^*)
    = \sum_{k=1}^K \frac{\Delta_k}{H_k^*}
    = \frac{1}{p} \left(\sum_{k=1}^K \sqrt{C_k\Delta_k}\right)^2,
\]
which completes the proof.
\end{proof}

Theorem~\ref{thm:MFMC_Iterative_RealValued_Sample_Size} establishes that the iterative construction preserves the optimality properties of the continuous MFMC allocation. This sequential framework naturally extends to {\it integer-valued} sample sizes, which are essential for practical implementations. We now develop an iterative scheme for computing such integer-valued allocations, using the continuous formulation as a foundation.

The key idea is to maintain the analytical structure of the continuous optimizer while enforcing integer feasibility through a forward recursive process. Unlike the direct flooring of the continuous solution -- which may lead to suboptimal budget utilization due to independent rounding -- the iterative approach updates the residual budget using the accumulated integer costs from preceding levels. This ensures that the allocation at each step adapts to the actual remaining resources. The recursive definition is given by
%
\begin{equation}\label{eq:MFMC_New_IntegerValued_Sample_Size}
    M_1^* = \sqrt{\frac{\Delta_1}{C_1}}\frac{p}{\sum_{j=1}^K\sqrt{C_j\Delta_j}}, 
    \qquad 
    M_k^* = \sqrt{\frac{\Delta_k}{C_k}}\frac{p-\sum_{j=1}^{k-1}C_j\left\lfloor M_j^* \right\rfloor}{\sum_{j=k}^K\sqrt{C_j\Delta_j}}, 
    \quad k = 2,\ldots, K,
\end{equation}
%
with the integer-valued sample sizes taken as $\lfloor M_k^* \rfloor$ for $k=1,\ldots,K$.

The procedure begins with the continuous allocation $M_1^*$, from which $\lfloor M_1^* \rfloor$ is derived. The cost of this integer allocation is deducted from the total budget, and the process repeats for subsequent levels using the updated residual budget. By construction, this sequential method guarantees that the total cost constraint is satisfied exactly, as the residual budget at each step reflects the actual integer expenditures. Moreover, it preserves the relative variance–cost balancing of the continuous optimizer, as the allocation weights remain proportional to $\sqrt{\Delta_k/C_k}$.


To ensure that each fidelity level contributes at least one sample, the total budget must satisfy
%
\begin{equation}\label{eq:p_bound}
    p \ge \sum_{k=1}^K C_k.
\end{equation}
%
This condition is necessary and sufficient for feasibility, as it prevents over-allocation that would violate the monotonicity constraints or lead to insufficient samples at higher fidelity levels.

The following theorem demonstrates that the iterative integer-valued scheme achieves superior budget utilization compared to direct flooring, formalizing the advantage of adaptive resource allocation.
%
\begin{theorem}[Cost bound for the iterative integer-valued sample allocation]
\label{thm:MFMC_New_IntegerValued_Cost} 
Let $\lfloor M_k^* \rfloor$ denote the integer-valued sample sizes from the iterative scheme \eqref{eq:MFMC_New_IntegerValued_Sample_Size}, and let $\lfloor N_k^* \rfloor$ denote those from direct flooring of the continuous solution \eqref{eq:MFMC_RealValued_Sample_Size}. Under the feasibility condition \eqref{eq:p_bound}, the total costs satisfy
\begin{equation}\label{eq:Iterative_integer_sample_size_cost_bound}
    \sum_{k=1}^K C_k \left\lfloor N_k^* \right\rfloor
    \;\le\;
    \sum_{k=1}^K C_k \left\lfloor M_k^* \right\rfloor
    \;\le\;
    p.
\end{equation}
\end{theorem}
%












\begin{proof}
We first show that the total cost of the iterative scheme does not exceed the prescribed budget,
\[
\sum_{k=1}^K C_k \left\lfloor M_k^* \right\rfloor \le p.
\]
Define the cumulative integer cost up to level $k$ as
\[
T_k = \sum_{j=1}^k C_j\left\lfloor M_j^* \right\rfloor.
\]
Since $\lfloor M_j^* \rfloor \le M_j^*$, we claim, and prove by induction, that for each $k = 1, \ldots, K$,
\begin{equation}\label{eq:Tk_bound}
T_k \le \frac{p}{S}\sum_{j=1}^k \sqrt{C_j \Delta_j}.
\end{equation}
where $S$ is the aggregate cost–variance weight defined in \eqref{eq:aggregate_cost_variance_weight_S}. 
Inequality \eqref{eq:Tk_bound} bounds the cumulative integer cost at level $k$ by a proportional share of the total budget, scaled by $S$.







The base case $k=1$ follows immediately,
\[
T_1=C_1 \left\lfloor M_1^* \right\rfloor \le C_1M_1^* = \frac{p}{S}\sqrt{C_1\Delta_1},
\]
so \eqref{eq:Tk_bound} holds for \(k=1\). Assume \eqref{eq:Tk_bound} holds for \(k-1\). By definition of \(M_k^*\),
%
\[
M_k^* = \sqrt{\frac{\Delta_k}{C_k}}\frac{p - T_{k-1}}{\sum_{j=k}^K \sqrt{C_j\Delta_j}},
\]
%
and hence
%
\[
C_k \left\lfloor M_k^* \right\rfloor \le C_k M_k^*  = \sqrt{C_k\Delta_k}\frac{p-T_{k-1}}{\sum_{j=k}^K\sqrt{C_j\Delta_j}}.
\]
%
Using the inductive hypothesis and simplifying yields
\begin{align*}
    T_k &= T_{k-1}+C_k\left\lfloor M_k^* \right\rfloor \\
    &\le T_{k-1} + \sqrt{C_k\Delta_k}\frac{p-T_{k-1}}{\sum_{j=k}^K\sqrt{C_j\Delta_j}}
    =T_{k-1}\left(1-\frac{\sqrt{C_k\Delta_k}}{\sum_{j=k}^K\sqrt{C_j\Delta_j}}\right) + p\frac{\sqrt{C_k\Delta_k}}{\sum_{j=k}^K\sqrt{C_j\Delta_j}}\\
    &\le \frac{p}{S}\sum_{j=1}^{k-1} \sqrt{C_j\Delta_j}\frac{\sum_{j=k+1}^K\sqrt{C_j\Delta_j}}{\sum_{j=k}^K\sqrt{C_j\Delta_j}}+p\frac{\sqrt{C_k\Delta_k}}{\sum_{j=k}^K\sqrt{C_j\Delta_j}}=\frac{p}{S}\cdot \frac{\sum_{j=1}^{k-1} \sqrt{C_j\Delta_j}\sum_{j=k+1}^K\sqrt{C_j\Delta_j}+S\sqrt{C_k\Delta_k}}{\sum_{j=k}^K\sqrt{C_j\Delta_j}}\\
    &=\frac{p}{S}\cdot \frac{\sum_{j=1}^{k-1} \sqrt{C_j\Delta_j}\sum_{j=k+1}^K\sqrt{C_j\Delta_j}+\sqrt{C_k\Delta_k}\left(\sum_{j=1}^{k-1}\sqrt{C_j\Delta_j}+\sum_{j=k}^K\sqrt{C_j\Delta_j}\right)}{\sum_{j=k}^K\sqrt{C_j\Delta_j}}
    %=\frac{p}{S}\frac{\sum_{j=k}^K\sqrt{C_j\Delta_j}\sum_{j=1}^k\sqrt{C_j\Delta_j}}{\sum_{j=k}^K\sqrt{C_j\Delta_j}}
    =\frac{p}{S}\sum_{j=1}^k\sqrt{C_j\Delta_j}.
\end{align*}
%
Thus, inequality \eqref{eq:Tk_bound} holds for all $k$. 

In particular, when $k=K$, we obtain
\begin{equation}\label{eq:MFMC_iterative_total_cost}
T_K = \sum_{j=1}^K C_j\left\lfloor M_j^*\right\rfloor \le p,
\end{equation}
confirming that the iterative scheme never exceeds the prescribed computational budget. To establish the lower bound in \eqref{eq:Iterative_integer_sample_size_cost_bound}, we compare the auxiliary sequences $M_k^*$ and $N_k^*$. From \eqref{eq:Tk_bound},
%
\[
M_k^* = \sqrt{\frac{\Delta_k}{C_k}}\frac{p - T_{k-1}}{\sum_{j=k}^K\sqrt{C_j\Delta_j}} \ge \sqrt{\frac{\Delta_k}{C_k}}\frac{p-\frac{p}{S}\sum_{j=1}^{k-1}\sqrt{C_j\Delta_j}}{\sum_{j=k}^K\sqrt{C_j\Delta_j}} = \sqrt{\frac{\Delta_k}{C_k}}\frac{p}{S}=N_k^*, \qquad k \ge 1.
\]
% 
Monotonicity of the floor function implies \(\lfloor M_k^*\rfloor\ge\lfloor N_k^*\rfloor\) for every \(k\). Multiplying by \(C_k\) and summing yields the desired lower bound in 
\eqref{eq:Iterative_integer_sample_size_cost_bound}.  
Combining this with \eqref{eq:MFMC_iterative_total_cost} completes the proof of the cost bound.


\medskip
\noindent
\textit{ Equality conditions.}
The upper bound in \eqref{eq:Iterative_integer_sample_size_cost_bound} is attained if and only if no budget remains unused, i.e.,
\[
\sum_{k=1}^K C_k \left(M_k^* - \left\lfloor M_k^*\right\rfloor\right) = 0.
\]
Because each fractional part satisfies $M_k^* - \lfloor M_k^* \rfloor \in [0,1)$, this condition holds precisely when every $M_k^*$ is an integer. Similarly, the lower bound is attained if and only if \(\lfloor M_k^* \rfloor = \lfloor N_k^* \rfloor\) for all \(k\), equivalently when both real-valued allocations \(M_k^*\) and \(N_k^*\) lie in the same integer interval
%
\[
\left\lfloor N_k^*\right\rfloor\le M_k^* < \left\lfloor N_k^*\right\rfloor + 1.
\]
Since \(M_k^* \ge N_k^*\), this occurs precisely when they share the same integer part for every \(k\).

\end{proof}


Theorem \ref{thm:MFMC_New_IntegerValued_Variance} establishes that the iterative integer-valued allocation achieves superior variance performance compared to direct flooring while maintaining proximity to the continuous optimum, thus providing an improved variance--cost tradeoff under fixed budget constraints.
%
\begin{theorem}[Normalized variance bound for the iterative integer-valued sample allocation]
\label{thm:MFMC_New_IntegerValued_Variance}
Let $\lfloor M_k^* \rfloor$ denote the integer-valued sample sizes obtained from the iterative allocation scheme \eqref{eq:MFMC_New_IntegerValued_Sample_Size}, and let $\lfloor N_k^* \rfloor$ denote those obtained by directly flooring the real-valued optimal allocation \eqref{eq:MFMC_RealValued_Sample_Size}. Under the computational budget constraint \eqref{eq:p_bound} and the standard variance conditions on $\Delta_k$ from Theorem~\ref{thm:Sample_size_est}, the normalized variance satisfies the following bounds
%
\begin{equation}\label{eq:Iterative_Integer_Variance_Bound}
\frac{1}{p}\left(\sum_{k=1}^K \sqrt{C_k \Delta_k}\right)^2
= \sum_{k=1}^K \frac{\Delta_k}{N_k^*}
\;\le\;
\sum_{k=1}^K \frac{\Delta_k}{\left\lfloor M_k^* \right\rfloor}
\;\le\;
\sum_{k=1}^K \frac{\Delta_k}{\left\lfloor N_k^* \right\rfloor}.
\end{equation}
%
\end{theorem}
%




\begin{proof}
We first prove the upper bound in \eqref{eq:Iterative_Integer_Variance_Bound}.  
From Theorem~\ref{thm:MFMC_New_IntegerValued_Cost}, it follows that $\lfloor M_k^* \rfloor \ge \lfloor N_k^* \rfloor$ for all $k$.  
Since $x \mapsto \Delta_k/x$ is strictly decreasing for $x > 0$, it immediately follows that
\[
\sum_{k=1}^K \frac{\Delta_k}{\left\lfloor M_k^* \right\rfloor} 
\le \sum_{k=1}^K \frac{\Delta_k}{\left\lfloor N_k^* \right\rfloor}.
\]

\medskip
\noindent
To establish the lower bound, we apply the Cauchy--Schwarz inequality:
%
\[
\left(\sum_{k=1}^K \sqrt{C_k \Delta_k}\right)^2
\le
\left(\sum_{k=1}^K C_k \left\lfloor M_k^* \right\rfloor\right)
\left(\sum_{k=1}^K \frac{\Delta_k}{\left\lfloor M_k^* \right\rfloor}\right).
\]
%
From the cost bound \eqref{eq:MFMC_iterative_total_cost}, it follows that 
$\sum_{k=1}^K C_k \lfloor M_k^* \rfloor \le p$, and hence
%
\[
\frac{1}{p}\left(\sum_{k=1}^K \sqrt{C_k \Delta_k}\right)^2
\le
\sum_{k=1}^K \frac{\Delta_k}{\left\lfloor M_k^* \right\rfloor}.
\]
%
which proves the lower bound.

\medskip
\noindent
\textit{Equality conditions.} 
Equality in the Cauchy--Schwarz step occurs if and only if there exists a constant $\lambda>0$ such that
\[
\left\lfloor M_k^* \right\rfloor = \lambda \sqrt{\frac{\Delta_k}{C_k}}, \qquad k=1,\ldots,K,
\]
which corresponds to the continuous optimal allocation $M_k^* = N_k^*$. Therefore, equality in \eqref{eq:Iterative_Integer_Variance_Bound} holds if and only if the iterative scheme reproduces the continuous solution exactly, i.e., when all $\lfloor M_k^* \rfloor = N_k^*$ and the total cost equals $p$.
\end{proof}



% ------------------
% Next consider the variance
% \begin{align*}
%     f_{\text{act}}(\overline{N_k})&=\sum_{i=1}^{K}\frac{\Delta_k}{\overline{N_k}}=\sum_{i=1}^{k-1}\frac{\Delta_i}{\overline{N_i}}+\frac{\Delta_k}{\overline{N_k}}+\sum_{i=k+1}^{K}\frac{\Delta_i}{\overline{N_i}}\\
%     &\in \left[\sum_{i=1}^{k-1}\frac{\Delta_i}{\overline{N_i}}+\frac{\Delta_k}{\overline{N_k}}+\sum_{i=k+1}^K\frac{\Delta_{i}}{N_i^*},\; \sum_{i=1}^{k-1}\frac{\Delta_i}{\overline{N_i}}+\frac{\Delta_k}{\overline{N_k}}+\sum_{i=k+1}^K\frac{\Delta_{i}}{N_i^*-1}\right)=[f_1,f_2)\\
%     f_1&=\sum_{i=1}^{k-1}\frac{\Delta_i}{\overline{N_i}}+\frac{\Delta_k}{\overline{N_k}}+\sum_{i=k+1}^K\frac{\Delta_{i}}{N_i^*}=\sum_{i=1}^{k-1}\frac{\Delta_i}{\overline{N_i}}+\frac{\Delta_k}{\overline{N_k}}+\sum_{i=k+1}^K\sqrt{C_i\Delta_i}\frac{\sum_{j=i}^K\sqrt{C_j\Delta_j}}{p-\sum_{j=1}^{i-1}C_j\overline{N_j}}\\
%     f_2&=\sum_{i=1}^{k-1}\frac{\Delta_i}{\overline{N_i}}+\frac{\Delta_k}{\overline{N_k}}+\sum_{i=k+1}^K \ldots\\
% \end{align*}
% \begin{align*}
%     \frac{d f_1}{d \overline{N_k}}&=-\frac{\Delta_k}{\overline{N_k}^2}+C_k\sum_{i=k+1}^K\sqrt{C_i\Delta_i}\frac{\sum_{j=i}^K\sqrt{C_j\Delta_j}}{\left(p-\sum_{j=1}^{i-1}C_j\overline{N_j}\right)^2}\\
%     \frac{d^2 f_1}{d^2 \overline{N_k}}&=\frac{2\Delta_k}{\overline{N_k}^3}+2C_k^2\sum_{i=k+1}^K\sqrt{C_i\Delta_i}\frac{\sum_{j=i}^K\sqrt{C_j\Delta_j}}{\left(p-\sum_{j=1}^{i-1}C_j\overline{N_j}\right)^3}
% \end{align*}
% Note that $\frac{d^2 f_1}{d^2 \overline{N_k}}>0$ whenever $p>\sum_{j=1}^{i-1}C_j\overline{N_j}$. this means $f_1$ is convex in $\overline{N_k}$.