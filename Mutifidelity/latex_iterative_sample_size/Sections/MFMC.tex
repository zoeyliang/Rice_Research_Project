%!TEX root = ../main.tex
% ====================================================
\section{Multi-fidelity Monte Carlo}\label{sec:MFMC}
% ====================================================
We now briefly review the core principles underlying multi-fidelity Monte Carlo, drawing on the framework in \cite{PeWiGu:2016}.  The MFMC framework combines a high-fidelity model $u_{h,1} = u_h$ with a series of low-fidelity models $u_{h,k}: \Omega \rightarrow U$ for $k=2,\ldots,K$.  The high-fidelity model yields accurate but computationally expensive evaluations, whereas the low-fidelity models offer cheaper approximations with reduced accuracy. MFMC optimally allocates computational resources across the fidelity levels to reduce the overall estimator variance, reducing the reliance on costly high-fidelity samples while preserving estimator accuracy and robustness.


For convenience, we may use $u$ and $u_{h,k}$ to refer to the exact $u(\cdot,\boldsymbol \omega)$ and discrete $u_{h,k}(\cdot,\boldsymbol \omega)$ solutions with random variable $\boldsymbol \omega$ in the later context. For each pair of $u_{h,k}$ and $u_{h,j}$, we define the variance and the Pearson correlation coefficient as
%
\begin{equation*}
    \sigma_k^2 = \mathbb{V}\left[u_{h,k}\right],\qquad \rho_{k,j} = \frac{\text{Cov}\left[ u_{h,k}, u_{h,j}\right]}{\sigma_k\sigma_j}, \quad k,j=1,\dots, K,
\end{equation*}
%
where the covariance is $\text{Cov}[u_{h,k}, u_{h,j}] := \mathbb{E}[\langle u_{h,k} - \mathbb{E}[u_{h,k}], u_{h,j} - \mathbb{E}[u_{h,j}]\rangle_U]$ and, by definition, $\rho_{k,k}=1$. The multi-fidelity Monte Carlo finite element estimator $A^{\text{MF}}$ augments a high-fidelity Monte Carlo estimate with control variate corrections from low-fidelity models
%
\begin{equation}\label{eq:MFMC_estimator}
    A^{\text{MF}} := A^{\text{MC}}_{1,N_1} + \sum_{k=2}^K \alpha_k\left(\overline{A}_{k,N_k} - \overline{A}_{k,N_{k-1}} \right),
\end{equation}
%
where $A^{\text{MC}}_{1,N_1}$ is the standard Monte Carlo estimator using $N_1$ samples of the high-fidelity model, $\alpha_k\in \mathbb{R}$ are control variate weights, and $\overline{A}_{k,N_k}$ denotes the sample average of model $u_{h,k}$ over $N_k$ samples. The control variate construction requires the nesting condition $N_{k-1}\le N_k$ for $k=2,\ldots, K$, as $\overline{A}_{k,N_{k}}$ reuses all $N_{k-1}$ samples from $\overline{A}_{k,N_{k-1}}$, possibly supplemented by additional $N_{k} - N_{k-1}$ samples. This reuse introduces statistical dependence between $\overline{A}_{k,N_{k-1}}$ and $\overline{A}_{k,N_{k}}$. To eliminate sampling dependence in the correction terms, we partition the $N_k$ samples into two disjoint sets of sizes  $N_{k-1}$ and $N_k - N_{k-1}$. This allows us to reformulate the multi-fidelity estimator \eqref{eq:MFMC_estimator} as
%
\begin{equation}\label{eq:MFMC_estimator_independent}
    A^{\text{MF}} = A^{\text{MC}}_{1,N_1} +  \sum_{k=2}^K \alpha_k\left(1-\frac{N_{k-1}}{N_{k}}\right)\left(A_{k,N_k\backslash N_{k-1}}^{\text{MC}}-A_{k,N_{k-1}}^{\text{MC}}\right),
\end{equation}
%
where $A^{\text{MC}}_{k,N_k \backslash N_{k-1}}$ denotes the Monte Carlo average over the $N_k - N_{k-1}$ samples not included in $A^{\text{MC}}_{k,N_{k-1}}$, and is defined to be zero whenever $N_k=N_{k-1}$. In this formulation, the two terms in each correction are evaluated on independent sample sets, which simplifies variance analysis. We can now express the MFMC estimator in compact form
%
\begin{equation*}\label{eq:MFMC_estimator_Correction}
A^{\text{MF}} = Y_1 + \sum_{k=2}^K \alpha_k Y_k,
\end{equation*}
%
where the correction terms $Y_k$ are defined as
%
\begin{equation} \label{eq:MFMC_Yk}
Y_1 :=A^{\text{MC}}_{1,N_1},\qquad Y_k:=\overline{A}_{k,N_k} - \overline{A}_{k,N_{k-1}}=\left(1-\frac{N_{k-1}}{N_{k}}\right)\left(A_{k,N_k\backslash N_{k-1}}^{\text{MC}}-A_{k,N_{k-1}}^{\text{MC}}\right), k=2\ldots, K.
\end{equation}
%
Since $Y_k$ is defined as the difference between two independent Monte Carlo estimators of the same model, it is unbiased: $\mathbb{E}[Y_k] = 0$ for $k\ge 2$. Consequently, the MFMC estimator is itself unbiased, satisfying $\mathbb{E}[A^{\text{MF}}] = \mathbb{E}[u_{h,1}]$. The variances of the correction terms $Y_k$ are
%
\begin{equation}\label{eq:Var_Yk}
    \mathbb{V}\left[Y_1\right] = \frac{\sigma_1^2}{N_1}, \quad \mathbb{V}\left[Y_k\right] = \left(1-\frac{N_{k-1}}{N_{k}}\right)^2\left(\frac{\sigma_k^2}{N_{k-1}}+\frac{\sigma_k^2}{N_k-N_{k-1}}\right) = \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\sigma_k^2.
\end{equation}
%
Although $Y_k$ and $Y_j$ for $2\le k<j \le K$ share overlapping sample sets and are therefore statistically dependent, they remain uncorrelated, as established in Lemma~\ref{lemma:Y_k_Y_j} in the appendix. However, each correction term $Y_k$ with $k\ge 2$ is correlated with the high-fidelity estimator $Y_1$. Using the covariance identity derived from \cite[Lemma~3.2]{PeWiGu:2016}, yields
%
\begin{equation}\label{eq:Cov_Yk}
% \text{Cov}(Y_k,Y_j) =0,\quad \text{for } \;2\le k<j \le K,\qquad 
\text{Cov}[Y_1,Y_k] = - \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\rho_{1,k}\sigma_1\sigma_k, \quad \text{for } \; k\ge 2.
\end{equation}
%
Combining \eqref{eq:Var_Yk} and \eqref{eq:Cov_Yk}, the total variance of the MFMC estimator is 
%
\begin{align}
    \nonumber
    \mathbb{V}\left[A^{\text{MF}}\right] &= \mathbb{V}\left[Y_1\right] + \mathbb{V}\left[\sum_{k=2}^K \alpha_kY_k\right]+2\;\text{Cov}\left[Y_1,\sum_{k=2}^K \alpha_k Y_k \right],\\
    \nonumber
    &=\mathbb{V}\left[Y_1\right] + \sum_{k=2}^K \alpha_k^2 \mathbb{V}\left[Y_k\right]+2\sum_{2\le k<j\le K} \alpha_k\alpha_j\; \text{Cov}[Y_k,Y_j] +2\sum_{k=2}^K \alpha_k\;\text{Cov}\left[Y_1, Y_k\right],\\
    % \nonumber
    % &=\mathbb{V}\left(Y_1\right) + \sum_{k=2}^K \alpha_k^2 \mathbb{V}\left(Y_k\right) +2\sum_{k=2}^K \alpha_k\;\text{Cov}\left(Y_1, Y_k\right),\\
    \label{eq:MFMC_variance}
    &=\frac{\sigma_1^2}{N_1} + \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\left(\alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k\right).
\end{align}
%
`To determine the optimal sample sizes $N_k$ and control variate weights $\alpha_k$ in the MFMC estimator \eqref{eq:MFMC_estimator_independent}, we express the total computational cost for the MFMC estimator
%
\[
\mathcal{W}^{\text{MF}} = \sum_{k=1}^K C_kN_k,
\]
%
where $C_k$ is the cost of generating a single sample of model $u_{h,k}$, and $N_k$ is the corresponding sample count. Unlike previous formulations \cite{PeWiGu:2016} that derive sample sizes based on a fixed computational budget, our approach directly expresses the sample sizes and computational resources in terms of the desired accuracy $\epsilon$. This formulation offers greater flexibility in applications where accuracy targets are more relevant than rigid cost constraints. We formulate an optimization problem to determine the optimal sample sizes $N_k$ and weights $\alpha_k$ by minimizing the total sampling cost $\mathcal{W}^{\text{MF}}$, subject to three constraints. Second,  the monotonicity constraints $N_{k-1}\le N_k$ for $k=2,\ldots, K$ ensures consistent sample reuse across fidelity levels. Third, all sample sizes must be non-negative. This leads to the following constrained optimization problem
%
\begin{equation}\label{eq:Optimization_pb_sample_size}
    \begin{array}{ll}
    \min &\mathbb{V}\left[A^{\text{MF}}\right],\\
       \text{subject to} &\displaystyle\sum\limits_{k=1}^K C_kN_k=p,\\[2pt]
       &\displaystyle -N_1\le 0,\quad \displaystyle N_{k-1}-N_k\le 0, \;\; k=2\ldots,K,\\
       &N_1,\ldots, N_K\in \mathbb{R},\\
       &\alpha_2,\ldots,\alpha_K\in \mathbb{R}.
    \end{array}
\end{equation}
%


The real-valued solution to \eqref{eq:Optimization_pb_sample_size} is presented in Theorem~\ref{thm:Sample_size_est}.


%
\begin{theorem}[Optimal MFMC real-valued sample size allocation]
\label{thm:Sample_size_est}
Consider an ensemble of $K$ models $\{u_{h,k}\}_{k=1}^K$ each characterized by the standard deviation $\sigma_k$ of its output, the correlation coefficient $\rho_{1,k}$ with the highest-fidelity model $u_{h,1}$, and the computational cost per sample evaluation $C_k$. Define $\Delta_k = \rho_{1,k}^2 - \rho_{1,k+1}^2$ for $k = 1, \dots, K$, with the boundary convention $\rho_{1,K+1} = 0$. Assume the following conditions hold
%
\begin{alignat*}{3}
&(i)\;\; \textit{Correlation monotonicity}: \quad && |\rho_{1,1}| > \cdots > |\rho_{1,K}|, \\ 
&(ii)\;\; \textit{Cost-correlation ratio}: \quad && \frac{\Delta_{k}}{C_k} > \frac{\Delta_{k-1}}{C_{k-1}}, \quad k=2,\ldots,K. 
\end{alignat*}
%
Under these assumptions, the solution to the optimization problem \eqref{eq:Optimization_pb_sample_size} yields optimal weights $\alpha_k^*$
%
\begin{align}
    % \label{eq:MFMC_coefficients}
    % &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\\
    \label{eq:MFMC_SampleSize}
    &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},
\end{align}
%
define an intermediate vector $\boldsymbol{r}^* = [r_1,\ldots,r_k]^T$, then $\boldsymbol{r}^*$ and the sample sizes $N_k^*$
%
\[
r_k^* = \sqrt{\frac{C_1\Delta_k}{C_k\Delta_1}},\quad N_1^* = \frac{p}{\sum_{k=1}^K C_k r^*_k}, \quad N_k^*=N_1^*r_k^*.
\] 
%
\JLcolor{alternatively, in my way to represent it without mentioning the vector $\boldsymbol{r}^*$, we have}
%
\begin{equation}
\label{eq:MFMC_RealValued_Sample_Size}
\JLcolor{N_k^* = \sqrt{\frac{\Delta_k}{C_k}}\frac{p}{\sum_{j=1}^K \sqrt{C_j\Delta_j}},} 
\end{equation}
%
The resulting MFMC estimator achieves a variance of
%
\begin{equation}
\label{eq:MFMC_variance_optimal}
\mathbb{V}\left[A^{\text{MF}}\right] =
% \frac{\sigma_1^2}{N_1^*} - \sum_{k=2}^K \left(\frac{1}{N_{k-1}^*} - \frac{1}{N_k^*}\right)\rho_{1,k}^2\sigma_1^2=
\frac{\sigma_1^2}{p}\left(\sum_{k=1}^K\sqrt{C_k\Delta_{k}}\right)^2.
\end{equation}
%
\end{theorem}

The efficiency of the multi-fidelity Monte Carlo estimator relative to the standard Monte Carlo estimator is quantified by the ratio
%
\begin{equation}\label{eq:MFMC_sampling_cost_efficiency}
    \xi = \frac{\mathcal{W}^\text{MF}}{\mathcal{W}^\text{MC}} = \frac{1}{C_1} \left(\sum_{k=1}^K\sqrt{C_k\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)}\right)^2,
\end{equation}
%
A smaller value of $\xi$ corresponds to greater computational savings, indicating a more effective MFMC estimator.



In practice, the optimal $N_k^*$ must be rounded to integers. A common approach is to apply the floor function \cite{PeWiGu:2016}, which ensures feasibility with respect to the budget.  We use $N_k^*$ as the real-valued solution, and $\lfloor N_k^* \rfloor$ as the floored integer-valued solution. The total cost under rounding satisfies
%
\begin{equation}\label{eq:sampling_cost_bound}
    \sum_{k=1}^K C_k N_k^* - \sum_{k=1}^K C_k < \sum_{k=1}^K C_k \left\lfloor N_k^*\right\rfloor \le \sum_{k=1}^K C_k N_k^*,
\end{equation}
%
where the additional term $\sum_{k=1}^K C_k$ accounts for the fact that $N_k^*-1 < \lfloor N_k^*\rfloor \le N_k^*$. This naturally leads to the consideration of whether the additive overhead introduced by rounding distorts the asymptotic behavior of the total sampling cost, particularly in regimes where some $N_k^*$ fall below one. Define $f(N_k) = \mathbb{V}[A^\text{MF}]/\sigma_1^2$. Then from \eqref{eq:MFMC_variance_optimal} we have






\[
f(N_k^*) = \frac{1}{p}\left(\sum_{k=1}^K\sqrt{C_k\Delta_{k}}\right)^2.
\]
%
Using the floor function, the actual value for $f$ and $p$ using the floored sample size are
\[
f_{\text{act}}(\lfloor N_k^* \rfloor) = \sum_{k=1}^K\frac{\Delta_{k}}{\lfloor N_k^* \rfloor}\ge f(N_k^*), \quad p_{\text{act}} = \sum_{k=1}^K C_k\lfloor N_k^* \rfloor\le p
\]
Moreover, we know that $N_k^*-1<\lfloor N_k^* \rfloor\le N_k^*$, therefore
\begin{align*}
    f_{\text{act}}(\lfloor N_k^* \rfloor)&\in \left[\sum_{k=1}^K\frac{\Delta_{k}}{N_k^*},\; \sum_{k=1}^K\frac{\Delta_{k}}{N_k^*-1}\right) = \left[\frac{\left(\sum_{k=1}^K \sqrt{C_k\Delta_k}\right)^2}{p}, \sum_{k=1}^K\frac{\Delta_{k}}{\sqrt{\frac{\Delta_k}{C_k}}\frac{p}{\sum_{j=1}^K \sqrt{C_j\Delta_j}}-1}\right)\\
    &=\left[\frac{\left(\sum_{k=1}^K \sqrt{C_k\Delta_k}\right)^2}{p}, \sum_{k=1}^K \sqrt{C_k\Delta_k}\sum_{k=1}^K\frac{\sqrt{C_k\Delta_{k}}}{p-\sqrt{\frac{C_k}{\Delta_k}}\sum_{j=1}^K \sqrt{C_j\Delta_j}}\right)\\
    &=\sum_{k=1}^K \sqrt{C_k\Delta_k}\left[\frac{\sum_{k=1}^K \sqrt{C_k\Delta_k}}{p},\sum_{k=1}^K\frac{\sqrt{C_k\Delta_{k}}}{p-\sqrt{\frac{C_k}{\Delta_k}}\sum_{j=1}^K \sqrt{C_j\Delta_j}}\right)\\
    p_{\text{act}} &\in \left(\sum_{k=1}^KC_kN_k^*-\sum_{k=1}^K C_k, \sum_{k=1}^KC_kN_k^*\right]=\left( p-\sum_{k=1}^K C_k,p\right].
\end{align*}




