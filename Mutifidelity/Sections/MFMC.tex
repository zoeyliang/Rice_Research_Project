%!TEX root = ../main.tex
% ====================================================
\section{Multi-fidelity Monte Carlo}\label{sec:MFMC}
% ====================================================
This section reviews the multi-fidelity Monte Carlo (MFMC) method, following the foundational formulation in \cite{PeWiGu:2016}. The MFMC framework uses an ensemble of models with varying computational cost and accuracy to construct a variance-reduced estimator for high-fidelity expectation. Let $u_1:\Omega \to U$ denote the high-fidelity (HF) model that provides accurate but expensive evaluations, and let $\{u_k\}_{k=2}^K$ denote low-fidelity (LF) models that offer cheaper approximations. The central goal of MFMC is to allocate a fixed computational budget across these models to minimize estimator variance while maintaining unbiasedness.

We introduce some key statistical quantities that describe the model. We represent the random output of model $u_k$ on the probability space $(\Omega,\mathcal{F},\mathbb{P})$ by $u_k(\boldsymbol{\omega})$, abbreviated as $u_k$. For each pair of models $u_k,u_j$, define the variance and correlation coefficient
%
\begin{equation*}
    \sigma_k^2 = \mathbb{V}\!\left[u_k\right],\qquad 
    \rho_{k,j} = \frac{\text{Cov}\!\left[u_k,u_j\right]}{\sigma_k\sigma_j}, 
    \quad k,j=1,\dots,K,
\end{equation*}
%
where the covariance is defined as $\text{Cov}[u_k,u_j] := \mathbb{E}[\langle u_k - \mathbb{E}[u_k], u_j - \mathbb{E}[u_j]\rangle_U]$ and $\rho_{k,k}=1$. The pairwise correlations between fidelity levels quantify the statistical dependence that drives variance reduction through effective control variates.

The MFMC estimator adopts a nested sampling strategy, reusing samples from lower-fidelity models to enhance correlation with the high-fidelity model and reduce overall variance. Let $A_{1,N_1}^{\text{MC}}$ denote the standard Monte Carlo estimator of $\mathbb{E}[u_1]$ based on $N_1$ HF samples. The MFMC estimator augments this baseline with corrections from lower-fidelity models through a hierarchy of control variates
%
\begin{equation}\label{eq:MFMC_estimator}
A^{\text{MF}} := A^{\text{MC}}_{1,N_1} + \sum_{k=2}^K \alpha_k\left(\overline{A}_{k,N_k} - \overline{A}_{k,N_{k-1}}\right),
\end{equation}
%
where $\alpha_k \in \mathbb{R}$ are control-variate weights, and $\overline{A}_{k,N}$ denotes the sample average of $N$ evaluations of model $u_k$. The nested structure ensures that $\overline{A}_{k,N_k}$ reuses all $N_{k-1}$ samples from $\overline{A}_{k,N_{k-1}}$, supplemented by additional $N_k - N_{k-1}$ samples. This reuse significantly enhances computational efficiency but introduces statistical dependencies among terms, complicating the variance analysis of the estimator.

To enable a tractable analytical treatment, we reformulate the estimator so that its constituent terms are statistically independent. Partitioning the $N_k$ samples of $u_k$ into disjoint subsets of sizes $N_{k-1}$ and $N_k - N_{k-1}$ yields an equivalent independent representation 
%
\begin{equation}\label{eq:MFMC_estimator_independent}
    A^{\text{MF}} = A^{\text{MC}}_{1,N_1} +  \sum_{k=2}^K \alpha_k\!\left(1-\frac{N_{k-1}}{N_k}\right)\left(A^{\text{MC}}_{k,N_k\backslash N_{k-1}}-A^{\text{MC}}_{k,N_{k-1}}\right),
\end{equation}
%
where $A^{\text{MC}}_{k,N_k \backslash N_{k-1}}$ denotes the Monte Carlo average over the $N_k - N_{k-1}$ newly drawn samples, defined to be zero when $N_k = N_{k-1}$. 


The statistical properties of the MFMC estimator emerge clearly from its component-wise decomposition. Define
%
\begin{equation}\label{eq:MFMC_Yk}
Y_1 := A^{\text{MC}}_{1,N_1},\quad 
Y_k := \left(1-\frac{N_{k-1}}{N_k}\right)\!\left(A^{\text{MC}}_{k,N_k\backslash N_{k-1}} - A^{\text{MC}}_{k,N_{k-1}}\right), \;\; k=2\ldots, K,
\end{equation}
%
then the MFMC estimator can be expressed into a compact form $A^{\text{MF}} = Y_1 + \sum_{k=2}^K \alpha_k Y_k$. Since each $Y_k$ for $k\ge2$ represents a difference of two independent estimators for the same $\mathbb{E}[u_k]$, we immediately obtain $\mathbb{E}[Y_k]=0$ and the MFMC estimator is unbiased: $\mathbb{E}[A^{\text{MF}}]=\mathbb{E}[u_1]$. The variances of the components are
%
\begin{equation}\label{eq:Var_Yk}
    \mathbb{V}[Y_1] = \frac{\sigma_1^2}{N_1}, \qquad 
    \mathbb{V}[Y_k] = \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\sigma_k^2, \;\; k=2\ldots, K.
\end{equation}
%
A key statistical insight, formalized in Lemma~\ref{lemma:Y_k_Y_j}, establishes that the correction terms between the LF models are mutually uncorrelated despite sample reuse.
%
\begin{lemma}\label{lemma:Y_k_Y_j}
For $2\le k<j\le K$, 
% the correction terms $Y_k$ and $Y_j$ defined in \eqref{eq:MFMC_Yk} are uncorrelated, i.e., 
$\operatorname{Cov} [Y_k,Y_j ]=0$.
\end{lemma}
%
The proof is provided in the Appendix.

However, each correction $Y_k$($k\ge2$) of LF is correlated with $Y_1$, with covariance
\begin{equation}\label{eq:Cov_Yk}
\operatorname{Cov}[Y_1,Y_k] = -\!\left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\rho_{1,k}\sigma_1\sigma_k,
\end{equation}
as shown in \cite[Lemma~3.2]{PeWiGu:2016}. Combining \eqref{eq:Var_Yk} and \eqref{eq:Cov_Yk} gives
%
\begin{equation}\label{eq:MFMC_variance}
    \mathcal{V}^{\text{MF}}
    =\frac{\sigma_1^2}{N_1} 
    + \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\!\left(\alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k\right).
\end{equation}
%

In order to determine optimal sample sizes $N_k$ and weights $\alpha_k$ in the MFMC estimator \eqref{eq:MFMC_estimator_independent}, an optimization problem is formulated \cite{PeWiGu:2016} by minimizing the estimator variance \eqref{eq:MFMC_variance} subject to a fixed budget $p$. Let $C_k$ denote the per-sample cost of model $u_k$, the total computational cost is 
%
\[
\mathcal{W}^{\text{MF}} = \sum_{k=1}^K C_k N_k,
\]
%
and the constrained optimization problem becomes
%
\begin{equation}\label{eq:Optimization_pb_sample_size}
    \begin{array}{ll}
    \min &\mathcal{V}^{\text{MF}}\left(\alpha_k,N_k\right),\\
       \text{subject to} &\displaystyle\sum\limits_{k=1}^K C_kN_k=p,\\[2pt]
       &\displaystyle N_1\ge 0,\quad \displaystyle N_{k-1}\le N_k, \;\; k=2\ldots,K,\\
       &N_1,\ldots, N_K\in \mathbb{R},\\
       &\alpha_2,\ldots,\alpha_K\in \mathbb{R}.
    \end{array}
\end{equation}
%
Note that for each level $k\ge 2$, $\alpha_k$ enters only through a quadratic expression independent of $N_k$ in the variance term. This separable structure allows a fundamental simplification of the variance functional, which allows hierarchical minimization
%
\begin{equation*}
    \min_{\alpha_k,\, N_k} \mathcal{V}^{\text{MF}}\left(\alpha_k, N_k\right)
    = \min_{N_k}\Big(\min_{\alpha_k} \mathcal{V}^{\text{MF}}(\alpha_k, N_k)\Big).
\end{equation*}
%
The hierarchical minimization admits a closed-form solution for optimal weights by solving the inner optimization $\partial \mathcal{V}^{\text{MF}}/\partial \alpha_k = 0$, yielding 
%
\begin{equation}\label{eq:MFMC_weights}
    \alpha_k^* = \frac{\rho_{1,k}\sigma_1}{\sigma_k}.
\end{equation}
%
Substituting $\alpha_k^*$ into \eqref{eq:MFMC_variance} simplifies the variance to 
%
\begin{equation*}
    \mathcal{V}^{\text{MF}}\left(\alpha_k^*, N_k\right)
    = \sigma_1^2\sum_{k=1}^K \frac{\Delta_k}{N_k},
\end{equation*}
%
where $\Delta_k = \rho_{1,k}^2 - \rho_{1,k+1}^2$ for $k = 1, \dots, K$ with $\rho_{1,K+1}=0$. This reduces the joint optimization to a continuous resource allocation problem involving only sample allocation
%
\begin{equation}\label{eq:Optimization_pb_sample_size_reduced}
    \begin{array}{ll}
    \min &\displaystyle f(N_k) =\sum_{k=1}^K \frac{\Delta_k}{N_k},\\
       \text{subject to} &\displaystyle\sum\limits_{k=1}^K C_kN_k=p,\\[2pt]
       &\displaystyle -N_1\le 0,\quad \displaystyle N_{k-1}-N_k\le 0, \;\; k=2\ldots,K,\\
       &N_1,\ldots, N_K\in \mathbb{R},
    \end{array}
\end{equation}
%
where $f(N_k)$ is the {\it normalized variance functional}. Under suitable monotonicity and ordering assumptions, this problem admits an analytic solution that characterizes the optimal allocation of resources across fidelity levels. 


%
\begin{theorem}[Optimal MFMC real-valued sample allocation]
\label{thm:Sample_size_est}
Consider $K$ models $\{u_{k}\}_{k=1}^K$ with standard deviations $\sigma_k$, correlation coefficients $\rho_{1,k}$ of LF model $u_k$ with the HF model $u_1$, and per-sample costs $C_k$. Define $\Delta_k = \rho_{1,k}^2 - \rho_{1,k+1}^2$ for $k = 1, \dots, K$ with $\rho_{1,K+1}=0$. Assume the following conditions hold
%
\begin{alignat*}{3}
&(i)\;\textit{Monotone correlations:} &\quad& |\rho_{1,1}| > \cdots > |\rho_{1,K}|,\\
&(ii)\;\textit{Cost-correlation ratio:} &\quad& \frac{\Delta_{k}}{C_k} > \frac{\Delta_{k-1}}{C_{k-1}}, \quad k=2,\ldots,K.
\end{alignat*}
%
Then the optimal control weights and sample sizes for \eqref{eq:Optimization_pb_sample_size} are
%
\begin{equation}\label{eq:MFMC_RealValued_Sample_Size}
    \alpha_k^* = \frac{\rho_{1,k}\sigma_1}{\sigma_k}, \qquad
    N_k^* = \sqrt{\frac{\Delta_k}{C_k}}\,
    \frac{p}{\sum_{j=1}^K \sqrt{C_j \Delta_j}}.
\end{equation}
%
% \[
% r_k^* = \sqrt{\frac{C_1\Delta_k}{C_k\Delta_1}},\quad N_1^* = \frac{p}{\sum_{k=1}^K C_k r^*_k}, \quad N_k^*=N_1^*r_k^*.
% \] 
% %
% \JLcolor{alternatively, in my way to represent it without mentioning the vector $\boldsymbol{r}^*$, we have}
%
The resulting minimal variance of the MFMC estimator is
\begin{equation}\label{eq:MFMC_variance_optimal}
\mathcal{V}^{\text{MF}}
= \sigma_1^2\sum_{k=1}^K \frac{\Delta_k}{N_k^*}=\frac{\sigma_1^2}{p}\!\left(\sum_{k=1}^K \sqrt{C_k \Delta_k}\right)^{\!2}.
\end{equation}
\end{theorem}
%
This theorem provides an explicit expression for the optimal tradeoff between statistical efficiency and computational cost in MFMC estimation. The monotone correlation assumption ensures that each successive model provides diminishing but positive variance reduction, while the ordering of the costâ€“correlation ratios guarantees that adding lower-fidelity models yields net benefit within the total budget. 


In practical applications, Two stages of MFMC: first is model selection, the second uses the selected models to compute MFMC estimator. the quantities $\{C_k\}$ and $\{\rho_{1,k}\}$ are estimated from data or pilot simulations. Based on these estimates, one performs model selection by identifying the subset of fidelity levels that satisfy the monotonicity and ordering assumptions while minimizing the estimator variance. This subset defines the active models used in the MFMC estimator in the first stage, ensuring that the allocation $\{N_k^*\}$ derived from \eqref{eq:MFMC_RealValued_Sample_Size} attains the minimum variance \eqref{eq:MFMC_variance_optimal} under the prescribed computational budget in the second stage.

\subsection{Model selection}
We need to select the models from the available set such that the parameteres associated with the selected models satisfy the two conditions in Theorem \ref{thm:Sample_size_est}, as well as the $\mathcal{V}^{\text{MF}}$ as small as possible (Note that minimize \eqref{eq:MFMC_variance_optimal} is a selection of model that associated with different cost and $\Delta_k$, and is independent of budget $p$). Let $\mathcal{S}^*=\{1, \ldots, K^*\}$ be the indices of $K^*$ available models. We seek a subset $\mathcal{S}=\{i_1,i_2, \ldots,i_{K}\}\subseteq \mathcal{S}^* (K\le K^*)$ of indices that minimizes the sampling cost of multifidelity Monte Carlo estimator. Note that $\mathcal{S}$ is non-empty and $i_1=1$ since the high fidelity model must be included. We will follow the exhaustive algorithm in \cite[Algorithm~1]{PeWiGu:2016} for $2^{K^*-1}$ subsets of $\mathcal{S}^*$.  This algorithm gives the indices of the selected model.

% \begin{equation*}\label{eq:Optimization_pb_model_selection}
%     \begin{array}{lll}
%     \displaystyle\min_{k\in \mathcal{S}^*} &\displaystyle \mathcal{V}^\text{MF},\\
%        \text{s.t.} &\displaystyle |\rho_{1,1}|>\ldots>|\rho_{1,K^*}|,\\
%        &\displaystyle \frac{\Delta_k}{C_k}>\frac{\Delta_{k-1}}{C_{k-1}}, \quad k=2,\ldots,K^*, \quad \rho_{1,K^*+1}=0,\\
%        % &\JLcolor{\displaystyle \frac{\sum_{k\in S_1}\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}}{\sum_{k\in S_1} C_k}\sum_{k\in S_1}\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2\ge \frac{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2\epsilon^2}{\sigma_1^2},}\\
%        % &\JLcolor{\displaystyle \log_s\left\{\frac{\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2\epsilon^2}\sum_{k=1}^K\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}\sum_{k=1}^K\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2\right\}\ge \gamma,}\\
%         % &\displaystyle \rho_{1,0}=\infty \;\text{ and } \;\rho_{1,K+1}=0. \\%[6pt]
%     \end{array}
% \end{equation*}


\normalem
\begin{algorithm}[!ht]
\label{algo:MFMC_Algo_model_selection}
\DontPrintSemicolon    
   \KwIn{$K^*$ models $u_k$ with $C_k$ and $\rho_{1,k}.$}\vspace{1ex}
    
    \KwOut{ Selected index set $\mathcal{S}$.}\vspace{1ex}
    \hrule \vspace{1ex}

   % Estimate $\rho_{1,k}$ and $C_k$ for each model $f_k$ using $N_0$ samples.
   
   
   Reorder $u_k$ by decreasing $|\rho_{1,k}|$ with their corresponding $C_k$. Rename the model as 1 to $K^*$, $\mathcal{S}=\{1,\ldots, K^*\}$. 
   
   Initialize $v_{\min}=C_1$, $\mathcal{S}=\{1\}$. Let $ \mathcal{\widehat S}$ be all $2^{K-1}$ ordered subsets of $\mathcal{S}^*$, each containing the high fidelity model with index $1$. 
   % Set $ \mathcal{\widehat S}_1=\mathcal{S}^*$.

    % $(2 \le j \le 2^{K-1})$
    \For{each subset $\mathcal{\widehat S}_j$\,}{

    {
    \If{ condition $(ii)$ from Theorem \ref{thm:Sample_size_est} is satisfied}{
    Compute $\Delta_k$ and $v = \left(\sum_{k=1}^K \sqrt{C_k \Delta_k}\right)^{\!2}$.
    
    \If{$v<v_{\min}$}{
    {
    Update $\mathcal{S} = \mathcal{\widehat S}_j$ and $v_{\min} = v$.
    }
    } 
    }
    }
    $j=j+1$.
    }
    Return  $\mathcal{S}$.
\caption{Multi-fidelity Model Selection}
\end{algorithm}
\ULforem


Differentiating the normalized variance and cost with respect to the sample sizes gives
%
\[
\frac{\partial f}{\partial N_k} = -\frac{\Delta_k}{N_k^2},
\qquad 
\frac{\partial \mathcal{W}^{\text{MF}}}{\partial N_k} = C_k.
\]
%
These relations quantify the varianceâ€“cost trade-off: increasing samples at any level reduces variance at the expense of computational resources. At the continuous optimum \eqref{eq:MFMC_RealValued_Sample_Size}, the marginal variance reduction per unit cost $\Delta_k/(C_k N_k^2)$ is identical across all active models, establishing a balanced resource allocation that characterizes the optimal allocation.

While Theorem~\ref{thm:Sample_size_est} provides real-valued optimal allocations $N_k^*$, practical implementation requires integer sample sizes. The standard approach \cite{PeWiGu:2016} applies the floor function $\lfloor N_k^* \rfloor$ to ensure budget feasibility. The realized variance and cost are
%
\[
f\left(\left\lfloor N_k^* \right\rfloor\right) = \sum_{k=1}^K\frac{\Delta_{k}}{\left\lfloor N_k^* \right\rfloor}, \qquad \mathcal{W}^{\text{MF}}\left(\left\lfloor N_k^* \right\rfloor\right) = \sum_{k=1}^K C_k\left\lfloor N_k^* \right\rfloor.
\]
%
Since $N_k^*-1 < \lfloor N_k^*\rfloor \le N_k^*$, the floor operation induces bounded sub-optimality, producing the bounds
%
\begin{equation}\label{eq:bounds_for_floor}
\begin{aligned}
    % f\left(\left\lfloor N_k^* \right\rfloor\right)&\in \left[\sum_{k=1}^K\frac{\Delta_{k}}{N_k^*},\; \sum_{k=1}^K\frac{\Delta_{k}}{N_k^*-1}\right) = \left[\frac{1}{p}\left(\sum_{k=1}^K \sqrt{C_k\Delta_k}\right)^2, \sum_{k=1}^K\frac{\Delta_{k}}{\frac{p}{\sum_{j=1}^K \sqrt{C_j\Delta_j}}\sqrt{\frac{\Delta_k}{C_k}}-1}\right)\\
    % &=\left[\frac{1}{p}\left(\sum_{k=1}^K \sqrt{C_k\Delta_k}\right)^2, \sum_{k=1}^K \sqrt{C_k\Delta_k}\sum_{k=1}^K\frac{\sqrt{C_k\Delta_{k}}}{p-\sqrt{\frac{C_k}{\Delta_k}}\sum_{j=1}^K \sqrt{C_j\Delta_j}}\right)\\
    % &=\sum_{k=1}^K \sqrt{C_k\Delta_k}\left[\frac{\sum_{k=1}^K \sqrt{C_k\Delta_k}}{p},\sum_{k=1}^K\frac{\sqrt{C_k\Delta_{k}}}{p-\sqrt{\frac{C_k}{\Delta_k}}\sum_{j=1}^K \sqrt{C_j\Delta_j}}\right)\\
    % \mathcal{W}^{\text{MF}}\left(\left\lfloor N_k^* \right\rfloor\right) &\in \left(\sum_{k=1}^KC_kN_k^*-\sum_{k=1}^K C_k, \sum_{k=1}^KC_kN_k^*\right]=\left( p-\sum_{k=1}^K C_k,p\right].
    f\left(\left\lfloor N_k^* \right\rfloor\right) \in \left[\frac{1}{p}\left(\sum_{k=1}^K \sqrt{C_k\Delta_k}\right)^2, \sum_{k=1}^K\frac{\Delta_{k}}{N_k^*-1}\right), \qquad
\mathcal{W}^{\text{MF}}\left(\left\lfloor N_k^* \right\rfloor\right)\in \left( p-\sum_{k=1}^K C_k, p\right].
\end{aligned}
\end{equation}
%
The term $\sum_{k=1}^K C_k$ represents the rounding-induced slack in the budget, which becomes negligible asymptotically as $p \to \infty$. However, in the pre-asymptotic regime -- where the total budget $p$ is moderate -- this  slack can lead to significant under-utilization of the computational resources. This observation naturally motivates \textit{the development of  alternative integer-valued allocation strategies that reduce slack and achieve tighter budget utilization.}





% This quantity is the marginal variance reduction rate â€” how much the total variance decreases when you spend more samples at level by taking one more sample cost $C_k$. So the marginal variance reduction per unit cost
% \[
% \frac{-\frac{\partial f}{\partial N_k}}{C_k} = \frac{\Delta_k}{C_kN_k^2}
% \]
% It quantifies that How much variance reduction we get per unit cost at level $k$.
% At the optimum, the system reaches equilibrium where every active model yields the same return per cost unit,
% \[
% \frac{\Delta_k}{C_kN_k^2} = \text{Constant}=\frac{1}{p^2}\left(\sum_{k=1}^K \sqrt{C_k \Delta_k}\right)^{\!2}, \quad \text{for all active}\;\; k.
% \]
















