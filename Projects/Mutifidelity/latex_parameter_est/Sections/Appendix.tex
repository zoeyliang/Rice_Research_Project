% ====================================================
\section{Appendix}\label{sec:Appendix}
% ====================================================
\subsection{More lemmas and theorems}
\begin{lemma}\label{lemma:Y_k_Y_j}
Let $2\le k<j\le K$. Then the correction terms $Y_k$ and $Y_j$ defined in \eqref{eq:MFMC_Yk} are uncorrelated; that is,
\begin{equation*}
    \operatorname{Cov} \left[Y_k,Y_j\right]=0.
\end{equation*}
\end{lemma}

\begin{proof}
Fix indices $2\le k<j\le K$. Since $N_{k-1}\le N_k\le N_{j-1}$, we can partition the $N_{j-1}$ samples into three mutually disjoint subsets with sizes $N_{k-1}$, $N_{k}-N_{k-1}$, and $N_{j-1} - N_{k}$. The samples in these three subsets are mutually independent. From the definition of $Y_k$ in \eqref{eq:MFMC_Yk},  the covariance between $Y_k$ and $Y_j$ is then given by
\begin{align*}
    \operatorname{Cov}\left[Y_k,Y_j\right] &= \left(\frac{N_{k-1}}{N_k}-1\right) \left(\frac{N_{j-1}}{N_j}-1\right)\operatorname{Cov}\left[A_{k, N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}} - A_{j,N_{j}\backslash N_{j-1}}^{\text{MC}}\right] \\
    & = M \left(\operatorname{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}}\right] - \operatorname{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j}\backslash N_{j-1}}^{\text{MC}}\right] \right)\\
    & = M \operatorname{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}}\right]
\end{align*}
where we define $M = (N_{k-1}/N_k-1) (N_{j-1}/N_j-1)$. The second term in the covariance vanishes due to independence between the samples used in $A_{j,N_{j}\backslash N_{j-1}}^{\text{MC}}$ and those in $Y_k$. Next, we express $A_{j,N_{j-1}}^{\text{MC}}$ as a weighted Monte Carlo estimator over the three disjoint sample subsets
%
\begin{equation*}
    A_{j,N_{j-1}}^{\text{MC}} = \frac{N_{k-1}}{N_{j-1}}A_{j,N_{k-1}}^{\text{MC}} + \frac{N_k - N_{k-1}}{N_{j-1}} A_{j,N_{k}\backslash N_{k-1}}^{\text{MC}} + \frac{N_{j-1} - N_k}{N_{j-1}} A_{j,N_{j-1}\backslash N_{k}}^{\text{MC}}.
\end{equation*}
%
Substituting this expansion into the covariance expression yields
%
\begin{align*}
    % \frac{\operatorname{Cov}\left[Y_k,Y_j\right]}{M} &= 
    &\operatorname{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}}\right]\\
    % &= \operatorname{Cov}\left[A_{N_{k-1}}^k, \frac{N_{k-1}}{N_{j-1}}A_{N_{k-1}}^j + \frac{N_k - N_{k-1}}{N_{j-1}} A_{N_{k}\backslash N_{k-1}}^j + \frac{N_{j-1} - N_k}{N_{j-1}} A_{N_{j-1}\backslash N_{k}}^j\right] \\
    % &- \operatorname{Cov}\left[ A_{N_{k}\backslash N_{k-1}}^k, \frac{N_{k-1}}{N_{j-1}}A_{N_{k-1}}^j + \frac{N_k - N_{k-1}}{N_{j-1}} A_{N_{k}\backslash N_{k-1}}^j + \frac{N_{j-1} - N_k}{N_{j-1}} A_{N_{j-1}\backslash N_{k}}^j\right]\\
    &=\operatorname{Cov}\left[A_{k,N_{k-1}}^{\text{MC}}, \frac{N_{k-1}}{N_{j-1}}A_{j,N_{k-1}}^{\text{MC}}\right]-\operatorname{Cov}\left[ A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, \frac{N_k - N_{k-1}}{N_{j-1}} A_{j,N_{k}\backslash N_{k-1}}^{\text{MC}} \right]\\
    &=\frac{\operatorname{Cov}\left[N_{k-1}A_{k,N_{k-1}}^{\text{MC}}, N_{k-1} A_{j,N_{k-1}}^{\text{MC}}\right]}{N_{j-1}N_{k-1}}-\frac{\operatorname{Cov}\left[(N_k-N_{k-1}) A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, (N_k - N_{k-1}) A_{j,N_{k}\backslash N_{k-1}}^{\text{MC}} \right]}{N_{j-1}(N_k-N_{k-1})}\\
    &=\frac{\operatorname{Cov}\left[\sum_{i=1}^{N_{k-1}}u_{h,k}\left(\cdot, \boldsymbol{\omega}^{(i)}\right),\sum_{i=1}^{N_{k-1}}u_{h,j}\left(\cdot, \boldsymbol{\omega}^{(i)}\right)\right]}{N_{j-1}N_{k-1}}
    -\frac{\operatorname{Cov}\left[\sum_{i=1}^{N_k-N_{k-1}}u_{h,k}\left(\cdot, \boldsymbol{\omega}^{(i)}\right), \sum_{i=1}^{N_k-N_{k-1}}u_{h,j}\left(\cdot, \boldsymbol{\omega}^{(i)}\right)\right]}{N_{j-1}(N_k-N_{k-1})}\\
    &=\frac{\sum_{i=1}^{N_{k-1}}\operatorname{Cov}\left[u_{h,k}\left(\cdot, \boldsymbol{\omega}^{(i)}\right),u_{h,j}\left(\cdot, \boldsymbol{\omega}^{(i)}\right)\right]}{N_{j-1}N_{k-1}} -\frac{\sum_{i=1}^{N_k-N_{k-1}}\operatorname{Cov}\left[u_{h,k}\left(\cdot, \boldsymbol{\omega}^{(i)}\right), u_{h,j}\left(\cdot, \boldsymbol{\omega}^{(i)}\right)\right]}{N_{j-1}(N_k-N_{k-1})}\\
    &=\frac{N_{k-1}\rho_{k,j}\sigma_k\sigma_j}{N_{j-1}N_{k-1}}-\frac{(N_k-N_{k-1})\rho_{k,j}\sigma_k\sigma_j}{N_{j-1}(N_k-N_{k-1})}=0
\end{align*}
\end{proof}



The following theorem is taken from \cite{CaBe:1990,LeCa:1998}.
\begin{theorem}[Multivariate Delta Method]\label{thm:Delta_method}
Let $\boldsymbol{X} = (X_1,\ldots,X_p)^T$ be a vector-valued random variable. Let $\boldsymbol{X}^{(1)}, \ldots, \boldsymbol{X}^{(n)}$ be $n$ i.i.d samples, each with mean $\mathbb{E}[\boldsymbol{X}^{(k)}] = \boldsymbol{\mu}$ and covariance matrix $\mathbb{E}[(\boldsymbol{X}^{(k)}-\boldsymbol{\mu})(\boldsymbol{X}^{(k)}-\boldsymbol{\mu})^T] = \Sigma$. Let $\widehat {\boldsymbol{X}}$ denote an estimator of $\boldsymbol{\mu}$ such that 
%
\[
\widehat {\boldsymbol{X}}\rightarrow \boldsymbol{\mu} \;\; \text{in probability},\quad \text{ and } \quad\sqrt{n}\left(\widehat {\boldsymbol{X}}-\boldsymbol{\mu}\right)\rightarrow N(0,\Sigma) \;\; \text{in distribution}.
\]
%
Suppose we are interested in a real-valued function $g$ with continuous first partial derivatives and a specific value of $\boldsymbol{\mu}$ for which $\nabla g(\boldsymbol{\mu})^T\Sigma\nabla g(\boldsymbol{\mu})>0$,  Then, under the first-order Taylor approximation 
\[
g\left(\widehat {\boldsymbol{X}}\right) \approx g\left(\boldsymbol{\mu}\right) + \nabla g(\boldsymbol{\mu})^T\cdot \left(\widehat {\boldsymbol{X}}-\boldsymbol{\mu}\right),
\]
we obtain the asymptotic distribution
\[
\sqrt{n}\left(g\left(\widehat {\boldsymbol{X}}\right) - g(\boldsymbol{\mu})\right)\rightarrow \mathcal{N}\left(0,\nabla g(\boldsymbol{\mu})^T\Sigma\nabla g(\boldsymbol{\mu})\right),\qquad \text{ in distribution.}
\]
Accordingly, the asymptotic variance of $g(\widehat {\boldsymbol{X}})$ is 
\[
\text{Var}\left[g(\widehat {\boldsymbol{X}})\right] \approx\nabla g(\boldsymbol{\mu})^T\Sigma\nabla g(\boldsymbol{\mu}) = \sum_i \left(\frac{\partial g}{\partial \widehat {\boldsymbol{X}}_i}\right)\text{Var}\left[\widehat {\boldsymbol{X}}_i\right] +  2\sum_{i<j} \left(\frac{\partial g}{\partial \widehat {\boldsymbol{X}}_i}\right)\left(\frac{\partial g}{\partial \widehat {\boldsymbol{X}}_j}\right)\text{Cov}\left[\widehat {\boldsymbol{X}}_i,\widehat {\boldsymbol{X}}_j\right].
\]

\end{theorem}



\subsection{Proof of Theorem \ref{thm:Sample_size_est}}

\begin{proof}
Consider feasible solutions in which the sample sizes $N_k^*$ are non-decreasing but may include segments where they are constant. Let $\{\ell_1, \ldots, \ell_q\}\subseteq \{1,\ldots, K\}$ denote indices such that with $\ell_1=1$ and $\ell_{q+1} = K+1$, with
%
\[
N_{\ell_1}<N_{\ell_2}<\ldots < N_{\ell_{q}},\quad\quad  N_{\ell_i}=N_{\ell_i+1}=\ldots = N_{\ell_{i+1}-1} <N_{\ell_{i+1}}, \qquad \text{for}\;\;  i=1,\ldots,q-1.
\]
%
This partitions the index set ${1,\dots,K}$ into $q$ contiguous blocks with constant sample sizes, increasing from one block to the next. 

Three special cases arise. When $q=1$, all sample sizes are equal, i.e., $N_1=\ldots=N_K$. From \eqref{eq:MFMC_variance}, we then have $N_k=\frac{\sigma_1^2}{\epsilon_{\text{tar}}^2}$ for $k=1,\ldots, K$, $\alpha_k\in \mathbb{R}$, and the total cost is $\mathcal{W}^\text{MF} = \frac{\sigma_1^2}{\epsilon_{\text{tar}}^2} \sum_{k=1}^K C_k$. When $q=K$, the sample sizes are strictly increasing, i.e., $N_1<\ldots<N_K$. In this case, each model receives a distinct sample size, and we will demonstrate that this configuration yields the globally optimal solution to the constrained optimization problem. For intermediate values $1<q<K$, constant sample-size blocks arise, and the optimal solution must be characterized more carefully using the Karush-Kuhn-Tucker (KKT) conditions to the Lagrangian formulation of the constrained optimization problem.

The Lagrangian function includes the total cost, the variance constraint with multiplier $\lambda_0$, and multipliers $\lambda_1,\ldots, \lambda_k$ enforcing the monotonicity constraints
%
\begin{equation*}
L =\sum_{k=1}^K C_kN_k +\lambda_0 \left(\frac{\sigma_1^2}{N_1} + \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\left(\alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k\right)- \epsilon_{\text{tar}}^2\right)-\lambda_1 N_1+\sum_{k=2}^K\lambda_k(N_{k-1} - N_k),
\end{equation*}
%
with $\alpha_1 = 1, \alpha_{K+1} = 0$ and $\lambda_{K+1} = 0$.  The KKT conditions includes
%
\[
\begin{array}{ll}
\left[\text{Stationarity}\right]&\frac{\partial L}{\partial \alpha_j}=0,\quad \frac{\partial L}{\partial N_k}=0,\quad j=2\ldots,K, \quad k=1\ldots,K,\\
\left[\text{Primal feasibility}\right]&\mathbb{V}\left[A^{\text{MF}}\right]- \epsilon_{\text{tar}}^2 = 0, \\ 
\left[\text{Primal feasibility}\right] &-N_1\le 0,\qquad N_{k-1}-N_k \le 0, \quad k=2\ldots,K,\\ 
\left[\text{Dual feasibility}\right]  &\lambda_k \ge 0,\quad k=1\ldots,K, \\ 
\left[\text{Complementary slackness}\right]  &\lambda_1 N_1=0,\qquad\lambda_k(N_{k-1}-N_k)=0,\quad k=2\ldots,K.
\end{array}
\]
%
 In particular, complementary slackness implies $\lambda_{\ell_i}= 0$ all $i\ge 1$, since the sample sizes within each block are equal. Under the blockwise structure $N_{\ell_i}=N_{\ell_i+1}=\ldots=N_{\ell_{i+1}-1}< N_{\ell_{i+1}}$ for $i=1,\ldots,q$, the Lagrangian simplifies to
%
\begin{equation*}
L= \sum_{i=1}^q N_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_k +\lambda_0 \left(\frac{\sigma_1^2}{N_{\ell_1}} + \sum_{i=2}^q \left(\frac{1}{N_{\ell_i-1}} - \frac{1}{N_{\ell_i}}\right)\left(\alpha_{\ell_i}^2\sigma_{\ell_i}^2 - 2\alpha_{\ell_i}\rho_{1,\ell_i}\sigma_1\sigma_{\ell_i}\right)- \epsilon_{\text{tar}}^2\right)-\lambda_{\ell_1} N_{\ell_1}+\sum_{i=2}^q\lambda_{\ell_{i}}(N_{\ell_{i-1}} - N_{\ell_{i}}).
\end{equation*}
%
Stationarity of the Lagrangian with respect to $\alpha_{\ell_i}$ yields
%
\begin{align}
\label{eq:partial_L_alpha_k}
    \frac{\partial L}{\partial \alpha_{\ell_i}}&=\lambda_0\left(\frac{1}{N_{\ell_i-1}} - \frac{1}{N_{\ell_i}}\right)\left(2\alpha_{\ell_i}\sigma_{\ell_i}^2 - 2\rho_{1,\ell_i}\sigma_1\sigma_{\ell_i}\right),\quad i=1,\dots,q-1,
    % \frac{\partial L}{\partial N_1}&=C_1 + \lambda_0\left(-\frac{\sigma_1^2}{N_1^2} - \frac{\alpha_2^2\sigma_2^2-2\alpha2\rho_{1,2}\sigma_1\sigma_2}{N_1^2}\right)-\lambda_1+\lambda_2,\\
    % \label{eq:partial_L_N_k}
    % \frac{\partial L}{\partial N_k}&=C_k+\lambda_0\left(\frac{G_k}{N_k^2}-\frac{G_{k+1}}{N_k^2}\right)-\lambda_k+\lambda_{k+1}, \quad k=1,\dots,K,
    % \frac{\partial L}{\partial N_K}&=C_K + \lambda_0\left(\frac{\alpha_K^2\sigma_K^2 - 2\alpha_K\rho_{1,K}\sigma_1\sigma_K}{N_K^2}\right)-\lambda_K.
\end{align}
%
Solving $\partial L/\partial \alpha_{\ell_i} = 0$ gives the optimal coefficients
%
\[
\alpha_{k}^* = \frac{\rho_{1,k}\sigma_1}{\sigma_{k}}, \text{ if } k=\ell_i, \text{ for }i= 1,\ldots,q.
\]
%
Note that only indices $k=\ell_i$ contribute non-trivially to the estimator weights in \eqref{eq:MFMC_Yk}. For indices $k = \ell_i+1,\ldots, \ell_{i+1}-1$, the corresponding Monte Carlo estimators cancel due to shared sample sets, and thus do not appear explicitly in the final estimator. Using the optimal coefficients $\alpha_k^*$, and noting that $N_{\ell_{i}-1} = N_{\ell_{i-1}}$ for $i \ge 2$, we define $\Delta_k = (\rho_{1,k}^2 - \rho_{1,k+1}^2)\sigma_1^2$, where $\rho_{1,K+1} = 0$. Substituting into the variance expression yields the simplified form
%
\begin{equation}\label{eq:MFMC_var_convex}
    \mathbb{V}\left[A^{\text{MF}}\right] = \frac{\sigma_1^2}{N_1}+\sum_{i=2}^q
\left(\frac{1}{N_{\ell_{i}}}-\frac{1}{N_{\ell_{i}-1}}\right)\rho_{1,\ell_i}^2\sigma_1^2=\sum_{i=1}^{q} \frac{ \left(\rho_{1,\ell_i}^2-\rho_{1,\ell_{i+1}}^2\right)\sigma_1^2}{N_{\ell_i}}=\sum_{i=1}^{q} \frac{\Delta_{\ell_i}}{N_{\ell_i}}.
\end{equation}
%
Since $N_1 > 0$ is required to keep the variance \eqref{eq:MFMC_var_convex} finite, the problem remains well-posed. Rewriting the optimization problem \eqref{eq:Optimization_pb_sample_size}  in terms of $y_{\ell_i} = 1/N_{\ell_i}$, we obtain
%
\begin{equation*}\label{eq:Optimization_pb_sample_size3}
    \begin{array}{ll}
    \min \limits_{\begin{array}{c}\scriptstyle y_{\ell_1},\ldots, y_{\ell_q}\in \mathbb{R}
\end{array}} &\displaystyle \sum_{i=1}^q \frac{C_{\ell_i}}{y_{\ell_i}},\\
       \;\,\text{subject to} &\displaystyle \sum_{i=1}^q \Delta_{\ell_i} y_{\ell_i}= \epsilon_{\text{tar}}^2,\\[2pt]
       &\displaystyle -y_{\ell_1}\le 0,\quad \displaystyle y_{\ell_i}-y_{\ell_{i-1}}\le 0, \;\; k=2\ldots,K.
    \end{array}
\end{equation*}
%
The objective is convex in $N_{\ell_i}$ since $C_{\ell_i}>0$ and $f(y) = 1/y$ is convex for $y > 0$. The equality constraint is affine, and the inequality constraints define a convex feasible set.  Thus, the reformulated problem is convex, and any local minimum is global. This indicates that the local minimizer to our optimization problem \eqref{eq:Optimization_pb_sample_size} is globally optimal.




Substituting the optimal coefficients into the Lagrangian and using the block-wise sample size notation, we obtain
%
\begin{equation*}
L= \sum_{i=1}^q N_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_k +\lambda_0 \left( \sum_{i=1}^{q} \frac{\Delta_{\ell_i}}{N_{\ell_i}}- \epsilon_{\text{tar}}^2\right)-\lambda_1 N_1+\sum_{i=2}^q\lambda_{\ell_{i}}(N_{\ell_{i-1}} - N_{\ell_{i}}).
\end{equation*}
%
Taking derivatives with respect to $N_{\ell_i}$ and applying the stationarity condition gives
%
\[
\frac{\partial L}{\partial N_{\ell_i}} =\sum_{k=\ell_i}^{\ell_{i+1}-1} C_{k} -  \frac{\lambda_0\Delta_{\ell_i}}{N_{\ell_i}^2}-\lambda_{\ell_{i}}+\lambda_{\ell_{i+1}},\quad i = 1, \ldots,q,
\]
%
Substituting it to zero yields a representation for cost in terms of the sample sizes $N_{\ell_i}$, variance contributions, correlation coefficients, and the Lagrangian multipliers
%
\begin{equation*}
    \sum_{k=\ell_i}^{\ell_{i+1}-1} C_{k}=\frac{\lambda_0\Delta_{\ell_i}}{N_{\ell_i}^2}+\lambda_{\ell_i}-\lambda_{\ell_{i+1}}, \;\text{ for }\; i=1,\ldots,q, 
\end{equation*}
%
If all inequality constraints are inactive, i.e., $\lambda_{\ell_i} = 0$ for all $i=1,\dots, q$ in the complementary slackness conditions, then the sample sizes increase strictly across indices ($N_{\ell_{i-1}} < N_{\ell_i}$ for $i=2,\ldots, q$), and the optimal sample sizes are given by
%
\begin{equation}\label{eq:sample_size_1}
    N_{\ell_i} = \sqrt{\lambda_0} \sqrt{\frac{\Delta_{\ell_i}}{\sum_{k=\ell_i}^{\ell_{i+1}-1} C_{k}}}, \;\text{ for }\; i=1,\ldots,q,
\end{equation}
%
Substituting $\alpha_k^*$ and $N_{\ell_i}$ in \eqref{eq:sample_size_1} into the variance expression \eqref{eq:MFMC_var_convex} yields
%
\begin{equation*} \label{eq:MFMC_variance2}
    \mathbb{V}\left[A^{\text{MF}}\right] = \frac{1}{\sqrt{\lambda_0}}\sum_{i=1}^q\sqrt{\Delta_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_k},
\end{equation*}
%
Enforcing the variance constraint $\mathbb{V}[A^{\text{MF}}] = \epsilon_{\text{tar}}^2$, we solve for $\sqrt{\lambda_0}$ as
%
\[
\sqrt{\lambda_0}=\frac{1}{\epsilon_{\text{tar}}^2} \sum_{i=1}^{q} \sqrt{\Delta_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_{k}},
\]
%
Substituting $\sqrt{\lambda_0}$ into \eqref{eq:sample_size_1}, the optimal sample sizes become
%
\[
N_{\ell_i}^* = \frac{1}{\epsilon_{\text{tar}}^2}\sqrt{\frac{\Delta_{\ell_i}}{\sum_{k=\ell_i}^{\ell_{i+1}-1} C_{k}}}  \sum_{j=1}^{q} \sqrt{\Delta_{\ell_j}\sum_{k=\ell_j}^{\ell_{j+1}-1} C_{k}} \;\text{ for }\; i=1,\ldots,q.
\]
%
% Note that by ensuring the condition $(ii)$ is satisfied, we can guarantee that $N_k^*$ increases strictly as $k$ grows. 
The total cost $\mathcal{W}^\text{MF}$ associated with these optimal sample sizes is
%
\begin{equation*}
\mathcal{W}^\text{MF} = \sum_{i=1}^q \sum_{k=\ell_i}^{\ell_{i+1}-1} C_k N_k = \sum_{i=1}^q N_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_k =\frac{1}{\epsilon_{\text{tar}}^2}\left(\sum_{i=1}^{q} \sqrt{\Delta_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_{k}}\right)^2.
\end{equation*}
%

\noindent {\bf Optimal solution (strictly increasing sample sizes)}

As shown in \cite[Lemma~A.3]{PeWiGu:2016}, the minimal cost $\mathcal{W}^\text{MF}$ occurs when the sample sizes increases strictly with model index $k$. In this case, with no block repetitions, the optimal coefficients and sample sizes reduce to
%
\begin{align*}
    % \label{eq:MFMC_coefficients}
    % &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\\
    \label{eq:MFMC_SampleSize}
    &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\qquad \;N_k^*=\frac{1}{\epsilon_\text{tar}^2}\sqrt{\frac{\Delta_k}{C_k}}\sum_{j=1}^K\sqrt{C_j\Delta_j},\quad \text{with}\;\;\rho_{K+1}=0,
\end{align*}
%
and the corresponding total sampling cost is
%
\[
    \mathcal{W}^\text{MF} = \sum_{k=1}^K C_k N_k^* = \frac{1}{\epsilon_{\text{tar}}^2}\left(\sum_{k=1}^K\sqrt{C_k\Delta_k}\right)^2,\quad \text{with}\;\;\rho_{K+1}=0.
\]
%


% Next, we want to show the global minimizer is achieved for $N_k^*$.

% The optimization problem in \cite{PeWiGu:2016} is 

% %
% \begin{equation}\label{eq:Optimization_pb_Phe}
%     \begin{array}{ll}
%     \min \limits_{\begin{array}{c}\scriptstyle N_1,\ldots, N_K\in \mathbb{R} \\[-4pt]
% \scriptstyle \alpha_2,\ldots,\alpha_K\in \mathbb{R}
% \end{array}} &\displaystyle \mathbb{V}\left(A^{\text{MF}}\right),\\
%        \;\,\text{subject to} &\sum_{k=1}^K C_kN_k = p,\\[2pt]
%        &\displaystyle -N_1\le 0,\quad \displaystyle N_{k-1}-N_k\le 0, \;\; k=2\ldots,K.
%     \end{array}
% \end{equation}
% %
% \noindent {\bf Feasibility}

% In problem \eqref{eq:Optimization_pb_sample_size}, $\{N_k^*\}$ satisfies $\mathbb{V}\left(A^{\text{MF}}\right) = \epsilon_{\text{tar}}^2$, the associated cost is $p^* = \sum C_k N_k^*$.

% For problem \eqref{eq:Optimization_pb_Phe}, we constrain that $\sum C_kN_k =p^*$, we can see that $\{N_k^*\}$ satisfies the solution. So $\{N_k^*\}$ is feasible for problem \eqref{eq:Optimization_pb_Phe}.


% \noindent {\bf Optimality}

% Suppose $\{N_k^*\}$ is not optimal for problem \eqref{eq:Optimization_pb_Phe}, then there exists another set $\{N_k^\dagger\}$ such that
% \[
% \mathbb{V}\left(A^{\text{MF}}; \{N_k^\dagger\}\right)< \epsilon_{\text{tar}}^2,\quad \text{and}\quad \sum C_kN_k^\dagger = p^*.
% \]
% But then $\{N_k^\dagger\}$ is feasible for problem \eqref{eq:Optimization_pb_sample_size}, and has lower cost, contradicting the optimality of $\{N_k^*\}$ in Problem A.

% Therefore, $\{N_k^*\}$ must be optimal for Problem B as well.


% for $k=\ell_1,\ldots, \ell_2-1$, $\lambda_1=0$,
% \[
% \frac{\partial L}{\partial N_{k}} = C_{k} - \lambda_0\sigma_1^2\left(\frac{1-\rho_{1,\ell_2}^2}{N_{\ell_1}^2}\right)-\lambda_1+\lambda_{\ell_2}
% \]
\end{proof}
%





\subsection{Proof of Theorem \ref{thm:Model_Selection}}
\begin{proof}
% \noindent {\bf Transformation based method}
\noindent {\bf Condition (i) of Theorem~\ref{thm:Sample_size_est}.}
To verify condition (i) of Theorem~\ref{thm:Sample_size_est}, which requires a decreasing trend in the absolute correlation coefficients, we formulate a hypothesis test based on the Fisher transformation and multivariate delta method. Let $\rho_{1,k}$ and $\rho_{1,k+1}$ denote the population correlations between the high-fidelity model and two consecutive low-fidelity models. Their corresponding Fisher-transformed values are $z_k$ and $z_{k+1}$, as defined in \eqref{eq:Fisher_z}. To assess whether $|\rho_{1,k}|$ exceeds $|\rho_{1,k+1}|$, we define the nonlinear function
%
\[
g_1(z_k,z_{k+1}):=\text{tanh}^2(z_k)-\text{tanh}^2(z_{k+1})=\rho_{1,k}^2-\rho_{1,k+1}^2,
\]
%
and assess its statistical significance by testing the null hypothesis $H_0: g_1(z_k,z_{k+1}) \le 0$ against the alternative $H_a: g_1(z_k,z_{k+1}) > 0$. 
 % we use $g_1(z_k,z_{k+1})=0$ as the working hypothesis. 
Applying a first-order Delta approximation around the true parameter vector  $(z_k,z_{k+1})^T$, we obtain
%
\[
g_1(u,v)\approx g_1(z_k,z_{k+1})+ \frac{\partial g_1}{\partial z_k} (u- z_k)+\frac{\partial  g_1}{\partial z_{k+1}}(v-z_{k+1}),
\]
%
where $\frac{\partial g_1}{\partial z_k} = 2\text{tanh}(z_k)(1-\text{tanh}^2(z_k)) = 2\rho_{1,k}(1-\rho_{1,k}^2)$, and $\frac{\partial g_1}{\partial z_{k+1}} = -2\rho_{1,k+1}(1-\rho_{1,k+1}^2)$. We first order the model by decreasing $|\widehat \rho_{1,k}|$ or $|\widehat z_k|$ for all $k$.  The variance of the sample estimator $g_1(\widehat z_k, \widehat z_{k+1})$ is approximated by
%
\begin{align*}
    \text{Var}\left[g_1(\widehat z_k, \widehat z_{k+1})\right] &\approx \left(\frac{\partial g_1}{\partial z_k}\Bigg\vert_{(\widehat z_k, \widehat z_{k+1})^T}\right)^2 \text{Var}\left[\widehat z_k\right]+\left(\frac{\partial g_1}{\partial z_{k+1}}\Bigg\vert_{(\widehat z_k, \widehat z_{k+1})^T}\right)^2 \text{Var}\left[\widehat z_{k+1}\right]\\
    &+2 \frac{\partial g_1}{\partial z_k}\Bigg\vert_{(\widehat z_k, \widehat z_{k+1})^T} \frac{\partial g_1}{\partial  z_{k+1}}\Bigg\vert_{(\widehat z_k, \widehat z_{k+1})^T}\text{Cov}\left[\widehat z_{k}, \widehat z_{k+1}\right].
\end{align*}
%
Since $\widehat z_k$ and $\widehat z_{k+1}$ are computed from independent samples, the covariance term in the Delta method approximation vanishes. Using the known variance of Fisher-transformed correlation coefficients, $\text{Var}(\widehat z_k) = 1.03^2 / (Q - 3)$ as in \eqref{eq:SD_Fisher_Trans}, the resulting test statistic is
%
\begin{equation}\label{eq:Conidtion_1_t_test}
    t_k = \frac{g_1(\widehat z_k,\widehat z_{k+1})}{\sqrt{\text{Var}[g_1(\widehat z_k,\widehat z_{k+1})]}}=\frac{\widehat \rho_{1,k}^2-\widehat \rho_{1,k+1}^2}{\frac{1.03\cdot 2}{\sqrt{Q-3}}\sqrt{\widehat \rho_{1,k}^2\left(1-\widehat \rho_{1,k}^2\right)^2 +\widehat \rho_{1,k+1}^2\left(1-\widehat \rho_{1,k+1}^2\right)^2}}, 
\end{equation}
%
This test provides a principled statistical basis for verifying the monotonic decay of correlation magnitudes, even in the presence of overlapping confidence intervals. 
% To assess statistical significance, we approximate the degrees of freedom $\nu$ using the pooled variance via the {\it Welchâ€“Satterthwaite} equation \cite{F:1946}, which accounts for unequal variances
% %
% \[
% \nu = \left\lfloor\frac{\left(\text{Var}\left[|\widehat \rho_{1,k}|\right] + \text{Var}\left[|\widehat \rho_{1,k+1}|\right]\right)^2}{\frac{\left(\text{Var}\left[|\widehat \rho_{1,k}|\right]\right)^2 }{Q-1}+ \frac{\left(\text{Var}\left[|\widehat \rho_{1,k+1}|\right]\right)^2 }{Q-1} }\right\rfloor = \left\lfloor(Q-1)\frac{\left[(1-\widehat\rho_{1,k}^2)^2 + (1-\widehat\rho_{1,k+1}^2)^2\right]^2}{(1-\widehat\rho_{1,k}^2)^4 + (1-\widehat\rho_{1,k+1}^2)^4}\right\rfloor.
% \]
% %
Let $t_{\alpha,\nu}$ denote the upper $\alpha$ quantile of the $t$-distribution with $\nu$ degrees of freedom. Starting from the model index $k=2$, if $t_k > t_{\alpha,\nu}$, we reject the null hypothesis and conclude that $|\rho_{1,k}|$ is significantly greater than $|\rho_{1,k+1}|$, thereby retaining both models for further consideration. Otherwise, we determine that $|\rho_{1,k}|$ is not significantly larger than $|\rho_{1,k+1}|$, and we discard the model associated with $|\widehat \rho_{1,k+1}|$.

\noindent {\bf Condition (ii) of Theorem~\ref{thm:Sample_size_est}}

To evaluate condition (ii) of Theorem~\ref{thm:Sample_size_est}, we define the difference
%
\begin{align*}
    g_2 (z_{k-1},z_k,z_{k+1})&=-C_{k}\text{tanh}^2(z_{k-1}) + (C_{k-1} + C_k)\text{tanh}^2(z_{k})-C_{k-1}\text{tanh}^2(z_{k+1})\\
    &=C_{k-1}(\rho_{1,k}^2-\rho_{1,k+1}^2) - C_k (\rho_{1,k-1}^2 - \rho_{1,k}^2)
    %= -C_{k}\rho_{1,k-1}^2+(C_{k-1} + C_k)\rho_{1,k}^2  - C_{k-1}\rho_{1,k+1}^2
\end{align*}
%
and test whether $g_2 > 0$. Using a first-order delta approximation, we expand $g_2$ as
%
\begin{align*}
    g_2(u_{k-1},u_k,u_{k+1})  &\approx g_2(z_{k-1},z_k,z_{k+1}) +  \sum_{i=k-1}^{k+1}\frac{\partial g_2}{\partial z_i}(u_i - z_i),
\end{align*}
%
where the partial derivatives are given by
%
\begin{equation*}
    \frac{\partial g_2}{\partial z_i}= \left\{\begin{array}{ll}
-2C_k\text{tanh}(z_{k-1})\text{sech}^2(z_{k-1})=-2C_k\rho_{1,k-1}(1- \rho_{1,k-1}^2), & i=k-1,\\
2(C_{k-1} + C_k)\text{tanh}(z_{k})\text{sech}^2(z_{k})=2(C_{k-1} + C_k) \rho_{1,k}(1- \rho_{1,k}^2), & i=k,\\
- 2C_{k-1}\text{tanh}(z_{k+1})\text{sech}^2(z_{k+1})=- 2C_{k-1} \rho_{1,k+1}(1- \rho_{1,k+1}^2), & i=k+1.
\end{array}
\right.
\end{equation*}
%

with variance
\[
\text{Var}\left[g_2\left(\widehat z_{k-1}, \widehat z_k, \widehat z_{k+1}\right) \right]\approx \sum_{i=k-1}^{k+1}\left(\frac{\partial g_2 }{\partial z_i}\Bigg\vert_{(\widehat z_{k-1},\widehat z_k,\widehat z_{k+1})^T}\right)^2 \text{Var} \left[\widehat z_i\right] + 2\sum_{i<j}\frac{\partial g_2 }{\partial z_i}\Bigg\vert_{(\widehat z_{k-1},\widehat z_k,\widehat z_{k+1})^T}\frac{\partial g_2 }{\partial z_j}\Bigg\vert_{(\widehat z_{k-1},\widehat z_k,\widehat z_{k+1})^T}\text{Cov}\left[\widehat z_i,\widehat z_j\right].
\]
Since $\widehat z_{k-1}$, $\widehat z_k$, and $\widehat z_{k+1}$ are computed from independent samples, their covariances vanish, and the the test statistic becomes
%
\begin{align*}\label{eq:Conidtion_2_t_test}
    t_k&=\frac{ g_2 (\widehat z_{k-1}, \widehat z_k, \widehat z_{k+1})}{\sqrt{\text{Var}\left[g_2 (\widehat z_{k-1}, \widehat z_k, \widehat z_{k+1})\right]}} \\
    &= \frac{-C_{k}\widehat\rho_{1,k-1}^2+(C_{k-1} + C_k)\widehat \rho_{1,k}^2  - C_{k-1}\widehat \rho_{1,k+1}^2}{\frac{1.03}{\sqrt{Q-3}}\cdot 2\sqrt{C_k^2(\widehat\rho_{1,k-1}- \widehat\rho_{1,k-1}^3)^2+(C_{k-1} + C_k)^2 (\widehat\rho_{1,k}- \widehat\rho_{1,k}^3)^2+C_{k-1}^2 (\widehat\rho_{1,k+1}- \widehat\rho_{1,k+1}^3)^2}},
\end{align*}
%
% The degree of freedom is 
% %
% \begin{align*}
%     \nu &= \left\lfloor\frac{\left(\text{Var}\left[C_{k-1}(\widehat\rho_{1,k}^2-\widehat\rho_{1,k+1}^2)\right] + \text{Var}\left[C_k (\widehat\rho_{1,k-1}^2 - \widehat\rho_{1,k}^2)\right]\right)^2}{\frac{\left(\text{Var}\left[C_{k-1}(\widehat\rho_{1,k}^2-\widehat\rho_{1,k+1}^2)\right]\right)^2 }{Q-1}+ \frac{\left(\text{Var}\left[C_k (\widehat\rho_{1,k-1}^2 - \widehat\rho_{1,k}^2)\right]\right)^2 }{Q-1} }\right\rfloor,\\
%     &=\left\lfloor(Q-1)\frac{\left(C_{k-1}\left[\left(\widehat \rho_{1,k} - \widehat \rho_{1,k}^3\right)^2 +\left(\widehat \rho_{1,k+1} - \widehat \rho_{1,k+1}^3\right)^2\right] + C_{k}\left[\left(\widehat \rho_{1,k-1} - \widehat \rho_{1,k-1}^3\right)^2 +\left(\widehat \rho_{1,k} - \widehat \rho_{1,k}^3\right)^2\right]\right)^2 }{C_{k-1}^2\left[\left(\widehat \rho_{1,k} - \widehat \rho_{1,k}^3\right)^2 +\left(\widehat \rho_{1,k+1} - \widehat \rho_{1,k+1}^3\right)^2\right]^2 + C_{k}^2\left[\left(\widehat \rho_{1,k-1} - \widehat \rho_{1,k-1}^3\right)^2 +\left(\widehat \rho_{1,k} - \widehat \rho_{1,k}^3\right)^2\right]^2}\right\rfloor.
% \end{align*}
% %
% where the samples in estimating $\widehat \rho_{1,k}$ and $\widehat\rho_{1,k+1}$ are independent, then 
% %
% \begin{align*}
% &\text{Var}\left[\widehat \rho_{1,k}^2 - \widehat \rho_{1,k+1}^2\right] = \text{Var}\left[\widehat \rho_{1,k}^2\right] + \text{Var}\left[\widehat \rho_{1,k+1}^2\right]\approx 4\widehat \rho_{1,k}^2\text{Var}\left[\text{tanh}(z_k)\right]+4\widehat \rho_{1,k+1}^2\text{Var}\left[\text{tanh}(z_{k+1})\right]\\
% &\approx 4\widehat \rho_{1,k}^2\left(1-\widehat \rho_{1,k}^2\right)^2\text{Var}\left[z_k\right]+4\widehat \rho_{1,k+1}^2\left(1-\widehat \rho_{1,k+1}^2\right)^2\text{Var}\left[z_{k+1}\right] = \frac{1.03^2\cdot 4}{Q-3}\left(\left(\widehat \rho_{1,k}-\widehat \rho_{1,k}^3\right)^2+\left(\widehat \rho_{1,k+1}-\widehat \rho_{1,k+1}^3\right)^2\right).
% \end{align*}
% %
    
If $t>t_{\alpha,\nu}$, we accept accept $g_2>0$, namely the second condition (ii) holds.


\noindent {\bf Select the models with smallest sampling cost.}

Let $\mathcal{I} = \{1,\ldots,K\}$ denote the ordered indices corresponding to the models in $\mathcal{S}$, and let $\mathcal{I}^*\subseteq \mathcal{I}$ represent the indices of the selected subset $\mathcal{S}^*$. Since the high-fidelity model $u_{h,1}$ is always included, both $\mathcal{I}$ and $\mathcal{I}^*$ must contain index 1. The objective, then, is to identify an optimal index subset $\mathcal{I}^*$ of size $K^* = |\mathcal{I}^*| \leq K$ such that the resulting model combination minimizes $\xi$ while satisfying the conditions of Theorem~\ref{thm:Sample_size_est}.

Let 
\[
S_1=\sum_{p=1}^{K_1^*}\sqrt{C_{i_p}(\rho_{1,i_p}^2-\rho_{1,i_{p+1}}^2)} = \sqrt{C_{i_1}(\rho_{1,i_1}^2-\rho_{1,i_2}^2)}+\sqrt{C_{i_2}(\rho_{1,i_2}^2-\rho_{1,i_3}^2)}+\ldots+\sqrt{C_{i_{K_1^*}}\rho_{1,i_{K_1^*}}^2},
\]
where $i_1=1$.
\[
\frac{\partial S_1}{\partial z_{i_p}} =(1- \rho_{1,{i_p}}^2) \rho_{1,{i_p}} \left(\frac{-\sqrt{C_{i_{p-1}}}}{\sqrt{ \rho_{1,{i_{p-1}}}^2-\rho_{1,{i_{p}}}^2}}+\frac{\sqrt{C_{i_p}}}{\sqrt{ \rho_{1,{i_p}}^2-\rho_{1,{i_{p+1}}}^2}} \right) = (1- \rho_{1,{i_p}}^2)\frac{\partial S_1}{\partial \rho_{1,i_p}}, \quad \text{for} \quad p = 2,\ldots,K_1^*.
\]
and $\frac{\partial S_1}{\partial z_{i_1}} = 0$. Similarly, for a new set of indices, we can define $S_2=\sum_{p=1}^{K_2^*}\sqrt{C_{j_p}( \rho_{1,j_p}^2-\rho_{1,j_{p+1}}^2)}$ and $\partial S_2/\partial z_{j_p}$.

Define $g_3 = S_1-S_2$, want to check if $g_3>0$ or not.
\[
g_3(u_{i_1},\ldots,u_{i_{K_1^*}},u_{j_1},\ldots,u_{j_{K_2^*}})\approx g_3(z_{i_1},\ldots,z_{i_{K_1^*}},z_{j_1},\ldots,z_{j_{K_2^*}})+\sum_{p=1}^{K_1^*} \frac{\partial S_1}{\partial z_{i_p}}(u_{i_p} - z_{i_p}) - \sum_{p=1}^{K_2^*} \frac{\partial S_2}{\partial z_{j_p}}(u_{j_p} - z_{j_p})
\]
Notice that independent samples are use to compute each sample estimate 
\[
\text{Var}\left[g_3(\widehat z_{i_1},\ldots,\widehat z_{i_{K_1^*}}, \widehat z_{j_1},\ldots, \widehat z_{j_{K_2^*}})\right] \approx \sum_{p=1}^{K_1^*} \left(\frac{\partial S_1}{\partial z_{i_p}}\Bigg\vert_{(\widehat z_{i_1},\ldots,\widehat z_{i_{K_1^*}})^T}\right)^2\text{Var}\left[\widehat  z_{i_p}\right] + \sum_{p=1}^{K_2^*} \left(\frac{\partial S_2}{\partial z_{j_p}}\Bigg\vert_{(\widehat z_{j_1},\ldots,\widehat z_{j_{K_2^*}})^T}\right)^2\text{Var}\left[\widehat z_{j_p}\right],
\]
%
\begin{align}\label{eq:cost_decrease_t_test}
    t_k &= \frac{g_3(\widehat z_{i_1},\ldots,\widehat z_{i_{K_1^*}}, \widehat z_{j_1},\ldots, \widehat z_{j_{K_2^*}})}{\sqrt{\text{Var}\left[g_3(\widehat z_{i_1},\ldots,\widehat z_{i_{K_1^*}}, \widehat z_{j_1},\ldots, \widehat z_{j_{K_2^*}})\right] }}\\
    &= \frac{\sum_{p=1}^{K_1^*}\sqrt{C_{i_p}(\widehat \rho_{1,i_p}^2-\widehat\rho_{1,i_{p+1}}^2)} - \sum_{p=1}^{K_2^*}\sqrt{C_{j_p}(\widehat \rho_{1,j_p}^2-\widehat\rho_{1,j_{p+1}}^2)}}{\frac{1.03}{\sqrt{Q-3}}\sqrt{\sum_{p=1}^{K_1^*} (1- \widehat \rho_{1,i_p}^2)^2\left(\frac{\partial S_1}{\partial \rho_{1,i_p}}\big\vert_{(\widehat \rho_{1,i_1},\ldots,\widehat \rho_{1,i_{K_1^*}})^T}\right)^2 + \sum_{p=1}^{K_2^*} (1- \widehat \rho_{1,j_p}^2)^2\left(\frac{\partial S_2}{\partial \rho_{1,j_p}}\big\vert_{(\widehat \rho_{1,j_1},\ldots,\widehat \rho_{1,j_{K_2^*}})^T}\right)^2}}
\end{align}
%
% The degree of freedom is 
% %
% \begin{align*}
%     \nu &= \left\lfloor\frac{\left(\text{Var}\left[\sum_{p=1}^{K_1^*}\sqrt{C_{i_p}(\widehat\rho_{1,i_p}^2-\widehat\rho_{1,i_{p+1}}^2)} \right] + \text{Var}\left[\sum_{p=1}^{K_2^*}\sqrt{C_{j_p}( \widehat\rho_{1,j_p}^2-\widehat\rho_{1,j_{p+1}}^2)}\right]\right)^2}{\frac{\left(\text{Var}\left[\sum_{p=1}^{K_1^*}\sqrt{C_{i_p}(\widehat\rho_{1,i_p}^2-\widehat\rho_{1,i_{p+1}}^2)} \right]\right)^2 }{Q-1}+ \frac{\left(\text{Var}\left[\sum_{p=1}^{K_2^*}\sqrt{C_{j_p}( \widehat\rho_{1,j_p}^2-\widehat\rho_{1,j_{p+1}}^2)}\right]\right)^2 }{Q-1} }\right\rfloor,
% \end{align*}
% %
% Next we want to estimate $\nu$.
% Let $E_{i_p}=\sqrt{C_{i_p}(\rho_{1,i_p}^2 - \rho_{1,i_{p+1}}^2)}=\sqrt{C_{i_p}(\text{tanh}(z_{i_p})^2 - \text{tanh}(z_{i_{p+1}})^2)}$.
% \[
% \widehat E_{i_p} = \sqrt{C_{i_p}(\text{tanh}(\widehat z_{i_p})^2 - \text{tanh}(\widehat z_{i_{p+1}})^2)}\approx E_{i_p} + \frac{\partial E_{i_p}}{\partial z_{i_p}}(\widehat z_{i_p} - z_{i_p}) + \frac{\partial E_{i_p}}{\partial z_{i_{p+1}}}(\widehat z_{i_{p+1}} - z_{i_{p+1}}),
% \]
% where
% \[
% \frac{\partial E_{i_p}}{\partial z_{i_p}} = \frac{\sqrt{C_{i_p}}\rho_{1,i_p}(1-\rho_{1,i_p}^2)}{\sqrt{\rho_{1,i_p}^2 - \rho_{1,i_{p+1}}^2}}, \quad \frac{\partial E_{i_p}}{\partial z_{i_{p+1}}} = -\frac{\sqrt{C_{i_p}}\rho_{1,i_{p+1}}(1-\rho_{1,i_{p+1}}^2)}{\sqrt{\rho_{1,i_p}^2 - \rho_{1,i_{p+1}}^2}},
% \]
% \begin{align*}
%     \sum_{p=1}^{K_1^*} \widehat E_{i_p}& = \sum_{p=1}^{K_1^*}E_{i_p} +  \sum_{p=1}^{K_1^*}\frac{\partial E_{i_p}}{\partial z_{i_p}}(\widehat z_{i_p} - z_{i_p}) +  \sum_{p=1}^{K_1^*-1}\frac{\partial E_{i_p}}{\partial z_{i_{p+1}}}(\widehat z_{i_{p+1}} - z_{i_{p+1}})=\sum_{p=1}^{K_1^*}E_{i_p} +\sum_{p=1}^{K_1^*} \left(\frac{\partial E_{i_p}}{\partial z_{i_{p}}}+\frac{\partial E_{i_{p-1}}}{\partial z_{i_{p}}}\right)(\widehat z_{i_p} - z_{i_p})\\
%     &=\sum_{p=1}^{K_1^*}E_{i_p} +\sum_{p=1}^{K_1^*} \left(\frac{-\sqrt{C_{i_{p-1}}}}{\sqrt{ \rho_{1,{i_{p-1}}}^2-\rho_{1,{i_{p}}}^2}}+\frac{\sqrt{C_{i_p}}}{\sqrt{ \rho_{1,{i_p}}^2-\rho_{1,{i_{p+1}}}^2}}\right)\rho_{1,i_p}\left(1-\rho_{1,i_p}^2\right)\left(\widehat z_{i_p} - z_{i_p}\right)
% \end{align*}
% where $E_{i_0} = 0$. Since $\widehat \rho_{1,k}$ and $\widehat \rho_{1,k+1}$ are estimated using independent samples, then
% %
% \begin{align*}
%     \text{Var}\left[\sum_{p=1}^{K_1^*} \widehat E_{i_p}\right]&\approx \sum_{p=1}^{K_1^*} \left(\frac{-\sqrt{C_{i_{p-1}}}}{\sqrt{ \widehat\rho_{1,{i_{p-1}}}^2-\widehat\rho_{1,{i_{p}}}^2}}+\frac{\sqrt{C_{i_p}}}{\sqrt{ \widehat\rho_{1,{i_p}}^2-\widehat\rho_{1,{i_{p+1}}}^2}}\right)^2\widehat\rho_{1,i_p}^2\left(1-\widehat\rho_{1,i_p}^2\right)^2 \text{Var}\left[\widehat z_{i_p}\right]\\
%     &=\frac{1.03^2}{Q-3}\sum_{p=1}^{K_1^*} \left(\frac{-\sqrt{C_{i_{p-1}}}}{\sqrt{ \widehat\rho_{1,{i_{p-1}}}^2-\widehat\rho_{1,{i_{p}}}^2}}+\frac{\sqrt{C_{i_p}}}{\sqrt{ \widehat\rho_{1,{i_p}}^2-\widehat\rho_{1,{i_{p+1}}}^2}}\right)^2\widehat\rho_{1,i_p}^2\left(1-\widehat\rho_{1,i_p}^2\right)^2\\
%     &=\frac{1.03^2}{Q-3}\sum_{p=1}^{K_1^*}\left(\frac{\partial S_1}{\partial \rho_{1,i_p}}\bigg\vert_{(\widehat \rho_{1,i_1},\ldots,\widehat \rho_{1,i_{K_1^*}})^T}\right)^2 (1- \widehat \rho_{1,i_p}^2)^2
% \end{align*}
% %

% Therefore
% \[
% \nu = \left\lfloor (Q-1)\frac{\left[\sum_{p=1}^{K_1^*}\left(\frac{\partial S_1}{\partial \rho_{1,i_p}}\bigg\vert_{(\widehat \rho_{1,i_1},\ldots,\widehat \rho_{1,i_{K_1^*}})^T}\right)^2 (1- \widehat \rho_{1,i_p}^2)^2+\sum_{p=1}^{K_2^*}\left(\frac{\partial S_2}{\partial \rho_{1,j_p}}\bigg\vert_{(\widehat \rho_{1,j_1},\ldots,\widehat \rho_{1,j_{K_2^*}})^T}\right)^2 (1- \widehat \rho_{1,j_p}^2)^2\right]^2}{\left[\sum_{p=1}^{K_1^*}\left(\frac{\partial S_1}{\partial \rho_{1,i_p}}\bigg\vert_{(\widehat \rho_{1,i_1},\ldots,\widehat \rho_{1,i_{K_1^*}})^T}\right)^2 (1- \widehat \rho_{1,i_p}^2)^2\right]^2 + \left[\sum_{p=1}^{K_2^*}\left(\frac{\partial S_2}{\partial \rho_{1,j_p}}\bigg\vert_{(\widehat \rho_{1,j_1},\ldots,\widehat \rho_{1,j_{K_2^*}})^T}\right)^2 (1- \widehat \rho_{1,j_p}^2)^2\right]^2}\right\rfloor.
% \]



% \begin{align*}
%     \text{Var}\left[ \widehat E_k\right] &\approx \left(\frac{\partial E_k}{\partial z_{k}}\right)^2  \text{Var}\left[ \widehat z_{k}\right] + \left(\frac{\partial E_k}{\partial z_{k+1}}\right)^2  \text{Var}\left[ \widehat z_{k+1}\right]=\frac{1.03^2}{Q-3}\frac{C_k\left[\rho_{1,k}^2(1-\rho_{1,k}^2)^2+ \rho_{1,k+1}^2(1-\rho_{1,k+1}^2)^2\right]}{\rho_{1,k}^2 - \rho_{1,k+1}^2}
% \end{align*}


\end{proof}

% \subsection{Proof of Theorem \ref{thm:Sample_size_est_conf_interval} }

% \begin{proof}
% 	Since the samples in estimating $\widehat \rho_{1,k}$ and $\widehat\rho_{1,k+1}$ are independent, then 
% 	%
% 	\begin{align*}
% 	\text{Var}\left[\Delta_k\right] &= \text{Var}\left[\widehat \rho_{1,k}^2\right] + \text{Var}\left[\widehat \rho_{1,k+1}^2\right]\approx 4\widehat \rho_{1,k}^2\text{Var}\left[\text{tanh}(z_k)\right]+4\widehat \rho_{1,k+1}^2\text{Var}\left[\text{tanh}(z_{k+1})\right]\\
% 	&\approx 4\widehat \rho_{1,k}^2\left(1-\widehat \rho_{1,k}^2\right)^2\text{Var}\left[z_k\right]+4\widehat \rho_{1,k+1}^2\left(1-\widehat \rho_{1,k+1}^2\right)^2\text{Var}\left[z_{k+1}\right] \\
% 	&= 4\left(\widehat \rho_{1,k}^2\left(1-\widehat \rho_{1,k}^2\right)^2+\widehat \rho_{1,k+1}^2\left(1-\widehat \rho_{1,k+1}^2\right)^2\right)\frac{1.03^2}{Q-3}
% 	\end{align*}
% 	The confidence interval for $\Delta_k$ is 
% 	\[
% 	CI_{\Delta_k} = \left[\Delta_k-t_{\alpha/2, \nu} \sqrt{\text{Var}\left[\Delta_k\right]}, \Delta_k+t_{\alpha/2,\nu} \sqrt{\text{Var}\left[\Delta_k\right]}\right] = \left[\Delta_k^{\text{lower}},\Delta_k^{\text{upper}}\right]
% 	\]
% 	Since we have different and  unknown variance for $\widehat \rho_{1,k}^2$ and $\widehat \rho_{1,k+1}^2$ and same sample size $Q$, the degree of freedom $\nu$ is approximated using the {\it Welch-Satterthwaite} equation
% 	\[
% 	\nu = \frac{\left(\text{Var}\left[\widehat \rho_{1,k}^2\right] + \text{Var}\left[\widehat \rho_{1,k+1}^2\right]\right)^2}{\frac{\left(\text{Var}\left[\widehat \rho_{1,k}^2\right]\right)^2 }{Q-1}+ \frac{\left(\text{Var}\left[\widehat \rho_{1,k+1}^2\right]\right)^2 }{Q-1} }
% 	\]
	
	
	
% 	% If no normality is assumed, we consider the two successive level $k$ and $k+1$, and consider the worst square difference
% 	% \begin{equation}
% 	% \label{eq:delta_rho_square}
% 	%     \Delta_k^{\text{min}} = \max \left\{\left(\rho_{1,k}^{\text{lower}}\right)^2 - \left(\rho_{1,k+1}^{\text{upper}}\right)^2,0\right\},\quad \Delta_k^{\text{max}} = \left(\rho_{1,k}^{\text{upper}}\right)^2 - \left(\rho_{1,k+1}^{\text{lower}}\right)^2,
% 	% \end{equation}
% 	% where $\rho_{1,1}^{\text{lower}} =\rho_{1,1}^{\text{upper}}= 1, \rho_{1,K+1}^{\text{lower}}=\rho_{1,K+1}^{\text{upper}} = 0$. If $\Delta_k^{\text{min}}=0$, it indicates that level $k$ and $k+1$ may encounter overlap and does not have obvious distinction, we could consider to combine these two levels. Moreover, if uncertainty is small and confidence interval is almost symmetric, we believe that the true value is most likely to fall near the middle of the interval $\overline{\rho}_{1,k} = \frac{1}{2}(\rho_{1,k}^{\text{lower}}+\rho_{1,k}^{\text{upper}})$, we can consider the midpoint of each confidence interval (instead of two endpoints) and substitute into \eqref{eq:delta_rho_square}.
	
% 	% \[
% 	% \overline{\rho}_{1,k} = \frac{\rho_{1,k}^{\text{lower}}+\rho_{1,k}^{\text{upper}}}{2}, \quad \Delta_k = \max \left\{\overline{\rho}_{1,k}^2 - \overline{\rho}_{1,k+1}^2,0\right\},
% 	% \]
% 	% we consider the constrained optimization problem \ref{eq:Optimization_pb_sample_size}
% 	% %
% 	% \begin{equation}\label{eq:Optimization_pb_sample_size2}
% 	%     \begin{array}{ll}
% 	%     \min \limits_{\begin{array}{c}\scriptstyle N_1,\ldots, N_K\in \mathbb{R} \\[-4pt]
% 	% \end{array}} &\displaystyle\sum\limits_{k=1}^K C_kN_k,\\
% 	%        \;\,\text{subject to} &\mathbb{V}\left[A^{\text{MF}}\right]=\sigma_1^2 \sum_{k=1}^K\frac{\rho_{1,k}^2 - \rho_{1,k+1}^2}{N_k}=   \epsilon_{\text{tar}}^2,\\[2pt]
% 	%        &\displaystyle -N_1\le 0,\quad \displaystyle N_{k-1}-N_k\le 0, \;\; k=2\ldots,K.
% 	%     \end{array}
% 	% \end{equation}
% 	% %
% 	% Condition (ii) of Theorem \ref{thm:Sample_size_est} indicates that $S^\prime<0$ in \eqref{eq:S_n_S_prime}, therefore,  both $\mathbb{V}[A^{\text{MF}}]$ and optimal sample size $N_k$ are monotonically decreasing in  $\rho_{1,k}$ as shown in \eqref{eq:partial_var_rho}.  Thus, the largest sample size and variance occur at the lower confidence bounds $\widehat\rho_{1,k}^{\text{low}}$.
	
% 	% To compute this upper bound on sample size, Substituting  $\rho_{1,k} = \widehat\rho_{1,k}^{\text{low}}$ into the variance constraint of \eqref{eq:Optimization_pb_sample_size2}, we solve the optimization problem with equality using Lagrange multipliers. The resulting Lagrangian is
% 	% %
% 	% \[
% 	% L = \sum_{k=1}^K C_kN_k +\lambda_0 \left(\frac{\sigma_1^2}{N_1} - \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\left(\widehat\rho_{1,k}^{\text{low}}\right)^2\sigma_1^2\right)-\lambda_1 N_1+\sum_{k=2}^K\lambda_k(N_{k-1} - N_k),
% 	% \]
% 	% %
% 	% Setting $\partial L / \partial N_k = 0$ yields the sample sizes $N_k^{\text{max}}$ as claimed.
% 	% %
% 	% \[
% 	% N_k^{\text{max}} = \frac{\sigma_1^2}{\epsilon_{\text{tar}}^2}\sqrt{\frac{(\text{CI}_{\rho_{1,k}}^{\text{L}})^2-(\text{CI}_{\rho_{1,k+1}}^{\text{L}})^2}{C_k}}\sum_{j=1}^K\sqrt{C_j\left((\text{CI}_{\rho_{1,j}}^{\text{L}})^2-(\text{CI}_{\rho_{1,j+1}}^{\text{L}})^2\right)},\quad \text{for}\quad  k=1,\ldots,K.
% 	% \]
% 	% %
	
% 	% At the upper endpoint $\widehat\rho_{1,k}^{\text{high}}$, the variance constraint is not active, and the optimal solution to the relaxed problem yields no meaningful lower bound (e.g., $N_k = 0$ is feasible). To obtain a practical confidence interval, we instead linearize $N_k$ with respect to $\rho_{1,k}$ using a first-order Taylor expansion:
	
	
% 	% At the upper endpoint $\widehat\rho_{1,k}^{\text{high}}$, the variance constraint is strictly satisfied, and the corresponding Lagrange multiplier $\lambda_0$ vanishes due to complementary slackness. The optimization problem then reduces to minimizing the cost subject only to the non-decreasing sample sizes
% 	% %
% 	% \[
% 	% \min \limits_{\begin{array}{c}\scriptstyle N_1,\ldots, N_K\in \mathbb{R}\end{array}}\sum\limits_{k=1}^K C_kN_k, \quad \text{subject to} \quad  0\le N_1\le \cdots \le N_K.
% 	% \]
% 	% %
% 	% While the trivial solution $N_k^{\text{min}} = 0$ satisfies these constraints, it does not produce a meaningful lower bound. 
	
% 	Moreover, if $\widehat \rho_{1,k}$ is close to $\rho_k$ (the confidence interval has small length), then we  can linearize $N_k$ with respect to the estimated correlation $\widehat\rho_{1,k}$ using a first-order Taylor expansion
% 	%
% 	\[
% 	N_k\left(\widehat\rho_{1,k}\right)\approx N_k^*+ \frac{\partial N_k}{\partial \rho_{1,k}} \left( \widehat\rho_{1,k}-\rho_{1,k}\right).
% 	\]
% 	%
% 	The variance of this linearized estimate is approximated by
% 	%
% 	\begin{align*}
% 	\text{Var}\left[N_k\left(\widehat\rho_{1,k}\right)\right] &\approx \left(\frac{\partial N_k}{\partial \rho_{1,k}}\Bigg |_{\rho_{1,k} = \widehat\rho_{1,k}} \right)^2 \cdot \text{Var}\left[\widehat\rho_{1,k}\right] \approx \left(\frac{\partial N_k}{\partial \rho_{1,k}}\Bigg |_{\rho_{1,k} = \widehat\rho_{1,k}} \right)^2 \cdot \left(\frac{\partial \text{tanh}(z_k)}{\partial z_k}\right)^2\text{Var}[z_k],\\
% 	&= \left(\frac{\partial N_k}{\partial \rho_{1,k}}\Bigg |_{\rho_{1,k} = \widehat\rho_{1,k}} \right)^2\left(1-\widehat\rho_{1,k}^2\right)^2\frac{1.03^2}{Q-3}.
% 	\end{align*}
% 	%
% 	This leads to a confidence interval for the linearized sample size estimate
% 	%
% 	\[
% 	\text{CI}_{N_k} = N_k\left(\widehat\rho_{1,k}\right)\pm t_{\alpha/2,Q-3}\sqrt{\text{Var}\left[N_k\left(\widehat\rho_{1,k}\right)\right]}.
% 	\]
% 	%
% 	Moreover, given $\rho_{1,k}\in \text{CI}_{\rho_{1,k}}$, the sample size corresponding to \eqref{eq:delta_rho_square} is \eqref{eq:upper_lower_N_k}. Combining this with the linearized estimate yields the result.
% \end{proof}


