% ========================================
\section{Introduction}\label{sec:intro}
% ========================================
Quantifying uncertainty in computational models is a critical aspect of predictive modeling in many areas of science and engineering. In practical applications, model parameters often involve significant uncertainty arising from measurement error, limited data, or intrinsic variability. Accurately propagating this uncertainty through complex models is critical for risk assessment, decision-making, and model validation. One of the most widely used tools for this purpose is the Monte Carlo (MC) method, which estimates statistical quantities of interest by evaluating the model repeatedly under random samples. The MC method is attractive due to its simplicity, broad applicability, and non-intrusive nature: it requires no modifications to the deterministic solver and makes no assumptions about the regularity. However, its key drawback is the slow convergence rate of $\mathcal{O}(N^{-1/2})$, where $N$ is the number of samples. When each sample involves the solution of a computationally intensive model, such as a nonlinear partial differential equation (PDE) discretized on a fine mesh, the total cost can become prohibitively high. This challenge motivates the use of reduced-cost approximations that preserve sufficient fidelity for statistical inference.

To alleviate this burden, a common strategy is to use surrogate models that approximate the high-fidelity solver at a reduced computational expense. These models may involve coarser spatial discretizations, simplified physics, or data-driven approximations. While surrogate models can significantly accelerate MC sampling, their use introduces a trade-off between cost and accuracy. For instance, \cite{ElLiSa:2022} demonstrates that stochastic collocation surrogates can improve efficiency while maintaining acceptable error bounds, though performance is problem dependent.

Another class of methods designed to balance fidelity and cost is the multilevel Monte Carlo (MLMC) method \cite{BaScZo:2011,Gi:2008,Gi:2015}, which constructs a hierarchy of coarser discretizations to decompose the MC estimator. By exploiting the decay of variance across levels, MLMC allocates most of the samples to cheaper, coarse models, while using expensive fine models only for correction. This multilevel variance reduction significantly lowers the overall computational effort, and its effectiveness has been demonstrated in a range of PDE-based uncertainty quantification problems (e.g., \cite{ElLiSa:2023}).


While MLMC relies on a hierarchy of discretizations, more flexible alternatives have been developed in the form of multi-fidelity Monte Carlo (MFMC) methods \cite{PeWiGu:2016,PeGuWi:2018}. MFMC generalizes the multilevel approach by allowing arbitrary combinations of high- and low-fidelity models, including those with different discretization resolutions, physical approximations, or numerical schemes. The key idea is to construct a control variate estimator that exploits the correlation between models to reduce the overall computational cost required to achieve a specified accuracy. MFMC has demonstrated substantial efficiency gains over classical Monte Carlo, particularly when surrogate models are inexpensive to evaluate and exhibit strong correlations with the high-fidelity model.

A central challenge in the practical implementation of multi-fidelity Monte Carlo methods is the estimation of critical parameters, such as the correlation coefficients between high- and low-fidelity models and the corresponding optimal weights used in the control variate correction of the MFMC estimator. While these parameters are often assumed to be known in theoretical analyses, they must be estimated from sample statistics in practical applications. Poor estimates of parameters can degrade the performance of MFMC, undermine variance reduction, and lead to inefficient resource usage. This paper addresses this gap by presenting an adaptive parameter estimation framework tailored for MFMC. We derive sensitivity bounds and confidence intervals for the correlation coefficient estimator and analyze how parameter uncertainty propagates through the MFMC estimator. Building on this analysis, we develop a sequential pilot sampling algorithm that dynamically identifies the minimum number of samples required to estimate correlations with sufficient accuracy. Our strategy incorporates statistical stopping rules informed by confidence bounds and sensitivity thresholds that ensure both estimator reliability and computational efficiency. Unlike approaches that assume Gaussianity or strong parametric assumptions, our framework adopts a non-parametric asymptotic perspective, enhancing its applicability across a broad range of problems. By explicitly accounting for parameter uncertainty, the proposed methodology strengthens the robustness of MFMC and avoids unnecessary sampling. Numerical experiments confirm that the adaptive strategy achieves accurate statistical estimates while substantially reducing the number of expensive high-fidelity model evaluations required during pilot sampling.



The paper is organized as follows. Section~\ref{sec:MC} reviews the Monte Carlo finite element method. As the MLMC discussion closely follows \cite{ElLiSa:2025}, we do not provide a detailed exposition here. Section~\ref{sec:MFMC} introduces the multifidelity Monte Carlo finite element method. Section~\ref{sec:Parameter_Estimation} presents our adaptive methodology for estimating model correlation coefficients. Section~\ref{sec:Num-Exp} reports numerical experiments that assess the efficiency and accuracy of the proposed approach. Finally, technical details and supporting proofs are provided in the Appendix.

 




% Finally, the paper concludes with Section \ref{sec:Conclusion}, summarizing the key findings and contributions. 
% 


