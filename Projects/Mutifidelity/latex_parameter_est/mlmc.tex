\documentclass[final,3p,times,11pt]{elsarticle}
\usepackage[USenglish]{babel}
\usepackage{amsmath,amssymb,amsthm, mathrsfs,multirow}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{stmaryrd}
\usepackage[dvipsnames]{xcolor}
\usepackage{cancel}
\usepackage{ulem}
\usepackage{tabularx}
\usepackage{comment}
%\usepackage{subcaption}
%\usepackage[show]{ed}
%\usepackage{showkeys}
%\usepackage{showlabels}
%\usepackage[notcite,notref]{showkeys}
%\usepackage{refcheck}
% \usepackage[ruled,vlined]{algorithm2e}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\definecolor{Myblue}{rgb}{.2 0.4 1}

\usepackage{hyperref}
\hypersetup{
    %bookmarks=true,         % show bookmarks bar?
    colorlinks = true,       % false: boxed links; true: colored links
    % linkcolor=green
     %linkcolor=red,          % color of internal links (change box color with linkbordercolor)
     %citecolor=green,        % color of links to bibliography
    %filecolor=magenta,      % color of file links
    %urlcolor=cyan           % color of external links
}
%\usepackage{wrapfig}
%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%\usepackage{lineno}


% ==============   Macros  ====================
\newcommand{\mynabla}{\widetilde{\nabla}} 
\newcommand{\jump}[1]{[\![#1]\!]}
\newcommand{\HEcolor}[1]{{\textcolor{blue}{#1}}}
\newcommand{\TSVcolor}[1]{{\textcolor{orange}{#1}}}
\newcommand{\JLcolor}[1]{{\textcolor{violet}{#1}}} %violet
\newcommand{\Grids}{\boldsymbol{\chi}}

\newtheorem{theorem}{Theorem}%[section]
\newtheorem{VariationalForm}[theorem]{Variational Formulation}
% =============================================

\journal{}
\makeatletter
\def\ps@pprintTitle{%
 \let\@oddhead\@empty
 \let\@evenhead\@empty
 \def\@oddfoot{}%
 \let\@evenfoot\@oddfoot}
\makeatother




\begin{document}
\begin{frontmatter}
\title{Multi-level Monte Carlo}


% \author[umdcs]{Matthias Heinkenschloss}
% \ead{heinken@rice.edu}
% \address[umdcs]{Department of Computational Applied Mathematics \& Operations Research, Rice University.}
% \author[umdm]{Jiaxing Liang}
% \ead{jl508@rice.edu}
% \address[umdm]{Department of Computational Applied Mathematics \& Operations Research, Rice University.}
% \author[UA]{Tonatiuh S\'anchez-Vizuet}
% \ead{tonatiuh@arizona.edu}
% \address[UA]{Department of Mathematics, The University of Arizona.}
\begin{abstract}
% We investigate the Grad-Shafranov free boundary problem in Tokamak fusion reactors under the influence of parameter uncertainties. Using both traditional Monte Carlo and multi-fidelity Monte Carlo sampling approaches, we quantify the impact of these uncertainties on model predictions, emphasizing the statistical characterization of solution variability across diverse parameter regimes. Our numerical results reveals that the multi-fidelity Monte Carlo estimator achieves statistical accuracy comparable to the Monte Carlo approach. However, the multi-fidelity method demonstrates superior computational efficiency, achieving a cost reduction by a factor of ..., while preserving fidelity in representing plasma boundary dynamics and geometric parameters. This work underscores the efficiency of multi-fidelity frameworks in addressing the computational demands of uncertainty quantification in complex fusion reactor models, offering a robust pathway for enhancing predictive capabilities in plasma physics. 
\end{abstract}

\begin{keyword}
% Multi-fidelity Monte Carlo Finite-Element \sep Parametric expectation, \sep Sparse Grid Stochastic Collocation \sep Uncertainty Quantification \sep Free Boundary Grad-Shafranov Problem.
%
\MSC[2020] 
\end{keyword}
\end{frontmatter}

\section{MLMC algorithm}
%
\begin{equation}
\label{eq:MLMC_estimator}
    A_{\text{MLMC}} := \sum_{\ell=0}^L \widehat{Y}_\ell=\frac{1}{N_0}\sum_{i=1}^{N_0}u_0^{(i)} + \sum_{\ell = 1}^L \frac{1}{N_\ell}\sum_{i=1}^{N_\ell}\left(u_\ell^{(i)} - u_{\ell-1}^{(i)}\right).
\end{equation}
%

%
\begin{equation}
\label{eq:MLMC_SampleSize}
		N_{\ell} =  \Biggl\lceil \frac{1}{\theta \epsilon^2} \sqrt{\frac{V_\ell}{C_\ell}}\sum_{k = 0}^{L}\sqrt{V_k C_k}\Biggr\rceil.
\end{equation}
%
%
\begin{equation}\label{eq:MeanVarUpdate_Var}
\mathbb{V}(Y_\ell) = \frac{1}{N_\ell-1}\left(\sum_{i=1}^{N_\ell}\left\Vert Y_\ell^{(i)}\right\Vert_{Z}^2-\frac{1}{N_\ell}\left\Vert\sum_{i=1}^{N_\ell}Y_\ell^{(i)}\right\Vert_{Z}^2\right).
\end{equation}
%
%
\begin{equation}
\label{eq:VarExtrapolate_uniform}
    V_{L+1} = \left(M_{L+1}/M_L\right)^{-b_1}V_{L}. 
\end{equation}
%



\begin{algorithm}[!ht]
\DontPrintSemicolon

    \KwIn{Initial mesh level $L = 0,$ sequence of mesh available $\{\mathcal{T}_\ell\}_{\ell \ge 0}$, root nMSE $\epsilon$, $\zeta\in (0,1)$, initial sample size $\boldsymbol{N}_{\text{old}}=\left [N_\ell\right]_{\ell = 0}^L$, sample size corrections $\boldsymbol{dN}  = \boldsymbol{N}_{\text{old}}$.}
    \KwOut{$\left[N_\ell\right]_{\ell = 0}^L$, $\left[V_\ell\right]_{\ell = 0}^L$, $A_{\text{MLMC}}$.}
     \While{$\sum_\ell \boldsymbol{dN}>0$}{
    \For{$0\le \ell\le L$}{
    
        \For{$i = 1,\cdots,\boldsymbol{dN} $}
    {
    Solve the free boundary problem on $\mathcal{T}_\ell$ to get $Y_\ell^{(i)}$ for the $i$-th sample.
    }
    }
     Approximate $\left[V_\ell\right]_{\ell=0}^L$ by \eqref{eq:MeanVarUpdate_Var}.
        
     Update the sample size estimation $\left[N_\ell\right]_{\ell = 0}^L$ by \eqref{eq:MLMC_SampleSize}. 
     
    % $j=j+1$.
    
    $\boldsymbol{dN} = \max\left\{0,\;\left[N_\ell\right]_{\ell = 0}^L-\boldsymbol{N}_{\text{old}}\right\}.$
    
    $\boldsymbol{N}_{\text{old}}=\left[N_\ell\right]_{\ell = 0}^L$.
    
    \If{$\sum_\ell \boldsymbol{dN}=0$}{
    
    \If{ The a posteriori error estimator falls below $\sqrt{1-\theta}\epsilon \left\Vert\mathbb{E}(u) \right\Vert_{Z}$, }
    {
    Compute $A_{\text{MLMC}}$ by \eqref{eq:MLMC_estimator} and terminate the loop.
    }
    \Else {
     $L = L+1$.
     
     Approximate $V_L$ by \eqref{eq:VarExtrapolate_uniform} and compute $\left[N_\ell\right]_{\ell = 0}^L$ and go to Step 1.}
    }
    }  
\caption{Multilevel Monte Carlo Finite-Element method}\label{algo:MLMC_Algo_CorrectionVersion}
\end{algorithm}


\newpage
\section{Dynamic strategy for MFMC}

The calculation of correlation coefficients requires the knowledge of variance and covariance. To achieve robust and numerically stable updates for these sample statistics, Welford's algorithm \cite{Welford:1962} is used for both high- and low-fidelity models. The initialization of the proxies for the mean and variance of the high- and low-fidelity models is set to $m_w^{(1)}=0$, $v_w^{(1)}=0$, $\widehat m_w^{(1)}=0$, and $\widehat v_w^{(1)}=0$, respectively.  For the high-fidelity model  $\widehat u_{h,1}=u_h$, the proxies for the sample mean and variance are updated iteratively as new samples are incorporated through the recurrence relations
%
\[
m_w^{(i)} = m_w^{(i-1)} + \frac{u_h\left(\boldsymbol{\omega}^{(i)}\right)-m_w^{(i-1)}}{i},\qquad v_w^{(i)} = v_w^{(i-1)} + \left\langle u_h\left(\boldsymbol{\omega}^{(i)}\right)-m_w^{(i-1)}, \;\;u_h\left(\boldsymbol{\omega}^{(i)}\right)-m_w^{(i)}\right\rangle,
\]
%
Similarly, for the low-fidelity model $\widehat u_{h,k}$, the proxies are updated using analogous recurrence relations
%
\[
\widehat m_w^{(i)} = \widehat m_w^{(i-1)} + \frac{\widehat u_{h,k}\left(\boldsymbol{\omega}^{(i)}\right) - \widehat m_w^{(i-1)}}{i},\qquad \widehat v_w^{(i)} = \widehat v_w^{(i-1)} + \left\langle \widehat u_{h,k}\left(\boldsymbol{\omega}^{(i)}\right)-\widehat m_w^{(i-1)},\;\; \widehat u_{h,k}\left(\boldsymbol{\omega}^{(i)}\right)-\widehat m_w^{(i)}\right\rangle,
\]
%
The correlation between the high- and low-fidelity models is quantified through the covariance, for which the proxy is initialized as $r_w^{(1)}=0$. The iterative update for the covariance is performed using the relation
%
\[
r_w^{(i)} = r_w^{(i-1)} + \left \langle u_{h}\left(\boldsymbol{\omega}^{(i)}\right)-m_{w}^{(i-1)},\;\;\widehat u_{h,k}\left(\boldsymbol{\omega}^{(i)}\right)-\widehat m_{w}^{(i)}\right\rangle,
\]
%
These iterative updates ensure that the statistical parameters are dynamically adjusted without requiring the storage of all previous samples, thus maintaining computational efficiency. Using these updated proxies, the sample standard deviations of the high- and low-fidelity models are calculated as $\sigma^{(i)} = \sqrt{v_w^{(i)}/(i-1)}$ and $\widehat \sigma^{(i)} = \sqrt{\widehat v_w^{(i)}/(i-1)}$ respectively. The sample covariance between the high-fidelity model and each low-fidelity model is determined as $\text{Cov}^{(i)} = r_w^{(i)}/(i-1)$.

\begin{algorithm}[!ht]
\DontPrintSemicolon

    \KwIn{Tolerance $\delta$, number of low-fidelity model $K$, initial sample size $N_0$, sample size correction $dN = N_0$. Initializations for Welford's algorithm: mean of high- and low-fidelity models $m_w^{(0)} = 0, \widehat m_w^{(0)} = 0$, proxy of variance of high- and low-fidelity models $v_w^{(0)}=0, \widehat v_w^{(0)}=0$, and proxy of covariance $r_w^{(0)}=0$.}
    \KwOut{Sample size $N$, estimated parameters $\sigma_1,\sigma_k, \rho_{1,k}$.}

    $p=0$

    $\text{AddSample = True}$
    
    \While{AddSample = True}{
    \For{$k=2,\ldots, K$}{
    
        \For{$i = 1,\cdots, dN $}
    {
    $j=p+i$
    
    For the $j-$the update, estimate sample mean $m_w^{(j)}, \widehat m_w^{(j)}$, standard deviation $\sigma^{(j)}, \widehat \sigma^{(j)}$, covariance $\text{Cov}^{(j)}$ and correlated coefficients $\rho_{1,k}^{(j)}$ by Welford's algorithm.
    }
    }
    \If{$\left|\frac{\rho_{1,k}^{(j)}-\rho_{1,k}^{(j-1)}}{\rho_{1,k}^{(j)}}\right|<\delta$ for all $k=2,\ldots, K$}
    % $\&$ $\left|\frac{\sigma_{k}^{(j)}-\sigma_{k}^{(j-1)}}{\sigma_{k}^{(j)}}\right|<\delta$ $\&$ $\left|\frac{\widehat \sigma_{k}^{(j)}-\widehat \sigma_{k}^{(j-1)}}{\widehat \sigma_{k}^{(j)}}\right|<\delta$ for all $k=2,\ldots, K$}
    {
    \text{AddSample = False}
    
    }
    \Else {
    \text{AddSample = True}
    
     $p=p+dN$}
    }
    $N=j$, $\sigma_1 = \sigma_w^{(j)}$, $\sigma_k = \widehat\sigma_w^{(j)}$, $\rho_{1,k} = \rho_{1,k}^{(j)}$.
\caption{Dynamic strategy to estimate parameters}\label{algo:MLMC_Algo_CorrectionVersion}
\end{algorithm}

Note that in this offline procedure for generating correlated parameters of MFMC, I use different batches of samples to estimate parameters such as $\rho_{1,k}$ for different 
$k$. Specifically, if there are $K$ low-fidelity models and estimating each $\rho_{1,k}$ requires $N$ samples, the total number of high-fidelity evaluations needed is $KN$. The reason for using separate sample groups is that sharing samples across different $\rho_{1,k}$ estimates could introduce additional dependencies between them.

According to \cite{PeWiGu:2016}, page A3174, in the case of a two low-fidelity model example, the sample size, weighted coefficients $\alpha_k$, and cost vary smoothly with respect to the correlation coefficient $\rho_{1,k}$ and the cost ratio. The choice of $\delta$ depends on the magnitude of the correlation coefficients and the computational resources available. By adjusting $\delta$, we can estimate the required sample size.  When the correlation coefficients are not very close to each other, $\delta$ can be set relatively large, reducing the number of samples needed. Conversely, if the correlation coefficients are very similar—indicating that almost all low-fidelity models perform comparably well—$\delta$ can again be chosen large. In this case, the correlated coefficients will be nearly identical up to a certain number of digits. As a result, during the model selection process in MFMC, low-fidelity models with relatively high accuracy will be discarded, while those with relatively lower accuracy will be selected, which aligns with our objective. 


% On the other hand, if a small sample size is used for parameter estimation, the varinace

% Once these parameters are estimated in the offline procedure, they will be used in the online process and kept fixed without any update. The reason is that each update of the correlated coefficient $\rho_{1,k}$ requires realizations of the high and the k-th low fidelity models. In this case, updating of parameters in the online process incurs extra cost related with the high-fidelity model. For the online sampling, if all samples are reused from the offline sampling for parameter estimation, since the variance and correlated coefficients are estimated using the same set of samples, the parameters depend on the samples, even though the variance is estimated using unbiased formula, the reuse of samples make the variance of MFMC estimator 

\bibliographystyle{abbrv}
% \bibliographystyle{alphaurl}
\bibliography{references_liang}
\end{document}


