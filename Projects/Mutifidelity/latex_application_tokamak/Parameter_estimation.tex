\subsection{Pilot sample size for parameter estimation}\label{sec:Parameter_Estimation}

Accurate estimation of correlation coefficients is a central challenge in multi-fidelity Monte Carlo, as the efficiency of the estimator is highly sensitive to these values. While variance estimation converges at the canonical Monte Carlo rate $\mathcal{O}(1/\sqrt{Q})$ with unbiased estimators, where $Q$ is the number of pilot samples, correlation estimation is significantly more delicate. The sample Pearson correlation coefficient,
%
\[
\widehat{\rho}_{1,k} = \frac{\sum_{i=1}^Q\left\langle u_{1}^{(i)} - A_{1,Q}^{\text{MC}},\; u_{k}^{(i)} - A_{k,Q}^{\text{MC}} \right\rangle_U}{\sqrt{\sum_{i=1}^Q \left\|u_{1}^{(i)} - A_{1,Q}^{\text{MC}}\right\|_U^2} \sqrt{\sum_{i=1}^Q \left\|u_{k}^{(i)} - A_{k,Q}^{\text{MC}}\right\|_U^2}},
\]
%
where $u_k^{(i)} := u_{h,k}(\cdot, \boldsymbol{\omega}^{(i)})$ denotes the $i$-th realization of the $k$-th model, is a non-linear ratio estimator that is statistically biased. Even under the idealized assumption of bivariate normality, it exhibits an asymptotic bias of $\frac{\rho_{1,k}^3 - \rho_{1,k}}{2Q}$ and variance decaying as $\frac{(1 - \rho_{1,k}^2)^2}{Q}$ \cite{Fi:1915, Ha:2007, Ri:1932}, with pronounced skewness as $|\rho_{1,k}| \to 1$. These pathologies worsen in the high-correlation regime most beneficial for variance reduction, requiring substantially larger pilot samples to ensure reliable estimates. As a result, correlation estimation, rather than variance estimation, often becomes the bottleneck in MFMC parameter estimation.




Insufficient pilot sampling introduces two sources of error that degrade MFMC performance. First, biased correlation estimates compromise model selection, potentially excluding low-cost, high-correlation surrogates or overestimating the utility of weak models. This leads to suboptimal sample allocations that deviate from the theoretical optimum, reducing estimator efficiency. Second, the variability in correlation estimates propagates non-linearly through the efficiency ratio $\xi$. The perturbation $\Delta\xi = \xi(\widehat{\boldsymbol{\rho}}) - \xi(\boldsymbol{\rho})$ is controlled by the gradient $\nabla_{\boldsymbol{\rho}} \xi$, whose components
%
\[
\frac{\partial \xi}{\partial \rho_{1,k}} = \frac{2\rho_{1,k}}{C_1}\left(\sqrt{\frac{C_k}{\Delta_k }} - \sqrt{\frac{C_{k-1}}{\Delta_{k-1} }}\right)\cdot \sum_{k=1}^K\sqrt{C_k\Delta_k },
\]
%
exhibit large magnitude when any $\Delta_k \to 0$. In such cases, small deviations $\Delta \boldsymbol{\rho}$ can induce disproportionately large changes in $\xi$. As a result, the distribution of $\widehat{\xi}$ becomes heavy-tailed, especially when correlations approach $\pm 1$, leading to the violations of the variance guarantee $\mathbb{V}[A^{\mathrm{MF}}] \leq \epsilon_{\mathrm{tar}}^2$. These errors can compound across models, particularly when several models exhibit similar correlations, resulting in inefficiencies for sampling and accuracy for the estimator.




To mitigate these effects, we establish a pilot sampling framework anchored in three criteria: (i) constraint of error propagation in the efficiency ratio $\xi$ through gradient-based sensitivity analysis; and (ii) stability of the selected model set under sampling variability, which will be addressed in Section~\ref{sec:Model_Selection}; (iii) construction of statistically confidence intervals for each correlation coefficient $\rho_{1,k}$ for validation.


We begin by constructing confidence intervals for $\rho_{1,k}$. When the model outputs $u_{h,1}(\cdot, \boldsymbol{\omega})$ and $u_{h,k}(\cdot, \boldsymbol{\omega})$ are bivariate normal, the Fisher $z$-transformation $\widehat z_k = \tanh^{-1}(\widehat \rho_{1,k})$ yields a variance-stabilized estimator with standard deviation $\sigma_{\widehat z_k} = 1/\sqrt{Q - 3}$ \cite{BiHi:2017,BoWr:1998, FiHaPe:1957,Fi:1915, Fi:1921}. The resulting $(1 - \alpha)$ confidence interval for $\rho_{1,k}$ is given by
%
\begin{align}
    \label{eq:Confidence_Interval_fisher}
    \text{CI}_{\rho_{1,k}}^{\text{fisher}} &= \text{tanh}\left(\widehat z_k \pm  z_{\alpha/2}\sigma_{\widehat z_k}\right)
    =\left[1-\frac{2}{R_k e^{-2z_{\alpha/2}\sigma_{\widehat z_k}}+1}, 1-\frac{2}{R_k e^{2z_{\alpha/2}\sigma_{\widehat z_k}}+1}\right] := [a_k,b_k].
    % = \left[\frac{e^{2(z_k - 1.96\sigma_{z_k})}-1}{e^{2(z_k - 1.96\sigma_{z_k})}+1},\; \frac{e^{2(z_k + 1.96\sigma_{z_k})}-1}{e^{2(z_k + 1.96\sigma_{z_k})}+1}\right].
\end{align}
%
where $z_{\alpha/2}$ is the standard normal quantile, $R_k = (1+\widehat\rho_{1,k})/(1-\widehat\rho_{1,k})$, and the interval width $t_k = b_k - a_k$ satisfies $\mathbb{P}(|\rho_{1,k} - \widehat{\rho}_{1,k}| \leq t_k/2) \geq 1 - \alpha$. This method provides analytically tractable bounds under Gaussian assumptions, with variance stabilization ensuring well-behaved coverage even for moderate sample sizes.

\noindent \textbf{Nonparametric confidence intervals via BCa bootstrap.}
When joint normality is violated, we estimate confidence intervals for correlation coefficients using the bias-corrected and accelerated (BCa) bootstrap method~\cite{Ef:1987}, which avoids parametric assumptions and improves finite-sample accuracy by correcting both median bias and skewness in the bootstrap distribution. Given pilot data of size $Q$, for each $k$-th low fidelity model, the BCa procedure executes as follows:
%
\begin{enumerate}
    \item Generate $B$ independent bootstrap samples by sampling $Q$ observations with replacement from the original dataset.
    
    \item For each bootstrap resample, compute the correlation coefficient $\widehat{\rho}_{1,k}^{(b)}$.
    
    \item Calculate the bias-correction parameter $\widehat{z}_0 = \Phi^{-1}\left( B^{-1} \sum_{b=1}^B \mathbb{I}(\widehat{\rho}_{1,k}^{(b)} < \widehat{\rho}_{1,k}) \right)$ where $\Phi^{-1}$ is the standard normal quantile function, quantifying median bias via the proportion of bootstrap estimates below the original statistic.
    
    \item Compute the acceleration factor $\widehat{a} = \frac{1}{6} \frac{\sum_{j=1}^Q (U_\bullet - U_j)^3}{\left[ \sum_{j=1}^Q (U_\bullet - U_j)^2 \right]^{3/2}}$ where $U_j$ is the jackknife estimate of $\rho_{1,k}$ with observation $j$ removed, and $U_\bullet$ their mean. This captures the correlation coefficient's influence function skewness.
    
    \item Construct the $(1-\alpha)$ BCa confidence interval as
    %
    \begin{equation}\label{eq:Confidence_Interval_bootstrap}
        \text{CI}_{\rho_{1,k}}^{\text{BCa}} = \left[ \widehat{\rho}_{1,k}^{(\alpha_1)}, \widehat{\rho}_{1,k}^{(\alpha_2)} \right],
    \end{equation}
    %
    where $\widehat{\rho}_{1,k}^{(\gamma)}$ denotes the $\gamma$-quantile of the bootstrap distribution, with adjusted quantiles 
    %
    \begin{equation*}
        \alpha_1 = \Phi\left( \widehat{z}_0 + \frac{\widehat{z}_0 + z_{\alpha/2}}{1 - \widehat{a}(\widehat{z}_0 + z_{\alpha/2})} \right),\qquad
        \alpha_2 = \Phi\left( \widehat{z}_0 + \frac{\widehat{z}_0 + z_{1-\alpha/2}}{1 - \widehat{a}(\widehat{z}_0 + z_{1-\alpha/2})} \right).
    \end{equation*}
    %
    For a 95\% confidence interval, we use $\alpha = 0.05$, so that $z_{\alpha/2} \approx 1.96$.
\end{enumerate}
%
The BCa method provides two critical advantages: (i) complete elimination of parametric distributional assumptions, accommodating complex dependence structures; and (ii) dual correction for sampling bias via $\widehat{z}_0$ and distributional asymmetry via $\widehat{a}$, ensuring coverage accuracy even under substantial non-Gaussianity. The jackknife-based acceleration factor specifically accounts for the correlation coefficient's sensitivity to outliers, while the bias-correction adapts to finite-sample median shifts.


From a computational standpoint, bootstrap resampling incurs no additional model evaluations if the pilot samples are retained in memory. The primary cost lies in recomputing correlation coefficients across resamples, which involves inner product evaluations and variance computations for each $\widehat{\rho}_{1,k}^{(b)}$. However, this overhead is modest compared to full model evaluations, making the bootstrap method computationally practical for confidence interval estimation.

To quantitatively link statistical precision to computational efficiency, we bound the relative error in the estimated efficiency ratio using the Cauchy–Schwarz inequality
%
\begin{equation}\label{eq:delta_xi_bound}
    \frac{\left|\Delta \xi\right|}{\xi}\le \underbrace{\frac{1}{\xi}\sqrt{\sum_{k=2}^K \left(\frac{\partial \xi}{\partial \rho_{1,k}}\right)^2}}_{E(\boldsymbol{\rho})} \cdot \sqrt{\sum_{k=2}^K\left(\Delta\rho_{1,k}\right)^2}
    % =\underbrace{\frac{2}{S}\sqrt{\sum_{k=2}^K(S^\prime)^2} }_{E(\boldsymbol{\rho})}\cdot \sqrt{\sum_{k=2}^K\left(\Delta\rho_{1,k}\right)^2}
    \le \frac{E}{2} \sqrt{\sum_{k=2}^K t_k^2}\le \frac{E\sqrt{K-1}}{2}t_K,
\end{equation}
%
with probability at least $(1 - \alpha)^K$ assuming independent estimators and using either Fisher or bootstrap-based interval widths $t_k$. This inequality defines a worst-case bound on the efficiency perturbation $\Delta \xi$ in terms of the sensitivity factor $E(\boldsymbol{\rho})$ and the correlation uncertainty $t_k$. 
% Enforcing a uniform bound $t_k \leq \delta$ ensures $\frac{|\Delta \xi|}{\xi} \leq E \sqrt{K - 1} \delta / 2$, yielding a principled criterion for sample size selection. 
Especially for bivariate normal distribution, inverting the Fisher-based confidence width $t_k$ yields a closed-form lower bound on the required sample size
%
\begin{equation*}\label{eq:Pilot_sample_size_estimate}
Q \geq 3 + \left( \frac{2 z_{\alpha/2}}{\log m} \right)^2, \quad m = \frac{(R_k^2 + 1)t_k + \sqrt{(R_k^2 - 1)^2 t_k^2 + 16 R_k^2}}{2 R_k (2 - t_k)}.
\end{equation*}
%



We implement this strategy using a dynamic sampling framework, also known as sequential analysis \cite{La:2001,Wa:1947}, as outlined in Algorithm~\ref{algo:Parameter_Estimation}, to prevent unnecessary sampling. Before running the algorithm, diagnostic checks for bivariate normality are performed to determine whether confidence intervals should be constructed using the Fisher $z$-transformation or bootstrap resampling. Within the algorithm, a minimum of $Q_{\min} = 30$ pilot samples is enforced to ensure the validity of the asymptotic approximations required by both methods. The sample size is then incrementally increased until the maximal confidence width satisfies $\max_k t_k \leq \delta$, while simultaneously monitoring the sensitivity factor $E(\boldsymbol{\rho})$ to avoid unstable perturbations in the efficiency ratio. At each iteration, Welford’s algorithm is used to efficiently update sample means and variances without storing the full history of evaluations. For the bootstrap method, however, sample realizations must be retained to enable resampling. The resulting framework integrates with the enhanced backtracking model selection strategy (Algorithm~\ref{algo:enhanced_mfmc_selection}), enabling robust and efficient MFMC parameter estimation.


%
\normalem
\begin{algorithm}[!ht]
\label{algo:Parameter_Estimation}
\DontPrintSemicolon
\SetAlgoVlined
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
    
\KwIn{Relative error $\vartheta$ in $\xi$, number of models $K_c$, initial sample size $Q_0$, cost vector $\boldsymbol{C} = (C_1, \dots, C_{K_c})$, stability threshold $\tau=3$, number of bootstrap samples $B$ (default: 1000).}
\KwOut{Final sample size $Q$, estimated parameters $\widehat{\boldsymbol{\sigma}},\widehat{\boldsymbol{\alpha}}, \widehat{\boldsymbol{\rho}}$, efficiency $\widehat{\xi}$, model index set $\mathcal{I}$.}
\hrule

Initialization:

\hspace{3mm}\textbullet~ $dQ \gets Q_0$, $p \gets 0$, $\text{converged} \gets \texttt{False}$, $\text{stable}\_\text{count} \gets 0$, $\mathcal{I}^{prev}\gets \emptyset$. %\tcp*{}
    
\hspace{3mm}\textbullet~  Welford's initialization: $m_k^{(0)} \gets 0$, $v_k^{(0)} \gets 0$ for $k=1,\dots,K_c$,  $r_k^{(0)} \gets 0$, for $k=2,\dots,K_c$. %\tcp*{Welford's algorithm}



\While{$\neg \text{converged}$ \& $\text{stable}\_\text{count}< \tau$}{
    
    \For{$k=2,\ldots, K_c$}{
    
        \For{$i = 1,\cdots, dQ $}
    {
    $Q \gets  p+i$.

    Draw $dQ$ new samples $\{\boldsymbol{\omega}^{(i)}\}_{i=1}^{dQ}$, compute sample realization $u_{1}^{(Q)}$ and $u_{k}^{(Q)}$.\\

    Update mean $m_1^{(Q)}$, $m_k^{(Q)}$, proxies $v_1^{(Q)}$, $v_k^{(Q)}$ and $r_{k}^{(Q)}$ via Welford's algorithm.


    }
    
    Compute correlations: $\widehat\rho_{1,k}^{(Q)}\gets \frac{r_k^{(Q)}}{\sqrt{v_1^{(Q)}v_k^{(Q)}}}$ for $k=2,\ldots, K_c$. 

    
    
    % \eIf{bivariate normality holds}{
        
    %         $\widehat{z}_k \gets \tanh^{-1}\left(\widehat{\rho}_{1,k}^{(Q)}\right)$, $\sigma_{\widehat{z}_k} \gets 1/\sqrt{Q - 3}$. \\
    %         Compute confidence interval $\text{CI}_{\rho_{1,k}}^{\text{fisher}}$ and its width $t_k$ via Fisher z-transform in \eqref{eq:Confidence_Interval_fisher}.
        
    % }{
        
    %         Generate $B$ bootstrap replicates of $\widehat{\rho}_{1,k}^{(Q)}$.  \\
    %         Compute  confidence interval $\text{CI}_{\rho_{1,k}}^{\text{boot}}$ and its width $t_k$ via bootstrap in \eqref{eq:Confidence_Interval_bootstrap}.
        
    % }
    }

    $\widehat{\boldsymbol{\rho}}^{(Q)} \gets (1,\widehat{\rho}_{1,2}^{(Q)}, \dots, \widehat{\rho}_{1,K_c}^{(Q)})$.

    % $\delta \gets \frac{2\vartheta}{E\sqrt{K-1}}$
    % [$\text{index},\xi^{(p)}$] = Multi-fidelity Model Selection ($\boldsymbol{\rho}^{(p)},\boldsymbol{C}$).

    % $Q \geq \max\{30, Q_{\max \{\text{idx}^{current}\}} \}$ \text{as in } \eqref{eq:Pilot_sample_size_estimate} and
    $\left[\mathcal{I}^{cur},\widehat{\xi}^{(Q)}\right]$ $\gets$ \textsc{ModelSelectionBacktrack} $\left(\widehat{\boldsymbol{\rho}}^{(Q)},\boldsymbol{C}\right)$.\\

    \eIf{$\frac{\widehat \xi^{(Q)}- \widehat \xi^{(p)}}{\widehat \xi^{(Q)}}\leq \vartheta$, $\mathcal{I}^{prev}=\mathcal{I}^{cur}$ }{ %$\max_k t_k \leq \delta$
            $\text{converged} \gets \texttt{True}$.

            $\text{stable}\_\text{count} \gets  \text{stable}\_\text{count}+1$.
    }
    {
            $\text{stable}\_\text{count} \gets  0$.
    
        $p \gets  p+dQ$. %\tcc*{Increase sample size.}
        
        $\widehat{\xi}^{(p)} \gets  \widehat{\xi}^{(Q)}$.
    }
    

    $\mathcal{I}^{prev} \gets \mathcal{I}^{cur}$.
    }
    
$\widehat{\sigma}_1 \gets \sqrt{v_1^{(Q)}}$, $\widehat{\sigma}_k \gets \sqrt{v_k^{(Q)}}$, $\widehat{\alpha}_k \gets \widehat{\rho}_{1,k}^{(Q)} \widehat{\sigma}_1^{(Q)} / \widehat{\sigma}_k^{(Q)}$, for $k \in \mathcal{I}^{cur}$.

Return $\widehat{\boldsymbol{\sigma}} \gets [\widehat{\sigma}_1,\ldots,\widehat{\sigma}_{K}]$, $\widehat{\boldsymbol{\alpha}} \gets [1,\widehat{\sigma}_2\ldots,\widehat{\sigma}_{K}]$, $\widehat{\boldsymbol{\rho}} \gets \widehat{\boldsymbol{\rho}}^{(Q)}$, $\widehat{\xi} \gets \widehat{\xi}^{(Q)}$, $\mathcal{I} \gets \mathcal{I}^{cur}$.
\caption{Dynamic Parameter Estimation}
\end{algorithm}
\ULforem
%






% $\mathbb{E}[|\widehat{\xi} - \xi|^p]^{1/p} \propto p \cdot (1-\rho^2)^{-1}$（$p>2$）
% Second, variance in correlation estimates introduces non-Gaussian tails in the sampling distribution of the cost efficiency $\xi$, leading to high-probability violations of the theoretical variance reduction bound. These effects compound when multiple correlated models are active, as errors propagate geometrically through the weight allocation system.

% Using these two estimates, we determine the optimal choice of $Q$ by ensuring that the mean square error does not exceed a prescribed threshold $\delta$, we allocate a fraction $\theta_1$ to bias and $1-\theta_1$ to variance. Using the error splitting in \eqref{eq:MSE_rho}, we obtain the required pilot sample size
% applying Chebyshev’s inequality $P(|\mathbb{E}(\rho_{1,k}^{(Q)})-\rho_{1,k}^{(Q)}|\ge \nu)\le \text{Var}(\rho_{1,k}^{(Q)})/\nu^2$ with $\nu = (1-\theta_1)\delta_1$ gives
% %
% \[
% P\left(\left|\mathbb{E}\left(\rho_{1,k}^{(Q)}\right)-\rho_{1,k}^{(Q)}\right|\ge \nu\right)\le \frac{\text{Var}\left(\rho_{1,k}^{(Q)}\right)}{\nu^2}
% \]
% %
% %
% \[
% \frac{(1-\rho_{1,k}^2)^2}{Q\nu^2} = \frac{(1-\rho_{1,k}^2)^2}{(1-\theta_1)^2Q\delta_1^2}\le 1\rightarrow Q\ge \frac{(1-\rho_{1,k}^2)^2}{(1-\theta_1)^2\delta_1^2}.
% \]
% %
% Combining these results, a lower bound on $Q$ can be determined as
%
% \begin{equation}
% \label{eq:Offline_Sample_Size}
%     Q\ge \max_{k} \left(\frac{\left|a_1\right|}{\sqrt{\theta_1\delta} }, \frac{a_2}{(1-\theta_1)\delta}\right).
% \end{equation}
% %
% Note in \eqref{eq:Offline_Sample_Size}, we still need to estimate the true correlation coefficients in order to estimate the lower bound of pilot sample size $Q$. However, the sample statistics also depends on $Q$,  we thus  iteratively update $Q$ until convergence is reached. % However, when sampling with a small sample size that does not rely on assumptions about the underlying data distribution, non-parametric method like  bootstrapping \cite{Wa:2006} and sequential analysis \cite{Wa:1947} provide alternative strategies for estimating $Q$. 







% \begin{align*}
% \frac{\partial  \xi}{\partial  \rho_{1,1}} &=\frac{2\sum_{j=1}^K\sqrt{C_j\left(\rho_{1,j}^2 - \rho_{1,j+1}^2\right)}}{C_1}\frac{C_1\rho_{1,1}}{\sqrt{C_1(\rho_{1,1}^2-\rho_{1,2}^2)  }}\\
%     \frac{\partial  \xi}{\partial  \rho_{1,k}} 
% &=\frac{2SS^\prime}{C_1}, \quad \forall\; k=2,\ldots, K.\\
% \frac{\partial  \mathbb{V}\left(A^{\text{MF}}\right)}{\partial  \rho_{1,k}} 
% &=\sigma_1^2\left[2\rho_{1,k}\left(\frac{1}{N_{k}} - \frac{1}{N_{k-1}}\right)-\left( \frac{\rho_{1,k-1}^2 -\rho_{1,k}^2 }{N_{k-1}^2}\frac{\partial N_{k-1}}{\partial  \rho_{1,k}}+\frac{\rho_{1,k}^2 -\rho_{1,k+1}^2 }{N_k^2}\frac{\partial N_k}{\partial  \rho_{1,k}}\right)\right]\\
% &=\epsilon^2(1-\theta)\left\Vert\mathbb{E}(u) \right\Vert_{Z}^2\frac{S^\prime \left(S-T\right)}{S^2},\\
% S& = \sum_{k=1}^K\sqrt{C_k\Delta_k },\quad
% S^\prime = \frac{\partial  S}{\partial  \rho_{1,k}} = \rho_{1,k}\left(\sqrt{\frac{C_k}{\Delta_k }} - \sqrt{\frac{C_{k-1}}{\Delta_{k-1} }}\right).\\
% T &=  \sqrt{C_{k-1}(\rho_{1,k-1}^2 - \rho_{1,k}^2)} - \sqrt{C_{k}(\rho_{1,k}^2 - \rho_{1,k+1}^2)}\\
% S^{\prime\prime}&= \frac{\partial^2  S}{\partial^2  \rho_{1,k}} = \frac{S^\prime}{\rho_{1,k}} - \rho_{1,k}^2\left(\frac{\sqrt{C_k}}{(\rho_{1,k}^2-\rho_{1,k+1}^2)^{3/2}}+\frac{\sqrt{C_{k-1}}}{(\rho_{1,k-1}^2-\rho_{1,k}^2)^{3/2}}\right).
% \end{align*}







% \begin{align}\label{eq:delta_var_bound}
%     \frac{\left|\Delta \mathbb{V}\left(A^{\text{MF}}\right)\right|}{\mathbb{V}\left(A^{\text{MF}}\right)}&\le
%     % \le \underbrace{\frac{1}{\mathbb{V}\left(A^{\text{MF}}\right)}\sqrt{\sum_{k=2}^K \left(\frac{\partial \mathbb{V}\left(A^{\text{MF}}\right)}{\partial \rho_{1,k}}\right)^2}}_{A_1}\cdot \sqrt{\sum_{k=2}^K\left(\Delta\rho_{1,k}\right)^2}=
%     \underbrace{\frac{1}{S^2}\sqrt{\sum_{k=2}^K\left(S^\prime \left(S-T\right)\right)^2}}_{A_1}\cdot \sqrt{\sum_{k=2}^K\left(\Delta\rho_{1,k}\right)^2}
% \end{align}

 
 
 % Note we dont know the exact value of $E$, since $[(S^\prime/S)^2]^{\prime} = 2S^\prime(SS^{\prime\prime} - (S^\prime)^2)/S^3$ is always positive,  an estimated upper bound $\widehat E$ for $E$ can be estimated as evaluating $E$ at the upper bound $b_k$ of each confidence interval $\text{CI}_{\rho_{1,k}}$. It follows that the relative error in $\xi$ is bounded by $\widehat E \sqrt{K - 1}\delta / 2$.




% \JLcolor{Given $\delta$, we first estimate $C^\prime$, then choose $\delta_2$ as $\delta/C^\prime$, $\delta_1=\delta_2/\sqrt{K-1}$, and select $N$ by \eqref{eq:Offline_Sample_Size} for all $k$.}


% The term $\left(1-\sqrt{\frac{C_{k-1}(\rho_{1,k}^2-\rho_{1,k+1}^2)}{C_k(\rho_{1,k-1}^2-\rho_{1,k}^2)}}\right)$ in $\partial \xi/\partial \rho_{1,k}$ encodes MFMC’s selection criteria, ensuring the derivative’s negativity when models are optimally ordered. This indicates that higher $\rho_{1,k}$ improves low-fidelity models’ variance reduction efficiency, reducing reliance on costly high-fidelity evaluations. This reinforces the idea that high-quality low-fidelity models—those that are more aligned with the high-fidelity results—can significantly lower the reliance on expensive high-fidelity evaluations, making the entire multi-fidelity approach more cost-effective.
















