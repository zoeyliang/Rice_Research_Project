% ====================================================
\section{Iterative sample size estimation for MFMC}\label{sec:Iterative_IntegerValued_Sample_Size}
% ====================================================
To overcome the budget under-utilization inherent in direct rounding strategies, we develop an iterative sample size estimation scheme for MFMC grounded in dynamic programming principles. The approach applies Bellman’s principle of optimality \cite{Be:1957} to decompose the global optimization into a sequence of tractable subproblems. Each subproblem preserves the analytical structure of the continuous optimum and maintains the essential variance–cost trade-offs of the original formulation. The resulting recursion yields an integer-compatible allocation strategy that achieves better variance reduction in trade of budget under-utilization.




% The mathematical foundation for this decomposition rests on the separable structure of the MFMC optimization problem. The objective function $\sum_{k=1}^K \Delta_k/N_k$ exhibits complete separability across fidelity levels, while the budget constraint $\sum_{k=1}^K C_kN_k = p$ admits additive decomposition. This structure ensures the problem possesses optimal substructure: any optimal sequence of decisions must contain within it optimal solutions to all subsequent subproblems. The state variable $R_k = p - \sum_{j=1}^{k-1} C_j H_j$ captures the essential information from previous allocations, satisfying the Markov property that future optimization depends only on the current remaining budget. At each stage $k$, the subproblem over levels $k$ through $K$ with residual budget $R_k$ preserves the original problem's structure while maintaining all monotonicity and budget constraints.

\subsection{Theoretical Justification for Dynamic Programming Decomposition}

% The decomposition of the MFMC optimization problem \eqref{eq:Optimization_pb_sample_size_reduced} into a sequence of subproblems is justified by its intrinsic separability and the optimal substructure of the objective function. The objective $\sum_{k=1}^K \Delta_k/N_k$ is completely separable across fidelity levels, while the budget constraint $\sum_{k=1}^K C_kN_k=p$ admits additive decomposition. This structure ensures that any optimal sequence of decisions contains within it optimal solutions to all subsequent subproblems—a hallmark of dynamic programming problems.

% Bellman’s principle of optimality formalizes this intuition: if $\{N_1^*,N_2^*,\ldots,N_K^*\}$ solves the global optimization, then for each $k$, the truncated sequence $\{N_k^*,N_{k+1}^*,\ldots,N_K^*\}$ must solve the corresponding subproblem defined over the remaining fidelity levels with residual budget 
% \[
% R_k = p - \sum_{j=1}^{k-1}C_jN_j^*.
% \]
% The residual budget $R_k$ serves as a sufficient state variable, encapsulating all necessary information from prior allocations. The Markov property follows: future decisions depend only on $R_k$ and not on the detailed allocation history.


% At stage $k$, the subproblem governing levels $k$ through $K$ with remaining budget $R_k$ retains the structure of the original optimization:
% \begin{equation}\label{eq:Sequential_Optimization}
% \begin{array}{ll}
% \displaystyle \min_{H_k,\ldots,H_K\in \mathbb{R}} &
% \displaystyle \sum_{j=k}^K \frac{\Delta_j}{H_j}, \\[2mm]
% \text{subject to} &
% \displaystyle \sum_{j=k}^K C_jH_j = R_k,\\[1mm]
% &H_k \ge 0,\quad H_{j-1}-H_j \le 0,\; j=k+1,\ldots,K.
% \end{array}
% \end{equation}
% The recursive budget update enforces the global cost constraint, while the local monotonicity constraints preserve the fidelity hierarchy, ensuring consistency with the MFMC structure.


% ===========================================

The decomposition of the original optimization problem \eqref{eq:Optimization_sample_size_m_relaxed} into sequential subproblems is justified by the problem's mathematical structure and fundamental optimization principles:


\begin{enumerate}

    \item \textbf{Optimal Substructure and Separability}: The objective function $\sum_{k=1}^K \frac{\Delta_k}{N_k}$ exhibits complete separability across fidelity levels, while the budget constraint $\sum_{k=1}^K C_kN_k = p$ admits additive decomposition. This separable structure ensures that the optimal solution possesses the key property that any subsequence of decisions must be optimal for the corresponding subproblem with the remaining budget.

    \item \textbf{Bellman's Principle of Optimality}: If $\{N_1^*, N_2^*, \ldots, N_K^*\}$ constitutes an optimal solution to the global problem, then for any $k \in \{1,\ldots,K\}$, the truncated sequence $\{N_k^*, N_{k+1}^*, \ldots, N_K^*\}$ must be an optimal solution to the subproblem defined on levels $k$ through $K$ with residual budget $R_k = p - \sum_{j=1}^{k-1} C_j N_j^*$. This principle follows directly from the contradiction that would arise if a superior partial solution existed for any subproblem.

    \item \textbf{State Variable Sufficiency}: The recursive definition of the state variable $R_k$ 
    \[
    R_1 := p, \qquad R_k := p - \sum_{j=1}^{k-1} C_j H_j, \quad k=2,\ldots,K.
    \]
    as the remaining computational budget captures all essential information from previous decisions. The Markovian property holds because future optimization depends only on the current remaining budget $R_k$ and not on the specific history of allocations that led to this state.


    \item \textbf{Sequential Decision Framework}: At each stage $k$, given the fixed decisions $\{H_1, \ldots, H_{k-1}\}$ and current state $R_k$, the optimization over remaining levels $\{H_k, \ldots, H_K\}$ constitutes a well-defined subproblem that preserves the original problem's structure:
    \begin{equation}\label{eq:Sequential_Optimization}
        \begin{array}{ll}
        \displaystyle 
        \min_{H_k,\ldots,H_K\in \mathbb{R}} & 
            \displaystyle 
            \sum_{j=k}^K \frac{\Delta_j}{H_{j}}, \\
        \text{subject to} &
            \displaystyle \sum_{j=k}^K C_j H_j = R_k,\\
            &H_k\ge 0,\quad H_{j-1}-H_j\le 0,\;\; j=k+1,\ldots,K.
        \end{array}
    \end{equation}

    \item \textbf{Constraint Preservation}: The sequential formulation inherently maintains all original constraints. The budget constraint is preserved through the recursive budget update, while the monotonicity constraints $H_{j-1} \leq H_j$ are enforced locally at each stage, ensuring global consistency with the MFMC hierarchy requirements.
\end{enumerate}



\subsection{Convexity and Global Optimality}
The convexity of each subproblem in \eqref{eq:Sequential_Optimization} guarantees global optimality throughout the decomposition. Since $\Delta_j > 0$ and $H_j \ge 0$, each subproblem's objective $\sum_{j=k}^K \Delta_j/H_j$ is convex, and the feasible set defined by the linear constraints forms a convex polyhedron. This convexity ensures that local optimality implies global optimality and prevents the sequential approach from converging to suboptimal solutions. 

Applying the optimality conditions from Theorem~\ref{thm:Sample_size_real} to each subproblem yields the recursive solution
%
\begin{equation*}
    H_k^* = \sqrt{\frac{\Delta_k}{C_k}} \frac{R_k}{\sum_{j=k}^K\sqrt{C_j\Delta_j}},
    \qquad 
    R_{k+1} = R_k - C_k H_k^*,
\end{equation*}
%
which unfolds into the explicit forward recursion
%
\begin{equation}\label{eq:MFMC_New_RealValued_Sample_Size}
    H_1^* = \sqrt{\frac{\Delta_1}{C_1}} \frac{p}{\sum_{j=1}^K\sqrt{C_j\Delta_j}}, 
    \qquad 
    H_k^* = \sqrt{\frac{\Delta_k}{C_k}} \frac{p-\sum_{j=1}^{k-1}C_jH_j^*}{\sum_{j=k}^K\sqrt{C_j\Delta_j}}, 
    \quad k = 2,\ldots, K.
\end{equation}
%
This recursive construction maintains feasibility at every step, progressively allocates the budget, and decomposes a high-dimensional constrained optimization into a sequence of one-dimensional problems, while preserving the guarantees of global optimality. It further aligns with the structure of multi-fidelity estimation, where lower-fidelity models receive larger sample allocations due to their lower computational cost.
%
\begin{theorem}[Monotonicity of the iterative formulation]
\label{thm:Monotonicity_H_k}
Under the assumptions of Theorem~\ref{thm:Sample_size_real}, the iterative allocation defined by \eqref{eq:MFMC_New_RealValued_Sample_Size} satisfies $H_k^* > H_{k-1}^*$ for $k=2,\ldots,K$, forming a strictly increasing sequence.
\end{theorem}
%
\begin{proof}
Introducing the aggregate cost variance partial sum from model $k$ up to $K$
%
\begin{equation}\label{eq:aggregate_cost_variance_weight_Sk}
    S_k=\sum_{j=k}^K\sqrt{C_j\Delta_j},
\end{equation}
%
then 
\begin{equation}\label{eq:S_k_H_k}
    S_{k-1}=\sqrt{C_{k-1}\Delta_{k-1}}+S_k, \qquad H_k^* = \sqrt{\frac{\Delta_k}{C_k}}\frac{R_k}{S_k},
\end{equation}
%
and
\begin{equation*}
R_k = R_{k-1} - C_{k-1}H_{k-1}^* = R_{k-1}\left(1 - \frac{\sqrt{C_{k-1}\Delta_{k-1}}}{S_{k-1}}\right) = R_{k-1}\frac{S_k}{S_{k-1}}.
\end{equation*}
It follows that 
%
\begin{equation}\label{eq:R_kS_k}
    \frac{R_k}{S_k} = \frac{R_{k-1}}{S_{k-1}},
\end{equation}
%
and hence
\[
H_k^*=\sqrt{\frac{\Delta_k}{C_k}}\frac{R_k}{S_k}
> \sqrt{\frac{\Delta_{k-1}}{C_{k-1}}}\frac{R_{k-1}}{S_{k-1}}
=H_{k-1}^*,
\]
where the inequality follows from assumption (ii) of Theorem~\ref{thm:Sample_size_real}.
\end{proof}
%

% Importantly, this monotonicity property is enforced implicitly. While the global problem includes explicit constraints $H_{j-1}\le H_j$, the sequential formulation maintains them automatically through the budget recursion. Thus, the constraint $H_k\ge0$ suffices, and the monotonicity follows directly from the problem structure, simplifying computation without sacrificing theoretical rigor.


The absence of explicit monotonicity constraints between $H_k$ and previously determined allocations $H_1, \ldots, H_{k-1}$ in formulation \eqref{eq:Sequential_Optimization} is mathematically justified by the problem structure. While the global optimization requires $H_{j-1} \le H_j$ for all $j=k+1,\ldots,K$, the sequential decomposition naturally preserves this property through the budget propagation mechanism. The recursive relationship $R_k = p - \sum_{j=1}^{k-1} C_j H_j$ ensures that the residual budget decreases monotonically, while the allocation formula $H_k^* = \sqrt{\Delta_k/C_k} \cdot R_k / \sum_{j=k}^K \sqrt{C_j\Delta_j}$ maintains the relative weighting $R_k / \sum_{j=k}^K \sqrt{C_j\Delta_j}$ across fidelity levels. Crucially, under the assumptions of Theorem~\ref{thm:Sample_size_real}, the optimal solution automatically satisfies $H_k^* > H_{k-1}^*$ without explicit enforcement, as established in Theorem~\ref{thm:Monotonicity_H_k}. Consequently, the constraint $H_k \ge 0$ suffices in the sequential formulation, as the more restrictive monotonicity conditions are inherently satisfied by the optimal solution. This structural insight significantly simplifies the recursive optimization while preserving all essential properties of the MFMC framework.

\subsection{Equivalence with Continuous MFMC Solution}

The following theorem establishes the fundamental equivalence between the iterative formulation and the standard MFMC allocation, ensuring that the sequential approach preserves all optimality properties of the original continuous solution.

%
\begin{theorem}[Equivalence of iterative and standard MFMC formulations]\label{thm:MFMC_Iterative_RealValued_Sample_Size}
Under the assumptions of Theorem \ref{thm:Sample_size_real},
The iteratively defined sample sizes $H_k^*$ in \eqref{eq:MFMC_New_RealValued_Sample_Size} are identical to the continuous MFMC solution $N_k^*$ in \eqref{eq:MFMC_RealValued_Sample_Size}, i.e.,
\[
H_k^* = N_k^*
    = \sqrt{\frac{\Delta_k}{C_k}}\,
      \frac{p}{\sum_{j=1}^K \sqrt{C_j\Delta_j}}.
\]
Moreover, the iterative scheme preserves both the total computational budget and the optimal variance
\[
\sum_{k=1}^K C_k H_k^* = p, 
\qquad  
f(H_k^*) = \sum_{k=1}^K \frac{\Delta_k}{H_k^*} = \frac{1}{p} \left(\sum_{k=1}^K \sqrt{C_k\Delta_k}\right)^2.
\]
\end{theorem}
%


\begin{proof}
% Define the cumulative cost and remaining budget by
% \[
%     T_k = \sum_{j=1}^k C_j H_j^*, 
%     \qquad 
%     R_k = p - T_k,
% \]
% so that $R_0 = p$ and $T_0 = 0$. 

Let $S_k$ be the aggregate cost–variance weight defined in \eqref{eq:aggregate_cost_variance_weight_Sk}. 
% For $k=1$, from \eqref{eq:MFMC_New_RealValued_Sample_Size} we obtain
% %
% \[
%     T_1 = C_1H_1^* 
%     = p\,\frac{\sqrt{C_1\Delta_1}}{S_1},
%     \qquad
%     R_1 = p - T_1
%     = p\,\frac{S_2}{S_1}.
% \]
%
% For general $k\ge 1$, the iterative definition \eqref{eq:MFMC_New_RealValued_Sample_Size} gives
% \[
%     H_k^*
%     = \sqrt{\frac{\Delta_k}{C_k}}\,
%       \frac{R_{k-1}}{S_k},
% \]
% which implies that the remaining budget satisfies
% \[
%     R_k 
%     = R_{k-1} - C_k H_k^*
%     = \frac{S_{k+1}}
%            {S_k} \, R_{k-1}.
% \]
Hence, $\{R_k\}_{k=1}^K$ forms a geometric sequence due to \eqref{eq:R_kS_k}. We obtain by recursion that
\[
    R_k = \frac{p}{S_1}S_k.
\]
Substituting this expression into the formula for $H_k^*$ in \eqref{eq:S_k_H_k} yields
\[
    H_k^*
    = \frac{p}{S_1}\sqrt{\frac{\Delta_k}{C_k}},
\]
which coincides with the closed-form MFMC sample size in \eqref{eq:MFMC_RealValued_Sample_Size}.  
% In particular, for $k=K$, we have $R_K=0$, and thus $T_K = p - R_K = p$, verifying that the total cost constraint $\sum_{k=1}^K C_k H_k^* = p$ is satisfied.  
% Finally,
% \[
%     f(H_k^*)
%     = \sum_{k=1}^K \frac{\Delta_k}{H_k^*}
%     = \frac{1}{p} \left(\sum_{k=1}^K \sqrt{C_k\Delta_k}\right)^2,
% \]
% which completes the proof.
\end{proof}


\subsection{Integer-valued allocation and cost bounds}

For practical implementation, we extend the recursive formulation to {\it integer-valued} sample sizes. The integer iteration preserves the analytical structure while enforcing feasibility through forward recursion
%
\begin{equation}\label{eq:MFMC_New_IntegerValued_Sample_Size}
    M_1^* = \sqrt{\frac{\Delta_1}{C_1}}\frac{p}{\sum_{j=1}^K\sqrt{C_j\Delta_j}}, 
    \qquad 
    M_k^* = \sqrt{\frac{\Delta_k}{C_k}}\frac{p-\sum_{j=1}^{k-1}C_j\left\lfloor M_j^* \right\rfloor}{\sum_{j=k}^K\sqrt{C_j\Delta_j}}, 
    \quad k = 2,\ldots, K,
\end{equation}
%
The procedure begins with the continuous allocation $M_1^*$, from which $\lfloor M_1^* \rfloor$ is derived. The cost of this integer allocation is deducted from the total budget, and the process repeats for subsequent levels using the updated residual budget. The resulting integer allocations $\lfloor M_k^*\rfloor$ adapt to the residual budget after accounting for the integer costs of previous levels, guaranteeing that the total cost remains below the budget $\sum_{k=1}^K C_k\lfloor M_k^*\rfloor \le p$. The minimum budget requirement 
%
\begin{equation}\label{eq:p_bound}
    p \ge \sum_{k=1}^K C_k
\end{equation}
%
guarantees each fidelity level receives at least one sample while preserving monotonicity.

The iterative integer scheme achieves superior budget utilization compared to direct flooring, as formalized in the following cost bound theorem.

\begin{theorem}[Cost bound for the iterative integer-valued allocation]
\label{thm:MFMC_New_IntegerValued_Cost}
Let $\lfloor M_k^* \rfloor$ and $\lfloor N_k^* \rfloor$ denote the integer-valued allocations from the iterative and direct rounding schemes, respectively. Under condition \eqref{eq:p_bound}, their total costs satisfy
\begin{equation}\label{eq:Iterative_integer_sample_size_cost_bound}
    \sum_{k=1}^K C_k \left\lfloor N_k^* \right\rfloor
    \;\le\;
    \sum_{k=1}^K C_k \left\lfloor M_k^* \right\rfloor
    \;\le\;
    p.
\end{equation}
\end{theorem}

\begin{proof}
We first show that the total cost of the iterative scheme does not exceed the prescribed budget,
\[
\sum_{k=1}^K C_k \left\lfloor M_k^* \right\rfloor \le p.
\]
Define the cumulative integer cost up to level $k$ as
\[
T_k = \sum_{j=1}^k C_j\left\lfloor M_j^* \right\rfloor.
\]
Since $\lfloor M_j^* \rfloor \le M_j^*$, we claim, and prove by induction, that for each $k = 1, \ldots, K$,
\begin{equation}\label{eq:Tk_bound}
T_k \le \frac{p}{S_1}\sum_{j=1}^k \sqrt{C_j \Delta_j}.
\end{equation}
where $S_k$ is the aggregate cost–variance weight defined in \eqref{eq:aggregate_cost_variance_weight_Sk}. 
Inequality \eqref{eq:Tk_bound} bounds the cumulative integer cost at level $k$ by a proportional share of the total budget, scaled by $S_1$.







The base case $k=1$ follows immediately,
\[
T_1=C_1 \left\lfloor M_1^* \right\rfloor \le C_1M_1^* = \frac{p}{S_1}\sqrt{C_1\Delta_1},
\]
so \eqref{eq:Tk_bound} holds for \(k=1\). Assume \eqref{eq:Tk_bound} holds for \(k-1\). By definition of \(M_k^*\),
%
\[
M_k^* = \sqrt{\frac{\Delta_k}{C_k}}\frac{p - T_{k-1}}{S_k},
\]
%
and hence
%
\[
C_k \left\lfloor M_k^* \right\rfloor \le C_k M_k^*  = \sqrt{C_k\Delta_k}\frac{p-T_{k-1}}{S_k}.
\]
%
Using the inductive hypothesis and simplifying yields
\begin{align*}
    T_k &= T_{k-1}+C_k\left\lfloor M_k^* \right\rfloor \\
    &\le T_{k-1} + \sqrt{C_k\Delta_k}\frac{p-T_{k-1}}{S_k}
    =T_{k-1}\left(1-\frac{\sqrt{C_k\Delta_k}}{S_k}\right) + p\frac{\sqrt{C_k\Delta_k}}{S_k}
    \le \frac{p}{S_1}\sum_{j=1}^{k-1} \sqrt{C_j\Delta_j}\frac{S_{k+1}}{S_k}+p\frac{\sqrt{C_k\Delta_k}}{S_k}\\
    % =\frac{p}{S_1}\cdot \frac{\sum_{j=1}^{k-1} \sqrt{C_j\Delta_j}S_{k+1}+S_1\sqrt{C_k\Delta_k}}{S_k}
    &=\frac{p}{S_1}\cdot \frac{\sum_{j=1}^{k-1} \sqrt{C_j\Delta_j}S_{k+1}+\sqrt{C_k\Delta_k}\left(\sum_{j=1}^{k-1}\sqrt{C_j\Delta_j}+S_k\right)}{S_k}
    %=\frac{p}{S}\frac{\sum_{j=k}^K\sqrt{C_j\Delta_j}\sum_{j=1}^k\sqrt{C_j\Delta_j}}{\sum_{j=k}^K\sqrt{C_j\Delta_j}}
    =\frac{p}{S_1}\sum_{j=1}^k\sqrt{C_j\Delta_j}.
\end{align*}
%
Thus, inequality \eqref{eq:Tk_bound} holds for all $k$. 

In particular, when $k=K$, we obtain
\begin{equation}\label{eq:MFMC_iterative_total_cost}
T_K = \sum_{j=1}^K C_j\left\lfloor M_j^*\right\rfloor \le p,
\end{equation}
confirming that the iterative scheme never exceeds the prescribed computational budget. To establish the lower bound in \eqref{eq:Iterative_integer_sample_size_cost_bound}, we compare the auxiliary sequences $M_k^*$ and $N_k^*$. From \eqref{eq:Tk_bound},
%
\[
M_k^* = \sqrt{\frac{\Delta_k}{C_k}}\frac{p - T_{k-1}}{S_k} \ge \sqrt{\frac{\Delta_k}{C_k}}\frac{p-\frac{p}{S}\sum_{j=1}^{k-1}\sqrt{C_j\Delta_j}}{S_k} = \sqrt{\frac{\Delta_k}{C_k}}\frac{p}{S_1}=N_k^*, \qquad k \ge 1.
\]
% 
Monotonicity of the floor function implies \(\lfloor M_k^*\rfloor\ge\lfloor N_k^*\rfloor\) for every \(k\). Multiplying by \(C_k\) and summing yields the desired lower bound in 
\eqref{eq:Iterative_integer_sample_size_cost_bound}.  
Combining this with \eqref{eq:MFMC_iterative_total_cost} completes the proof of the cost bound.


\medskip
\noindent
\textit{ Equality conditions.}
The upper bound in \eqref{eq:Iterative_integer_sample_size_cost_bound} is attained if and only if no budget remains unused, i.e.,
\[
\sum_{k=1}^K C_k \left(M_k^* - \left\lfloor M_k^*\right\rfloor\right) = 0.
\]
Because each fractional part satisfies $M_k^* - \lfloor M_k^* \rfloor \in [0,1)$, this condition holds precisely when every $M_k^*$ is an integer. Similarly, the lower bound is attained if and only if \(\lfloor M_k^* \rfloor = \lfloor N_k^* \rfloor\) for all \(k\), equivalently when both real-valued allocations \(M_k^*\) and \(N_k^*\) lie in the same integer interval
%
\[
\left\lfloor N_k^*\right\rfloor\le M_k^* < \left\lfloor N_k^*\right\rfloor + 1.
\]
Since \(M_k^* \ge N_k^*\), this occurs precisely when they share the same integer part for every \(k\).

\end{proof}


Theorem \ref{thm:MFMC_New_IntegerValued_Variance} establishes that the iterative integer-valued allocation achieves superior variance performance compared to direct flooring while maintaining proximity to the continuous optimum.

% \JLcolor{The lower bound establishes that the iterative scheme achieves variance no worse than the continuous optimum, while the upper bound guarantees improved performance compared to naive flooring. Together, these results confirm that the dynamic programming formulation preserves the optimal variance–cost structure of the MFMC method while enabling efficient integer-valued implementation.}

% thus providing an improved variance--cost tradeoff under fixed budget constraints.
%
\begin{theorem}[Normalized variance bound for the iterative integer-valued sample allocation]
\label{thm:MFMC_New_IntegerValued_Variance}
Let $\lfloor M_k^* \rfloor$ denote the integer-valued sample sizes obtained from the iterative allocation scheme \eqref{eq:MFMC_New_IntegerValued_Sample_Size}, and let $\lfloor N_k^* \rfloor$ denote those obtained by directly flooring the real-valued optimal allocation \eqref{eq:MFMC_RealValued_Sample_Size}. Under the computational budget constraint \eqref{eq:p_bound} and the standard variance conditions on $\Delta_k$ from Theorem~\ref{thm:Sample_size_real}, the normalized variance satisfies the following bounds
%
\begin{equation}\label{eq:Iterative_Integer_Variance_Bound}
\frac{1}{p}\left(\sum_{k=1}^K \sqrt{C_k \Delta_k}\right)^2
= \sum_{k=1}^K \frac{\Delta_k}{N_k^*}
\;\le\;
\sum_{k=1}^K \frac{\Delta_k}{\left\lfloor M_k^* \right\rfloor}
\;\le\;
\sum_{k=1}^K \frac{\Delta_k}{\left\lfloor N_k^* \right\rfloor}.
\end{equation}
%
\end{theorem}
%




\begin{proof}
We first prove the upper bound in \eqref{eq:Iterative_Integer_Variance_Bound}.  
From Theorem~\ref{thm:MFMC_New_IntegerValued_Cost}, it follows that $\lfloor M_k^* \rfloor \ge \lfloor N_k^* \rfloor$ for all $k$.  
Since $x \mapsto \Delta_k/x$ is strictly decreasing for $x > 0$, it immediately follows that
\[
\sum_{k=1}^K \frac{\Delta_k}{\left\lfloor M_k^* \right\rfloor} 
\le \sum_{k=1}^K \frac{\Delta_k}{\left\lfloor N_k^* \right\rfloor}.
\]

\medskip
\noindent
To establish the lower bound, we apply the Cauchy--Schwarz inequality:
%
\[
\left(\sum_{k=1}^K \sqrt{C_k \Delta_k}\right)^2
\le
\left(\sum_{k=1}^K C_k \left\lfloor M_k^* \right\rfloor\right)
\left(\sum_{k=1}^K \frac{\Delta_k}{\left\lfloor M_k^* \right\rfloor}\right).
\]
%
From the cost bound \eqref{eq:MFMC_iterative_total_cost}, it follows that 
$\sum_{k=1}^K C_k \lfloor M_k^* \rfloor \le p$, and hence
%
\[
\frac{1}{p}\left(\sum_{k=1}^K \sqrt{C_k \Delta_k}\right)^2
\le
\sum_{k=1}^K \frac{\Delta_k}{\left\lfloor M_k^* \right\rfloor}.
\]
%
which proves the lower bound.

\medskip
\noindent
\textit{Equality conditions.} 
Equality in the Cauchy--Schwarz step occurs if and only if there exists a constant $\lambda>0$ such that
\[
\left\lfloor M_k^* \right\rfloor = \lambda \sqrt{\frac{\Delta_k}{C_k}}, \qquad k=1,\ldots,K,
\]
which corresponds to the continuous optimal allocation $M_k^* = N_k^*$. Therefore, equality in \eqref{eq:Iterative_Integer_Variance_Bound} holds if and only if the iterative scheme reproduces the continuous solution exactly, i.e., when all $\lfloor M_k^* \rfloor = N_k^*$ and the total cost equals $p$.
\end{proof}



% ------------------
% Next consider the variance
% \begin{align*}
%     f_{\text{act}}(\overline{N_k})&=\sum_{i=1}^{K}\frac{\Delta_k}{\overline{N_k}}=\sum_{i=1}^{k-1}\frac{\Delta_i}{\overline{N_i}}+\frac{\Delta_k}{\overline{N_k}}+\sum_{i=k+1}^{K}\frac{\Delta_i}{\overline{N_i}}\\
%     &\in \left[\sum_{i=1}^{k-1}\frac{\Delta_i}{\overline{N_i}}+\frac{\Delta_k}{\overline{N_k}}+\sum_{i=k+1}^K\frac{\Delta_{i}}{N_i^*},\; \sum_{i=1}^{k-1}\frac{\Delta_i}{\overline{N_i}}+\frac{\Delta_k}{\overline{N_k}}+\sum_{i=k+1}^K\frac{\Delta_{i}}{N_i^*-1}\right)=[f_1,f_2)\\
%     f_1&=\sum_{i=1}^{k-1}\frac{\Delta_i}{\overline{N_i}}+\frac{\Delta_k}{\overline{N_k}}+\sum_{i=k+1}^K\frac{\Delta_{i}}{N_i^*}=\sum_{i=1}^{k-1}\frac{\Delta_i}{\overline{N_i}}+\frac{\Delta_k}{\overline{N_k}}+\sum_{i=k+1}^K\sqrt{C_i\Delta_i}\frac{\sum_{j=i}^K\sqrt{C_j\Delta_j}}{p-\sum_{j=1}^{i-1}C_j\overline{N_j}}\\
%     f_2&=\sum_{i=1}^{k-1}\frac{\Delta_i}{\overline{N_i}}+\frac{\Delta_k}{\overline{N_k}}+\sum_{i=k+1}^K \ldots\\
% \end{align*}
% \begin{align*}
%     \frac{d f_1}{d \overline{N_k}}&=-\frac{\Delta_k}{\overline{N_k}^2}+C_k\sum_{i=k+1}^K\sqrt{C_i\Delta_i}\frac{\sum_{j=i}^K\sqrt{C_j\Delta_j}}{\left(p-\sum_{j=1}^{i-1}C_j\overline{N_j}\right)^2}\\
%     \frac{d^2 f_1}{d^2 \overline{N_k}}&=\frac{2\Delta_k}{\overline{N_k}^3}+2C_k^2\sum_{i=k+1}^K\sqrt{C_i\Delta_i}\frac{\sum_{j=i}^K\sqrt{C_j\Delta_j}}{\left(p-\sum_{j=1}^{i-1}C_j\overline{N_j}\right)^3}
% \end{align*}
% Note that $\frac{d^2 f_1}{d^2 \overline{N_k}}>0$ whenever $p>\sum_{j=1}^{i-1}C_j\overline{N_j}$. this means $f_1$ is convex in $\overline{N_k}$.