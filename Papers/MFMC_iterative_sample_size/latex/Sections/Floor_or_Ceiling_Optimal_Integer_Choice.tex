% ====================================================
\section{Adaptive strategy}
% ====================================================
The rounding step in the iterative scheme can be strengthened by incorporating a local look-ahead that evaluates both the floor and ceiling choices and measures their effect on the total variance. To motivate the strategy, consider the relaxed optimization problem
\begin{subequations}\label{eq:Adaptive_Optimization}
\begin{align}
    \min \quad & \sigma_1^2 \sum_{k=j}^K \frac{\rho_{1,k}^2 - \rho_{1,k+1}^2}{m_k}, \\
    \text{s.t.} \quad & \sum_{k=j}^K C_k m_k \le b_j, \\
    & m_j \ge m_{j-1},\qquad m_k \ge m_{k-1}, \quad k=j+1,\ldots,K, \label{eq:Adaptive_Optimization_obj_c}\\
    & m_j,\ldots,m_K \in \mathbb{N}.
\end{align}
\end{subequations}
The relaxed problem of \eqref{eq:Adaptive_Optimization} differs from \eqref{eq:Sequential_Optimization} only by the monotonicity constraints \eqref{eq:Adaptive_Optimization_obj_c}. Under assumption \eqref{eq:Sample_size_real_assumptions_b}, the monotonicity enforced in \eqref{eq:Adaptive_Optimization} already holds for the continuous solution of \eqref{eq:Sequential_Optimization}, the real-valued solutions of the relaxed version of \eqref{eq:Sequential_Optimization} and \eqref{eq:Adaptive_Optimization} coincide. However, once integer constraints are imposed and both the floor and ceiling of the continuous minimizer are admissible, monotonicity is no longer automatic, and the requirement \(m_j \ge m_{j-1}\) between subproblem must be imposed explicitly. Solving \eqref{eq:Adaptive_Optimization} with integer variables would therefore require integer programming.

To avoid solving an integer program at each step, we propose an adaptive scheme that combines the actual variance contribution from previously computed integers at high fidelity levels with the continuous relaxation for future uncomputed low fidelity levels. At fidelity level \(k\), suppose the previously computed integers \(\overline m_1,\ldots,\overline m_{k-1} \in \mathbb{Z}_+\) are fixed. Let \(x\) be the real-valued sample size at level \(k\), and define the residual budget $b_k = b - \sum_{j=1}^{k-1} C_j \overline m_j$. Substituting the continuous optimal values for levels \(k+1,\ldots,K\) using \eqref{eq:Sequential_Optimization_mjj} yields the single-variable reduced objective
%
\begin{equation}\label{eq:Objective_adaptive_strategy}
    f(x)=\sum_{j=1}^{k-1}\frac{\Delta_j}{\overline m_j}
    + \frac{\Delta_k}{x}+\sum_{j=k+1}^{K}\frac{\Delta_j}{ m_j^*}
    = \sum_{j=1}^{k-1}\frac{\Delta_j}{\overline m_j}
    + \frac{\Delta_k}{x}
    + \frac{\Big(\sum_{j=k+1}^K \sqrt{C_j\Delta_j}\Big)^2}
    {\,b_k - C_k x\,},
    \qquad 0 < x < b_k/C_k,
\end{equation}
where feasibility requires positivity of the remaining budget for levels \(k+1,\ldots,K\). A direct computation shows that
\[
f''(x)
= \frac{2\Delta_k}{x^3}
+ \frac{2C_k^2\Big(\sum_{j=k+1}^K \sqrt{C_j\Delta_j}\Big)^2}
{(b_k - C_k x)^3}
> 0,
\]
so \(f\) is strictly convex on its domain. Hence the continuous minimizer \(m_k^*\) is unique, and the optimal integer choice must be one of the two nearest integers, \(\lfloor m_k^* \rfloor\) or \(\lceil m_k^* \rceil\). Each rounding step therefore reduces to evaluating only these two candidates.

This yields an efficient \(O(K)\) adaptive procedure: at each level \(k\), we compare the two neighboring integers, select the one yielding the smaller value of $f$ in \eqref{eq:Objective_adaptive_strategy}, update the remaining budget, and recompute the continuous relaxation for the remaining fidelities. Edge cases such as \(m_k^* < 1\) or infeasible rounded values are handled by enforcing feasibility or by discarding degenerate levels. In practice, this look-ahead strategy produces integer allocations closer to the true integer optimum than naive floor rounding, since each local decision accounts for its global impact on the variance. The full scheme is summarized in Algorithm~\ref{alg:improved_rounding}.









\begin{algorithm}[!ht]
\caption{Improved Sequential Rounding for Minimizing Variance}
\label{alg:improved_rounding}
\DontPrintSemicolon

\KwIn{Correlations $\{\rho_{1,k}\}_{k=1}^K$, costs $\{C_k\}_{k=1}^K$, budget $b$.}
\KwOut{Integer sample sizes $\{\overline{m}_k\}_{k=1}^{K_r}$.}
\hrule\vspace{1ex}

\If{$b < \sum_{k=1}^K C_k$}{
    \textbf{return} ``Insufficient budget: requires at least $\sum_{k=1}^K C_k$''.
}

Set $b_1=b$.  
Compute $\Delta_k = \rho_{1,k}^2 - \rho_{1,k+1}^2$ for $k=1,\ldots,K$ with $\rho_{1,K+1}=0$.

{\tt RestartFlag} = {\tt true}.\;

\While{{\tt RestartFlag}}{
{\tt RestartFlag} = {\tt false}.\;

\For{$k = 1$ \KwTo $K$}{

    Compute real-valued proxy
    \[
    m_k^* = \sqrt{\frac{\Delta_k}{C_k}}
        \frac{b_k}{\sum_{j=k}^K \sqrt{C_j \Delta_j}}.
    \]

    For $x \in \{\lfloor m_k^* \rfloor,\ \lceil m_k^* \rceil\}$: Compute $f(x)$ using \eqref{eq:Objective_adaptive_strategy} with  
    fixed $\overline{m}_1,\dots,\overline{m}_{k-1}$ and remaining budget $b_k - C_k x$.\;

    Select $\overline{m}_k = \arg\min_x f(x)$  (break ties by smaller cost).\;

    Update $b_{k+1}= b_k - C_k\,\overline{m}_k$.\;

    % ----- Consistency and nondecreasing checks -----
    \If{$f(\lceil m_k^* \rceil)<f(\lfloor m_k^* \rfloor)\quad$ \& $\quad\max(\lceil m_k^* \rceil,\overline{m}_{k-1})\cdot\sum_{j=k}^K C_j <b-\sum_{j=1}^{k-1}C_j\overline{m}_j$}{
        $\overline{m}_k = \max(\lceil m_k^* \rceil,\, \overline{m}_{k-1})$.\;
    }
    \ElseIf{$\lfloor m_k^* \rfloor<\overline{m}_{k-1}\quad$ \& $\quad\overline{m}_{k-1}\cdot\sum_{j=k}^K C_j <b-\sum_{j=1}^{k-1}C_j\overline{m}_j$}{
        $\overline{m}_k = \overline{m}_{k-1}$.\;
    }
    \Else{
        $\overline{m}_k = \lfloor m_k^* \rfloor$.\;
    }

    % ----- Model screening -----
    \If{$k\ge 2$ and $\lfloor M_{k-1}^* \rfloor = \lfloor M_k^* \rfloor$}{

            Compute merged increments: $\;\;\widetilde\Delta_{k-1} = \rho_{1,k-1}^2 - \rho_{1,k+1}^2,
            \quad
            \widetilde\Delta_{k}   = \rho_{1,k+1}^2 - \rho_{1,k+2}^2.$

            \If{$\widetilde\Delta_{k-1}/C_{k-1} < \widetilde\Delta_k/C_{k+1}$}{
                Remove model $k$; update $\rho$, $C$, and $\Delta$;  \;
                $K \leftarrow K-1$.\;
                {\tt RestartFlag} = {\tt true}; \;
                \textbf{break}.\;
            }
        }
}
}

$K_r = K$.\;
\textbf{return} $\overline{m}_1,\dots,\overline{m}_{K_r}$.
\end{algorithm}










