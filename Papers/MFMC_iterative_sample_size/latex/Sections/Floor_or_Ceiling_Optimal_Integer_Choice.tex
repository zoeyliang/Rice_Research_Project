% ====================================================
\section{Adaptive strategy}
% ====================================================
The rounding step in the iterative scheme can be further improved by incorporating a local look-ahead that evaluates both the floor and ceiling choices and accounts for their effect on the total variance. At fidelity level \(k\), suppose the previously chosen integers \(\overline m_1,\dots,\overline m_{k-1}\in\mathbb Z_+\) are fixed. Let \(x\) denote the real-valued sample size at level \(k\), and define the residual budget $b_k = b - \sum_{j=1}^{k-1} C_j \overline m_j$. Substituting the continuous optimal values for levels \(k+1,\dots,K\) yields the single-variable reduced objective
\begin{equation}\label{eq:Objective_adaptive_strategy}
    f(x)
    = \sum_{j=1}^{k-1}\frac{\Delta_j}{\overline m_j}
    + \frac{\Delta_k}{x}
    + \frac{\Big(\sum_{j=k+1}^K \sqrt{C_j\Delta_j}\Big)^2}
    {\,b_k - C_k x\,},
    \qquad 0 < x < b_k/C_k,
\end{equation}
where feasibility requires positivity of the remaining budget for levels \(k+1,\ldots,K\). A direct computation shows that
\[
f''(x)
= \frac{2\Delta_k}{x^3}
+ \frac{2C_k^2\Big(\sum_{j=k+1}^K \sqrt{C_j\Delta_j}\Big)^2}
{(b_k - C_k x)^3}
> 0,
\]
so \(f\) is strictly convex on its feasible interval. The continuous minimizer \(m_k^*\) is therefore unique, and strict convexity guarantees that the optimal integer choice must be one of the two nearest integers, \(\lfloor m_k^*\rfloor\) or \(\lceil m_k^*\rceil\). Consequently, each rounding step reduces to evaluating only two candidates.

This observation leads to an efficient \(O(K)\) sequential procedure: for each level \(k\), we test the two neighboring integers, fix \(\overline m_k\), update the remaining budget, and recompute the continuous relaxation for the remaining fidelities. Edge cases such as \(m_k^*<1\) or infeasible rounded values are handled by enforcing feasibility or discarding degenerate levels. In practice, this look-ahead strategy yields integer allocations far closer to the true optimum than naive floor rounding, since each local decision incorporates its global effect on the variance. The full procedure is summarized in Algorithm~\ref{alg:improved_rounding}.




% \begin{theorem}
% Let \(f\) be a strictly convex function on an interval of \(\mathbb R\) with unique minimizer \(x^*\).  The minimizer of \(f\) over integers in the feasible interval is attained at one of the two integers nearest to \(x^*\); that is,
% \[
% x^{\mathrm{int}} \in \{\lfloor x^*\rfloor,\;\lceil x^*\rceil\}.
% \]
% \end{theorem}

% \begin{proof}
% Because \(f\) is strictly convex with unique minimizer \(x^*\), \(f\) is strictly decreasing on \((-\infty,x^*]\) and strictly increasing on \([x^*,\infty)\). Let \(a=\lfloor x^*\rfloor\) and \(b=\lceil x^*\rceil\). For any integer \(n\le a-1\) we have \(n<a\le x^*\), hence \(f(n)>f(a)\). Similarly, for any integer \(n\ge b+1\) we have \(n>b\ge x^*\), hence \(f(n)>f(b)\). Therefore the integer minimizer must be either \(a\) or \(b\).
% \end{proof}





\begin{algorithm}[!ht]
\caption{Improved Sequential Rounding for Minimizing Variance}
\label{alg:improved_rounding}
\DontPrintSemicolon

\KwIn{Correlations $\{\rho_{1,k}\}_{k=1}^K$, costs $\{C_k\}_{k=1}^K$, budget $b$.}
\KwOut{Integer sample sizes $\{\overline{m}_k\}_{k=1}^{K_r}$.}
\hrule\vspace{1ex}

\If{$b < \sum_{k=1}^K C_k$}{
    \textbf{return} ``Insufficient budget: requires at least $\sum_{k=1}^K C_k$''.
}

Set $b_1=b$.  
Compute $\Delta_k = \rho_{1,k}^2 - \rho_{1,k+1}^2$ for $k=1,\ldots,K$ with $\rho_{1,K+1}=0$.

{\tt RestartFlag} = {\tt true}.\;

\While{{\tt RestartFlag}}{
{\tt RestartFlag} = {\tt false}.\;

\For{$k = 1$ \KwTo $K$}{

    Compute real-valued proxy
    \[
    m_k^* = \sqrt{\frac{\Delta_k}{C_k}}
        \frac{b_k}{\sum_{j=k}^K \sqrt{C_j \Delta_j}}.
    \]

    For $x \in \{\lfloor m_k^* \rfloor,\ \lceil m_k^* \rceil\}$: Compute $f(x)$ using \eqref{eq:Objective_adaptive_strategy} with  
    fixed $\overline{m}_1,\dots,\overline{m}_{k-1}$ and remaining budget $b_k - C_k x$.\;

    Select $\overline{m}_k = \arg\min_x f(x)$  (break ties by smaller cost).\;

    Update $b_{k+1}= b_k - C_k\,\overline{m}_k$.\;

    % ----- Consistency and nondecreasing checks -----
    \If{$f(\lceil m_k^* \rceil)<f(\lfloor m_k^* \rfloor)\quad$ \& $\quad\max(\lceil m_k^* \rceil,\overline{m}_{k-1})\cdot\sum_{j=k}^K C_j <b-\sum_{j=1}^{k-1}C_j\overline{m}_j$}{
        $\overline{m}_k = \max(\lceil m_k^* \rceil,\, \overline{m}_{k-1})$.\;
    }
    \ElseIf{$\lfloor m_k^* \rfloor<\overline{m}_{k-1}\quad$ \& $\quad\overline{m}_{k-1}\cdot\sum_{j=k}^K C_j <b-\sum_{j=1}^{k-1}C_j\overline{m}_j$}{
        $\overline{m}_k = \overline{m}_{k-1}$.\;
    }
    \Else{
        $\overline{m}_k = \lfloor m_k^* \rfloor$.\;
    }

    % ----- Model screening -----
    \If{$k\ge 2$ and $\lfloor M_{k-1}^* \rfloor = \lfloor M_k^* \rfloor$}{

            Compute merged increments: $\;\;\widetilde\Delta_{k-1} = \rho_{1,k-1}^2 - \rho_{1,k+1}^2,
            \quad
            \widetilde\Delta_{k}   = \rho_{1,k+1}^2 - \rho_{1,k+2}^2.$

            \If{$\widetilde\Delta_{k-1}/C_{k-1} < \widetilde\Delta_k/C_{k+1}$}{
                Remove model $k$; update $\rho$, $C$, and $\Delta$;  \;
                $K \leftarrow K-1$.\;
                {\tt RestartFlag} = {\tt true}; \;
                \textbf{break}.\;
            }
        }
}
}

$K_r = K$.\;
\textbf{return} $\overline{m}_1,\dots,\overline{m}_{K_r}$.
\end{algorithm}










