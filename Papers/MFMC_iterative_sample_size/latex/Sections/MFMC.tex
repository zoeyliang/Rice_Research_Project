%!TEX root = ../main.tex
% ====================================================
\section{Multi-fidelity Monte Carlo}\label{sec:MFMC}
% ====================================================

This section reviews the multi-fidelity Monte Carlo (MFMC) method, following the foundational formulation in \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a}. 

Let $(W, \mathcal{F}, \mathbb{P})$ be a complete probability space, where
$W \subset \real^d$ is the set of outcomes, $\mathcal{F} \subset 2^{W}$ is a
$\sigma$-algebra of events, and $\mathbb{P}: \mathcal{F} \to [0,1]$ is a
probability measure. Given a Hilbert space $({\mathcal U}, \langle \cdot, \cdot \rangle_{{\mathcal U}})$, we define the Bochner space
\begin{align}     \label{eq:BochnerL2}
  L_{\mathbb{P}}^2(W, {\mathcal U}) := \Big\{ u: W \rightarrow {\mathcal U} \;\big|\; & u \mbox{ is strongly measurable and } 
                                                                  \int_W \| u(\omega) \|_{{\mathcal U}}^2 \; \textup{d}\mathbb{P}(\omega) < \infty \; \Big\}.
\end{align}
The space $ L_{\mathbb{P}}^2(W, {\mathcal U})$ is a Hilbert space with inner product 
$ \langle u, v \rangle =  \int_W  \langle  u(\omega) , v(\omega)  \rangle_{{\mathcal U}} \; \textup{d}\mathbb{P}(\omega)$.

The goal is to estimate the expected value $\mathbb{E}[u_1 ]$ of a high fidelity model $u_1 \in   L_{\mathbb{P}}^2(W, {\mathcal U})$.
The  MC estimator draws $N$ independent and identically distributed (i.i.d.) realizations $\omega^{(i)}$
and estimates $\mathbb{E}[u ]$ using
\begin{equation}\label{eq:MC_estimator}
    A^{\text{MC}}_{N} := \frac{1}{N}\sum_{i=1}^{N} u_1 \big(\omega^{(i)} \big).
\end{equation}
The MC estimator is unbiased, $\mathbb{E}\big[ A^{\text{MC}}_N \big] = \mathbb{E}\big[ u_1 \big]$
and has variance $\mathbb{V}[A^{\text{MC}}_{N}] = N^{-1} \mathbb{V}[u_1 ]$, where, as usual,
\begin{equation}   \label{eq:variance_of_u}
           \mathbb{E}\left[u(\cdot,\boldsymbol \omega)\right]=\int_W u(\cdot,\omega) \, \pi(\boldsymbol\omega) \,d\omega
           \quad \mbox{ and } \quad
           \mathbb{V}[u] = \mathbb{E}\Big[  \big\| u - \mathbb{E}[u]  \big\|_{{\mathcal U}}^2 \Big].
  \end{equation}




The MFMC framework uses an ensemble of models with varying computational cost and fidelity 
to construct a variance-reduced estimator for high-fidelity expectation. 
In addition to the HF model $u_1 \in   L_{\mathbb{P}}^2(W, {\mathcal U})$ assume that we have lower-fidelity (LF) models
$\{u_k\}_{k=2}^K \in   L_{\mathbb{P}}^2(W, {\mathcal U})$ that can be sampled with lower computational cost.
The central goal of MFMC is to allocate a fixed computational budget across these models to minimize estimator variance while maintaining unbiasedness.

Given $1 \le m_1 \le m_2 \le \ldots \le m_K$ and i.i.d.\ samples $\omega^{(1)}, \ldots, \omega^{(m_K)}$,
the MFMC estimator $A^{\text{MF}}$ combines an MC estimate of the high-fidelity model with differences 
of MC estimates of the lower-fidelity level models.
The MFMC estimator is defined as
\begin{equation}\label{eq:MFMC_estimator}
    A^{\text{MF}}
     := A^{\text{MC}}_{1,m_1} + \sum_{k=2}^K \alpha_k\left(A^{\text{MC}}_{k,m_k} - A^{\text{MC}}_{k,m_{k-1}} \right),
\end{equation}
where
\begin{equation}\label{eq:MFMC_estimator_MCk}
     A^{\text{MC}}_{k,m} :=  \frac{1}{m} \sum_{i=1}^m   u_k(\omega^{(i)}), \quad m \in \{ m_{k-1}, m_k \},
\end{equation}
and $\alpha_k\in \mathbb{R}$ are weights that will be determined below.
Note that the $m_{k-1}$ evaluations $u_k(\omega^{(i)})$, $i = 1, \ldots,  m_{k-1}$,
used in  $A^{\text{MC}}_{k,m_{k-1}}$ are reused in  the computation of $A^{\text{MC}}_{k,m_k}$.
This reuse introduces statistical dependence between 
$A^{\text{MC}}_{k,m_{k-1}}$ and $A^{\text{MC}}_{k,m_k}$. 
If we denote the MC average over the $m_k - m_{k-1}$ samples $u_k(\omega^{(i)})$ 
not included in $A^{\text{MC}}_{k,m_{k-1}}$ by
\[
     A^{\text{MC}}_{k,m_k \backslash m_{k-1}}
      =  \frac{1}{m_k-m_{k-1}}  \sum_{i=m_{k-1}+1}^{m_k}   u_k(\omega^{(i)}), 
\]
the MFMC estimator \eqref{eq:MFMC_estimator} can be written as
\begin{equation}\label{eq:MFMC_estimator_independent}
    A^{\text{MF}} 
    = A^{\text{MC}}_{1,m_1} 
      +  \sum_{k=2}^K \alpha_k\left(1-\frac{m_{k-1}}{m_k}\right)
                               \left(A_{k,m_k\backslash m_{k-1}}^{\text{MC}}-A_{k,m_{k-1}}^{\text{MC}}\right).
\end{equation}
In this formulation, the two terms in each correction are evaluated on independent sample sets, 
which simplifies variance analysis. 
We also express the MFMC estimator in compact form
\begin{equation*}\label{eq:MFMC_estimator_Correction}
          A^{\text{MF}} = Y_1 + \sum_{k=2}^K \alpha_k Y_k,
\end{equation*}
where the correction terms $Y_k$ are defined as
\begin{equation} \label{eq:MFMC_Yk}
       Y_1 := A^{\text{MC}}_{1,m_1},\qquad 
       Y_k := A^{\text{MC}}_{k,m_k} - A^{\text{MC}}_{k,m_{k-1}}
               =\left(1-\frac{m_{k-1}}{m_k}\right)
                 \left(A_{k,m_k\backslash m_{k-1}}^{\text{MC}}-A_{k,m_{k-1}}^{\text{MC}}\right), \quad k=2\ldots, K.
\end{equation}
%
Because $\mathbb{E}\big[ A^{\text{MC}}_{k,m} \big] = \mathbb{E}\big[ u_k\big]$, $m \in \{ m_{k-1}, m_k \}$, $k\ge 1$,
it holds that  $\mathbb{E}[Y_1] =  \mathbb{E}[u_1]$ and  
$\mathbb{E}[Y_k] = 0$ for $k\ge 2$.
Consequently, for any selection of weights $\alpha_k$, 
the MFMC estimator is  unbiased, 
\begin{equation}\label{eq:MFMC_expectation}
            \mathbb{E}[A^{\text{MF}}] =  \mathbb{E}[u_1].
\end{equation}
Note that $m_1 \ge 1$ is needed to ensure \eqref{eq:MFMC_expectation}; otherwise the MFMC estimator
is biased.
To analyze the variance, we define the following quantties.
For each model $u_k$  and each pair of models $u_k$ and $u_j$, 
we define the variance and the Pearson correlation coefficient as
\begin{equation*}
    \sigma_k^2 = \mathbb{V}\left[u_k\right],\qquad \rho_{k,j} 
                       = \frac{\text{Cov}\left[ u_k, u_j\right]}{\sigma_k\sigma_j}, \quad k,j=1,\dots, K,
\end{equation*}
where the covariance is 
$\text{Cov}[u_k, u_j] := \mathbb{E}[\langle u_k - \mathbb{E}[u_k], u_j - \mathbb{E}[u_j]\rangle_{{\mathcal U}}]$.
By definition, $\rho_{k,k}=1$. 
The variances of the components are
\begin{equation}\label{eq:Var_Yk}
    \mathbb{V}[Y_1] = \frac{\sigma_1^2}{m_1}, \qquad 
    \mathbb{V}[Y_k] = \left(\frac{1}{m_{k-1}} - \frac{1}{m_k}\right)\sigma_k^2, \;\; k=2\ldots, K.
\end{equation}
The correlations satisfy
\begin{equation}\label{eq:Cov_Yk}
\operatorname{Cov} [Y_k,Y_j ]=0, \quad  2\le k<j\le K, 
\quad \mbox{ and } \quad
\operatorname{Cov}[Y_1,Y_k] = -\!\left(\frac{1}{m_{k-1}} - \frac{1}{m_k}\right)\rho_{1,k}\sigma_1\sigma_k,
\quad  2\le k \le K, 
\end{equation}
as shown in ??? and \cite[Lemma~3.2]{BPeherstorfer_KWillcox_MDGunzburger_2016a}. 
Combining \eqref{eq:Var_Yk} and \eqref{eq:Cov_Yk} gives
%
\begin{equation}\label{eq:MFMC_variance0}
    \mathcal{V}^{\text{MF}}(m_1, \ldots, m_K, \alpha_1, \ldots, \alpha_K)
    :=  \mathbb{V}[A^{\text{MF}}]
    =\frac{\sigma_1^2}{m_1} 
    + \sum_{k=2}^K \left(\frac{1}{m_{k-1}} - \frac{1}{m_k}\right)\!\left(\alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k\right).
\end{equation}

If $C_k$ denotes the per-sample cost of model $u_k$, the total computational cost of the MFMC estimator  is 
\begin{equation}\label{eq:MFMC_cost}
        \mathcal{W}^{\text{MF}}(m_1, \ldots, m_K)  = \sum_{k=1}^K C_k m_k.
\end{equation}



In order to determine optimal sample sizes $m_k$ and weights $\alpha_k$ in the MFMC estimator \eqref{eq:MFMC_estimator_independent} the MFMC estimator  variance \eqref{eq:MFMC_variance0} 
is minimized subject to a fixed budget $b$ on the computational cost \eqref{eq:MFMC_cost}.
We note that the problem of computing optimal sample sizes $m_k$ and weights $\alpha_k$ 
can be separated. 
If ${\mathcal N}$ is a set of feasible integer sample sizes satisfying $1 \le m_1 \le m_2 \le \ldots \le m_K$,
then
\begin{align}   \label{eq:MFMC_variance_min_alpha}
    & \min_{\alpha_2, \ldots, \alpha_K \in \real,\, (m_1, \ldots, m_K) \in {\mathcal N}} 
           \mathcal{V}^{\text{MF}}(m_1, \ldots, m_K, \alpha_1, \ldots, \alpha_K)                \nonumber \\
    & = \min_{(m_1, \ldots, m_K) \in {\mathcal N}} 
          \Big( \min_{\alpha_2, \ldots, \alpha_K \in \real} 
                       \mathcal{V}^{\text{MF}}(m_1, \ldots, m_K, \alpha_1, \ldots, \alpha_K)\Big).
\end{align}
From \eqref{eq:MFMC_variance0} it is easy to see that a solution of the  inner minimization problem in
\eqref{eq:MFMC_variance_min_alpha} is
\begin{equation}   \label{eq:MFMC_weights}
    \alpha_k^* = \frac{\rho_{1,k}\sigma_1}{\sigma_k}.
\end{equation}
If $1 \le m_1 < m_2 < \ldots < m_K$, \eqref{eq:MFMC_weights} is the unique solution.
With the optimal weights \eqref{eq:MFMC_weights}, the MFMC estimator  variance \eqref{eq:MFMC_variance0} 
\begin{subequations}  \label{eq:MFMC_variance}
\begin{align}
    \mathcal{V}^{\text{MF}}(m_1, \ldots, m_K)
    :=  \mathbb{V}[A^{\text{MF}}]
   &= \frac{\sigma_1^2}{m_1} 
       - \sum_{k=2}^K \left(\frac{1}{m_{k-1}} - \frac{1}{m_k}\right) \rho_{1,k}^2 \sigma_1^2,   \label{eq:MFMC_variance_a} \\
  &= \sigma_1^2\sum_{k=1}^K \frac{ \rho_{1,k}^2 - \rho_{1,k+1}^2}{m_k},           \label{eq:MFMC_variance_b}
\end{align}
\end{subequations}
where 
\[
                 \rho_{1,K+1} :=0.
\]

As noted earlier,  $m_1 \ge 1$ is needed to ensure the unbiasedness \eqref{eq:MFMC_expectation}
of the MFMC estimator.
The expression  \eqref{eq:MFMC_variance_a} for the variance reveals that if
$m_k = m_{k-1}$ for some model $k$, then this model does not contribute to the variance reduction, i.e.,
\begin{equation}  \label{eq:MFMC_variance_k}
    \mathcal{V}^{\text{MF}}(m_1, \ldots, m_K)
    :=  \mathbb{V}[A^{\text{MF}}]
    = \frac{\sigma_1^2}{m_1} 
       - \sum_{m_k > m_{k-1}} \left(\frac{1}{m_{k-1}} - \frac{1}{m_k}\right) \rho_{1,k}^2 \sigma_1^2
   = \sigma_1^2\sum_{m_k > m_{k-1}} \frac{ \rho_{1,k}^2 - \rho_{1,k+1}^2}{m_k},
\end{equation}
where again $\rho_{1,K+1} :=0$ and $m_0 := 0$.
