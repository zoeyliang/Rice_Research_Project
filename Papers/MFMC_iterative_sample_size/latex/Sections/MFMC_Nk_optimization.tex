%!TEX root = ../main.tex
% ====================================================
\section{Optimal Sample Size Allocation}\label{sec:MFMC_Nk_optimize}
% ====================================================




\begin{equation}\label{eq:Optimization_pb_sample_size}
    \begin{array}{ll}
    \min &\mathcal{V}^{\text{MF}}\left(\alpha_k,N_k\right),\\
       \text{subject to} &\displaystyle\sum\limits_{k=1}^K C_kN_k=p,\\[2pt]
       &\displaystyle N_1\ge 0,\quad \displaystyle N_{k-1}\le N_k, \;\; k=2\ldots,K,\\
       &N_1,\ldots, N_K\in \mathbb{R},\\
       &\alpha_2,\ldots,\alpha_K\in \mathbb{R}.
    \end{array}
\end{equation}
%
Note that for each level $k\ge 2$, $\alpha_k$ enters only through a quadratic expression independent of $N_k$ in the variance term. This separable structure allows a fundamental simplification of the variance functional, which allows hierarchical minimization
%
\begin{equation*}
    \min_{\alpha_k,\, N_k} \mathcal{V}^{\text{MF}}\left(\alpha_k, N_k\right)
    = \min_{N_k}\Big(\min_{\alpha_k} \mathcal{V}^{\text{MF}}(\alpha_k, N_k)\Big).
\end{equation*}
%
The hierarchical minimization admits a closed-form solution for optimal weights by solving the inner optimization $\partial \mathcal{V}^{\text{MF}}/\partial \alpha_k = 0$, yielding 
%
\begin{equation}\label{eq:MFMC_weights}
    \alpha_k^* = \frac{\rho_{1,k}\sigma_1}{\sigma_k}.
\end{equation}
%
Substituting $\alpha_k^*$ into \eqref{eq:MFMC_variance} simplifies the variance to 
%
\begin{equation*}
    \mathcal{V}^{\text{MF}}\left(\alpha_k^*, N_k\right)
    = \sigma_1^2\sum_{k=1}^K \frac{\Delta_k}{N_k},
\end{equation*}
%
where $\Delta_k = \rho_{1,k}^2 - \rho_{1,k+1}^2$ for $k = 1, \dots, K$ with $\rho_{1,K+1}=0$. This reduces the joint optimization to a continuous resource allocation problem involving only sample allocation
%
\begin{equation}\label{eq:Optimization_pb_sample_size_reduced}
    \begin{array}{ll}
    \min &\displaystyle f(N_k) =\sum_{k=1}^K \frac{\Delta_k}{N_k},\\
       \text{subject to} &\displaystyle\sum\limits_{k=1}^K C_kN_k=p,\\[2pt]
       &\displaystyle -N_1\le 0,\quad \displaystyle N_{k-1}-N_k\le 0, \;\; k=2\ldots,K,\\
       &N_1,\ldots, N_K\in \mathbb{R},
    \end{array}
\end{equation}
%
where $f(N_k)$ is the {\it normalized variance functional}. Under suitable monotonicity and ordering assumptions, this problem admits an analytic solution that characterizes the optimal allocation of resources across fidelity levels. 


%
\begin{theorem}[Optimal MFMC real-valued sample allocation]
\label{thm:Sample_size_est}
Consider $K$ models $\{u_{k}\}_{k=1}^K$ with standard deviations $\sigma_k$, correlation coefficients $\rho_{1,k}$ of LF model $u_k$ with the HF model $u_1$, and per-sample costs $C_k$. Define $\Delta_k = \rho_{1,k}^2 - \rho_{1,k+1}^2$ for $k = 1, \dots, K$ with $\rho_{1,K+1}=0$. Assume the following conditions hold
%
\begin{alignat*}{3}
&(i)\;\textit{Monotone correlations:} &\quad& |\rho_{1,1}| > \cdots > |\rho_{1,K}|,\\
&(ii)\;\textit{Cost-correlation ratio:} &\quad& \frac{\Delta_{k}}{C_k} > \frac{\Delta_{k-1}}{C_{k-1}}, \quad k=2,\ldots,K.
\end{alignat*}
%
Then the optimal control weights and sample sizes for \eqref{eq:Optimization_pb_sample_size} are
%
\begin{equation}\label{eq:MFMC_RealValued_Sample_Size}
    \alpha_k^* = \frac{\rho_{1,k}\sigma_1}{\sigma_k}, \qquad
    N_k^* = \sqrt{\frac{\Delta_k}{C_k}}\,
    \frac{p}{\sum_{j=1}^K \sqrt{C_j \Delta_j}}.
\end{equation}
%
% \[
% r_k^* = \sqrt{\frac{C_1\Delta_k}{C_k\Delta_1}},\quad N_1^* = \frac{p}{\sum_{k=1}^K C_k r^*_k}, \quad N_k^*=N_1^*r_k^*.
% \] 
% %
% \JLcolor{alternatively, in my way to represent it without mentioning the vector $\boldsymbol{r}^*$, we have}
%
The resulting minimal variance of the MFMC estimator is
\begin{equation}\label{eq:MFMC_variance_optimal}
\mathcal{V}^{\text{MF}}
= \sigma_1^2\sum_{k=1}^K \frac{\Delta_k}{N_k^*}=\frac{\sigma_1^2}{p}\!\left(\sum_{k=1}^K \sqrt{C_k \Delta_k}\right)^{\!2}.
\end{equation}
\end{theorem}
%
This theorem provides an explicit expression for the optimal tradeoff between statistical efficiency and computational cost in MFMC estimation. The monotone correlation assumption ensures that each successive model provides diminishing but positive variance reduction, while the ordering of the cost–correlation ratios guarantees that adding lower-fidelity models yields net benefit within the total budget. 


In practical applications, Two stages of MFMC: first is model selection, the second uses the selected models to compute MFMC estimator. the quantities $\{C_k\}$ and $\{\rho_{1,k}\}$ are estimated from data or pilot simulations. Based on these estimates, one performs model selection by identifying the subset of fidelity levels that satisfy the monotonicity and ordering assumptions while minimizing the estimator variance. This subset defines the active models used in the MFMC estimator in the first stage, ensuring that the allocation $\{N_k^*\}$ derived from \eqref{eq:MFMC_RealValued_Sample_Size} attains the minimum variance \eqref{eq:MFMC_variance_optimal} under the prescribed computational budget in the second stage.

\subsection{Model selection}
We need to select the models from the available set such that the parameteres associated with the selected models satisfy the two conditions in Theorem \ref{thm:Sample_size_est}, as well as the $\mathcal{V}^{\text{MF}}$ as small as possible (Note that minimize \eqref{eq:MFMC_variance_optimal} is a selection of model that associated with different cost and $\Delta_k$, and is independent of budget $p$). Let $\mathcal{S}^*=\{1, \ldots, K^*\}$ be the indices of $K^*$ available models. We seek a subset $\mathcal{S}=\{i_1,i_2, \ldots,i_{K}\}\subseteq \mathcal{S}^* (K\le K^*)$ of indices that minimizes the sampling cost of multifidelity Monte Carlo estimator. Note that $\mathcal{S}$ is non-empty and $i_1=1$ since the high fidelity model must be included. We will follow the exhaustive algorithm in \cite[Algorithm~1]{PeWiGu:2016} for $2^{K^*-1}$ subsets of $\mathcal{S}^*$.  This algorithm gives the indices of the selected model.

% \begin{equation*}\label{eq:Optimization_pb_model_selection}
%     \begin{array}{lll}
%     \displaystyle\min_{k\in \mathcal{S}^*} &\displaystyle \mathcal{V}^\text{MF},\\
%        \text{s.t.} &\displaystyle |\rho_{1,1}|>\ldots>|\rho_{1,K^*}|,\\
%        &\displaystyle \frac{\Delta_k}{C_k}>\frac{\Delta_{k-1}}{C_{k-1}}, \quad k=2,\ldots,K^*, \quad \rho_{1,K^*+1}=0,\\
%        % &\JLcolor{\displaystyle \frac{\sum_{k\in S_1}\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}}{\sum_{k\in S_1} C_k}\sum_{k\in S_1}\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2\ge \frac{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2\epsilon^2}{\sigma_1^2},}\\
%        % &\JLcolor{\displaystyle \log_s\left\{\frac{\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{Z}^2\epsilon^2}\sum_{k=1}^K\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}\sum_{k=1}^K\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2\right\}\ge \gamma,}\\
%         % &\displaystyle \rho_{1,0}=\infty \;\text{ and } \;\rho_{1,K+1}=0. \\%[6pt]
%     \end{array}
% \end{equation*}


\normalem
\begin{algorithm}[!ht]
\label{algo:MFMC_Algo_model_selection}
\DontPrintSemicolon    
   \KwIn{$K^*$ models $u_k$ with $C_k$ and $\rho_{1,k}.$}\vspace{1ex}
    
    \KwOut{ Selected index set $\mathcal{S}$.}\vspace{1ex}
    \hrule \vspace{1ex}

   % Estimate $\rho_{1,k}$ and $C_k$ for each model $f_k$ using $N_0$ samples.
   
   
   Reorder $u_k$ by decreasing $|\rho_{1,k}|$ with their corresponding $C_k$. Rename the model as 1 to $K^*$, $\mathcal{S}=\{1,\ldots, K^*\}$. 
   
   Initialize $v_{\min}=C_1$, $\mathcal{S}=\{1\}$. Let $ \mathcal{\widehat S}$ be all $2^{K-1}$ ordered subsets of $\mathcal{S}^*$, each containing the high fidelity model with index $1$. 
   % Set $ \mathcal{\widehat S}_1=\mathcal{S}^*$.

    % $(2 \le j \le 2^{K-1})$
    \For{each subset $\mathcal{\widehat S}_j$\,}{

    {
    \If{ condition $(ii)$ from Theorem \ref{thm:Sample_size_est} is satisfied}{
    Compute $\Delta_k$ and $v = \left(\sum_{k=1}^K \sqrt{C_k \Delta_k}\right)^{\!2}$.
    
    \If{$v<v_{\min}$}{
    {
    Update $\mathcal{S} = \mathcal{\widehat S}_j$ and $v_{\min} = v$.
    }
    } 
    }
    }
    $j=j+1$.
    }
    Return  $\mathcal{S}$.
\caption{Multi-fidelity Model Selection}
\end{algorithm}
\ULforem


Differentiating the normalized variance and cost with respect to the sample sizes gives
%
\[
\frac{\partial f}{\partial N_k} = -\frac{\Delta_k}{N_k^2},
\qquad 
\frac{\partial \mathcal{W}^{\text{MF}}}{\partial N_k} = C_k.
\]
%
These relations quantify the variance–cost trade-off: increasing samples at any level reduces variance at the expense of computational resources. At the continuous optimum \eqref{eq:MFMC_RealValued_Sample_Size}, the marginal variance reduction per unit cost $\Delta_k/(C_k N_k^2)$ is identical across all active models, establishing a balanced resource allocation that characterizes the optimal allocation.

While Theorem~\ref{thm:Sample_size_est} provides real-valued optimal allocations $N_k^*$, practical implementation requires integer sample sizes. The standard approach \cite{PeWiGu:2016} applies the floor function $\lfloor N_k^* \rfloor$ to ensure budget feasibility. The realized variance and cost are
%
\[
f\left(\left\lfloor N_k^* \right\rfloor\right) = \sum_{k=1}^K\frac{\Delta_{k}}{\left\lfloor N_k^* \right\rfloor}, \qquad \mathcal{W}^{\text{MF}}\left(\left\lfloor N_k^* \right\rfloor\right) = \sum_{k=1}^K C_k\left\lfloor N_k^* \right\rfloor.
\]
%
Since $N_k^*-1 < \lfloor N_k^*\rfloor \le N_k^*$, the floor operation induces bounded sub-optimality, producing the bounds
%
\begin{equation}\label{eq:bounds_for_floor}
\begin{aligned}
    % f\left(\left\lfloor N_k^* \right\rfloor\right)&\in \left[\sum_{k=1}^K\frac{\Delta_{k}}{N_k^*},\; \sum_{k=1}^K\frac{\Delta_{k}}{N_k^*-1}\right) = \left[\frac{1}{p}\left(\sum_{k=1}^K \sqrt{C_k\Delta_k}\right)^2, \sum_{k=1}^K\frac{\Delta_{k}}{\frac{p}{\sum_{j=1}^K \sqrt{C_j\Delta_j}}\sqrt{\frac{\Delta_k}{C_k}}-1}\right)\\
    % &=\left[\frac{1}{p}\left(\sum_{k=1}^K \sqrt{C_k\Delta_k}\right)^2, \sum_{k=1}^K \sqrt{C_k\Delta_k}\sum_{k=1}^K\frac{\sqrt{C_k\Delta_{k}}}{p-\sqrt{\frac{C_k}{\Delta_k}}\sum_{j=1}^K \sqrt{C_j\Delta_j}}\right)\\
    % &=\sum_{k=1}^K \sqrt{C_k\Delta_k}\left[\frac{\sum_{k=1}^K \sqrt{C_k\Delta_k}}{p},\sum_{k=1}^K\frac{\sqrt{C_k\Delta_{k}}}{p-\sqrt{\frac{C_k}{\Delta_k}}\sum_{j=1}^K \sqrt{C_j\Delta_j}}\right)\\
    % \mathcal{W}^{\text{MF}}\left(\left\lfloor N_k^* \right\rfloor\right) &\in \left(\sum_{k=1}^KC_kN_k^*-\sum_{k=1}^K C_k, \sum_{k=1}^KC_kN_k^*\right]=\left( p-\sum_{k=1}^K C_k,p\right].
    f\left(\left\lfloor N_k^* \right\rfloor\right) \in \left[\frac{1}{p}\left(\sum_{k=1}^K \sqrt{C_k\Delta_k}\right)^2, \sum_{k=1}^K\frac{\Delta_{k}}{N_k^*-1}\right), \qquad
\mathcal{W}^{\text{MF}}\left(\left\lfloor N_k^* \right\rfloor\right)\in \left( p-\sum_{k=1}^K C_k, p\right].
\end{aligned}
\end{equation}
%
The term $\sum_{k=1}^K C_k$ represents the rounding-induced slack in the budget, which becomes negligible asymptotically as $p \to \infty$. However, in the pre-asymptotic regime -- where the total budget $p$ is moderate -- this  slack can lead to significant under-utilization of the computational resources. This observation naturally motivates \textit{the development of  alternative integer-valued allocation strategies that reduce slack and achieve tighter budget utilization.}





% This quantity is the marginal variance reduction rate — how much the total variance decreases when you spend more samples at level by taking one more sample cost $C_k$. So the marginal variance reduction per unit cost
% \[
% \frac{-\frac{\partial f}{\partial N_k}}{C_k} = \frac{\Delta_k}{C_kN_k^2}
% \]
% It quantifies that How much variance reduction we get per unit cost at level $k$.
% At the optimum, the system reaches equilibrium where every active model yields the same return per cost unit,
% \[
% \frac{\Delta_k}{C_kN_k^2} = \text{Constant}=\frac{1}{p^2}\left(\sum_{k=1}^K \sqrt{C_k \Delta_k}\right)^{\!2}, \quad \text{for all active}\;\; k.
% \]
















