%!TEX root = ../main.tex
% ====================================================
\section{Optimal Sample Size Allocation}\label{sec:MFMC_Nk_optimize}
% ====================================================

% ====================================================
\subsection{Integer Programming Formulation}  \label{sec:MFMC_Nk_optimize_IP}
% ====================================================
The optimal samples $1 \le m_1 \le m_2 \le \ldots \le m_K$ are computed to minimize
the variance \eqref{eq:MFMC_variance} of the MFMC estimator
subject to (s.t.) a constraint on the cost \eqref{eq:MFMC_cost} to execute the MFMC estimator.
A first version of the optimization problem to compute $1 \le m_1 \le m_2 \le \ldots \le m_K$ is
\begin{subequations}\label{eq:Optimization_sample_size_N}
    \begin{align}
    \min \quad &\sigma_1^2  \sum_{k=1}^K \frac{ \rho_{1,k}^2 - \rho_{1,k+1}^2}{m_k},   \\
       \text{s.t.}\quad & \sum_{k=1}^K C_km_k \le b,       \label{eq:Optimization_sample_size_m_budget}  \\
                                & m_1\ge 1,\quad  m_k \ge m_{k-1}, \quad k=2\ldots,K,\\
                                &m_1,\ldots, m_K\in \nat.
    \end{align}
\end{subequations}
The issue with this formulation is that if $m_k = m_{k-1}$, the $k$-th model
does not contribute to the variance, which is easier to see from \eqref{eq:MFMC_variance_a}.
Therefore, if $m_k = m_{k-1}$, the $k$-th model should not be executed, but it still contributes to
the computing budget \eqref{eq:Optimization_sample_size_m_budget}.
To fix this issue we introduce binary variables $z_2, \ldots, z_K \in \{0,1\}$ such that
$z_k = 1$ if $m_k > m_{k-1}$ and model $k$ will be sampled $m_k$ times, and 
$z_k = 0$ if $m_k = m_{k-1}$ and model $k$ will be skippted.
The optimization formulation requires 
an upper bound $M$ for all possible differences $m_k - m_{k-1}$, $k=2\ldots,K$.
For example, $M = p/ C_K$ is such an upper bound because  $p/ C_K \ge m_K \ge m_k - m_{k-1}$, $k=2\ldots,K$.
The optimization problem formulation for the optimal sample size selection is
\begin{subequations}\label{eq:Optimization_sample_size_Nz}
    \begin{align}
    \min \quad &\sigma_1^2  \sum_{k=1}^K \frac{ \rho_{1,k}^2 - \rho_{1,k+1}^2}{m_k},   \\
       \text{s.t.}\quad &  C_1 m_1 + \sum_{k=2}^K z_k C_k m_k \le b,       \label{eq:Optimization_sample_size_Nz_budget}  \\
                                & m_1\ge 1,\quad  m_k \ge m_{k-1}, \quad k=2\ldots,K,\\
                                & m_k - m_{k-1} \ge z_k, \quad M z_k \ge m_k - m_{k-1},  \quad k=2\ldots,K,  \label{eq:Optimization_sample_size_Nz_z}  \\
                                &m_1,\ldots, m_K\in \nat, \quad z_2 ,\ldots, z_K\in \{0,1\}.
    \end{align}
\end{subequations}
The constraints \eqref{eq:Optimization_sample_size_Nz_z} ensures that $z_k = 0$ if $m_k = m_{k-1}$ and 
$z_k = 1$ if $m_k >  m_{k-1}$. If $z_k = 0$, model $k$ will not be executed and does not contribute to the 
computational cost \eqref{eq:Optimization_sample_size_Nz_budget}.




% ====================================================
\subsection{Relaxation}  \label{sec:MFMC_Nk_optimize_relax}
% ====================================================
Instead of solving the integer programming problem \eqref{eq:Optimization_sample_size_Nz} or
even \eqref{eq:Optimization_sample_size_N}, previous papers including
\cite{BPeherstorfer_KWillcox_MDGunzburger_2016a} and \cite{AGruber_MGunzburger_LJu_ZWang_2023a}
have considered a relaxation  and then used rounding to obtain integer sample sizes.
Specifically, \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a}
considered the problem
\begin{subequations}\label{eq:Optimization_sample_size_m_relaxed}
    \begin{align}
    \label{eq:Optimization_sample_size_m_relaxed_obj}
    \min \quad &\sigma_1^2  \sum_{k=1}^K \frac{ \rho_{1,k}^2 - \rho_{1,k+1}^2}{m_k},   \\
       \text{s.t.}\quad & \sum_{k=1}^K C_km_k \le b,       \\
                                & m_1\ge 0,\quad  m_k \ge m_{k-1}, \quad k=2\ldots,K,\\
                                &m_1,\ldots, m_K\in \real.
    \end{align}
\end{subequations}
The formulation \eqref{eq:Optimization_sample_size_m_relaxed} potentially suffers from the same isses as 
\eqref{eq:Optimization_sample_size_N}, namely that if f $m_k = m_{k-1}$, the $k$-th model does not
provide variance reduction, but it's cost is included. 
However, under conditions specified in the following Theorem~\ref{thm:Sample_size_real}, the 
real valued solution
of \eqref{eq:Optimization_sample_size_m_relaxed} can be computed analytically, and it satisfies
$0  < m_1^* < m_2^* <  \ldots < m_K^*$.
The following theorem is proven in \cite[Th.~3.4]{BPeherstorfer_KWillcox_MDGunzburger_2016a}.

\begin{theorem}[Optimal MFMC Real-Valued Sample Allocation]   \label{thm:Sample_size_real}
   Let models $u_1, \ldots, u_k \in   L_{\mathbb{b}}^2(W, {\mathcal U})$ with
   standard deviations $\sigma_k$, correlation coefficients $\rho_{1,k}$, $k = 2, \ldots, K$,
   between the HF model $u_1$  and the  LF models $u_k$, $k = 2, \ldots, K$, and 
   per-sample costs $C_1, \ldots, C_K$ be given.
   If 
   \begin{subequations}\label{eq:Sample_size_real_assumptions}
   \begin{align}
       \label{eq:Sample_size_real_assumptions_a}
        & |\rho_{1,1}| > \cdots > |\rho_{1,K}|, & \text{(monotone correlations)} \\
      \label{eq:Sample_size_real_assumptions_b}
        & \frac{ \rho_{1,k}^2 - \rho_{1,k+1}^2 }{C_k} > \frac{ \rho_{1,k-1}^2 - \rho_{1,k}^2 }{C_{k-1}}, \quad k=2,\ldots,K, 
                                                                  & \text{(cost-correlation ratio)}
    \end{align}
    \end{subequations}
    hold, where $\rho_{1,K+1} :=0$, then the unique solution of \eqref{eq:Optimization_sample_size_m_relaxed} is
    \begin{equation}\label{eq:MFMC_RealValued_Sample_Size}
             m_k^* = \sqrt{\frac{\rho_{1,k}^2 - \rho_{1,k+1}^2}{C_k}} \; 
                          \frac{b}{\sum_{l=1}^K \sqrt{C_l (\rho_{1,l}^2 - \rho_{1,l+1}^2)}},
             \quad k = 1, \ldots, K, 
     \end{equation}
    the cost constraint is active $\sum_{k=1}^K C_km_k^* = p$, 
    and the resulting minimal variance of the MFMC estimator is
     \begin{equation}\label{eq:MFMC_variance_optimal}
           \mathcal{V}^{\text{MF}}(m_1^*, \ldots, m_K^*)
           =   \sigma_1^2  \sum_{k=1}^K \frac{ \rho_{1,k}^2 - \rho_{1,k+1}^2}{m_k^*}
           =  \frac{\sigma_1^2}{b}\!\left(\sum_{k=1}^K \sqrt{C_k (\rho_{1,k}^2 - \rho_{1,k+1}^2) }\right)^{\!2}.
       \end{equation}
\end{theorem}

Note that because of the cost-correlation ratio assumption \eqref{eq:Sample_size_real_assumptions_b}, 
the  solution \eqref{eq:MFMC_RealValued_Sample_Size}
satisfies $0  < m_1^* < m_2^* <  \ldots < m_K^*$.

To obtain integer samples,  \cite[p.~A3171]{BPeherstorfer_KWillcox_MDGunzburger_2016a}
round down, i.e., use $\lfloor m_1^* \rfloor, \ldots, \lfloor m_K^* \rfloor$. 
Rounding down will reduce the cost, i.e., the rounded down sample sizes are still feasible for
\eqref{eq:Optimization_sample_size_m_relaxed}.  Rounding down increases the variance.
Thus, rounding down the sample sizes of more costly, higher fidelity samples, frees up computational 
budget that could be used for additional  samples of less costly, lower fidelity samples to reduce the variance.

Rounding down may lead to $\lfloor m_1^* \rfloor = 0$, which introduces bias,  
$\mathbb{E}[A^{\text{MF}}] \not=  \mathbb{E}[u_1]$,
or may lead to $\lfloor m_k^* \rfloor = \lfloor m_{k-1}^* \rfloor$ for some $k \in \{ 2, \ldots, K\}$, 
which means the $k$-th model does not contribute to variance reduction, but its cost is included in the
computational budget.
The case $\lfloor m_1^* \rfloor = 0$ can happen if the computational budget $p$ is small.
If $0< m_1^* < 1$, \cite{AGruber_MGunzburger_LJu_ZWang_2023a} use $\lceil m_1^* \rceil = 1$,
and iterative recompute integer sample sizes from a modified version of 
Theorem~\ref{thm:Sample_size_real}. See Algorithm~2 in \cite{AGruber_MGunzburger_LJu_ZWang_2023a}.
However, their iterative sample size computation can generate integer sample sizes
with $1 = m_1 =  m_2 = \ldots = m_k$ for some $l \in \{ 2, \ldots, K\}$. 
See Tables~1 and 2 in \cite{AGruber_MGunzburger_LJu_ZWang_2023a}.
In this case, the models $2$ to $k$ do not contribute to variance reduction, but their cost is included in the
computational budget.

Finally, while the  solution \eqref{eq:MFMC_RealValued_Sample_Size} satisfies $0  < m_1^* < m_2^* <  \ldots < m_K^*$,
some rounded down sample sizes may be identical,  $\lfloor m_k^* \rfloor = \lfloor m_{k-1}^* \rfloor$ for some 
$k \in \{ 2, \ldots, K\}$, in which case the samples of the $k$-th model do not contribute to variance
reduction, but their costs are charged to the budget. 
The issues $\lfloor m_1^* \rfloor = 0$ or $\lfloor m_k^* \rfloor = \lfloor m_{k-1}^* \rfloor$ for some 
$k \in \{ 2, \ldots, K\}$, are more likely to occur when the total computational budget $b$ is small relative
 to the cost $C_1$ of the HF.
See, e.g., the numerical examples in  \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a} or \cite{AGruber_MGunzburger_LJu_ZWang_2023a}.

To avoid issues with simple rounding, we propose to compute sample sizes recursively.
The general idea is similar to the one in \cite{AGruber_MGunzburger_LJu_ZWang_2023a},
but \cite{AGruber_MGunzburger_LJu_ZWang_2023a} use a recursion only to avoid
$\lfloor m_1^* \rfloor = \lfloor m_k^* \rfloor = 0$ for some $k \in \{ 2, \ldots, K\}$,
which can happen when the budget $b$ is small.

%Generally, because $m_k^*-1 < \lfloor m_k^*\rfloor \le m_k^*$, the floor operation induces bounded 
%sub-optimality, producing the bounds
%%
%\begin{subequations}\label{eq:bounds_for_floor}
%\begin{align}
%    \mathcal{V}^{\text{MF}}\left(\lfloor m_1^* \rfloor, \ldots \lfloor m_K^* \rfloor \right)
%    & \in \left[\frac{\sigma_1^2}{b}\left(\sum_{k=1}^K \sqrt{C_k  (\rho_{1,k}^2 - \rho_{1,k+1}^2)}\right)^2,
%                   \sum_{k=1}^K\frac{ (\rho_{1,k}^2 - \rho_{1,k+1}^2)}{m_k^*-1}\right),       \\
%   \mathcal{W}^{\text{MF}}\left(\lfloor m_1^* \rfloor, \ldots \lfloor m_K^* \rfloor \right)
%   &\in \left( p-\sum_{k=1}^K C_k, p\right].
%\end{align}
%\end{subequations}
%If $\sum_{k=1}^K C_k \ll p$, the work 
%$\mathcal{W}^{\text{MF}}\big(\lfloor m_1^* \rfloor, \ldots, \lfloor m_K^* \rfloor \big)
% \approx  \mathcal{W}^{\text{MF}}\big( m_1^* , \ldots,  m_K^* \big) = p$.



% ====================================================
\subsection{Model selection}
% ====================================================
\MH{may not need a subsection on this. Can simply reference}
The analytical solution of \eqref{eq:Optimization_sample_size_m_relaxed} in 
Theorem~\ref{thm:Sample_size_real} requires that the models satsfy 
\eqref{eq:Sample_size_real_assumptions}.
Thus, given models, we need to select the models from the available set such that the parameters 
associated with the selected models satisfy the two conditions in Theorem \ref{thm:Sample_size_real}, as well as the $\mathcal{V}^{\text{MF}}$ as small as possible (Note that minimize \eqref{eq:MFMC_variance_optimal} is a selection of model that associated with 
different cost and $\rho_{1,k}^2 - \rho_{1,k+1}^2$, 
and is independent of budget $p$). Let $\mathcal{S}^*=\{1, \ldots, K^*\}$ be the indices of $K^*$ available models. 
We seek a subset $\mathcal{S}=\{i_1,i_2, \ldots,i_{K}\}\subseteq \mathcal{S}^* (K\le K^*)$ of indices that minimizes the sampling cost of multifidelity Monte Carlo estimator. Note that $\mathcal{S}$ is non-empty and $i_1=1$ since the high fidelity model must be included. 
We will follow the exhaustive algorithm in \cite[Algorithm~1]{BPeherstorfer_KWillcox_MDGunzburger_2016a} 
for $2^{K^*-1}$ subsets of $\mathcal{S}^*$.  This algorithm gives the indices of the selected model.

\normalem
\begin{algorithm}[!ht]
\label{algo:MFMC_Algo_model_selection}
\DontPrintSemicolon    
   \KwIn{Models $u_1, \ldots, u_{K^*}$ ordered such that $|\rho_{1,2}| \ge \ldots \ge |\rho_{1,K^*}|$,
             and corresponding sample costs  $C_1, \ldots, C_{K^*}$.}\vspace{1ex}
    
    \KwOut{ Selected index set $\mathcal{S}$.}\vspace{1ex}
    \hrule \vspace{1ex}

   % Estimate $\rho_{1,k}$ and $C_k$ for each model $f_k$ using $m_0$ samples.
   
   
   Set $\mathcal{S}=\{1,\ldots, K^*\}$. 
   
   Initialize $v_{\min}=C_1$, $\mathcal{S}=\{1\}$. Let $ \mathcal{\widehat S}$ be all $2^{K-1}$ ordered subsets of $\mathcal{S}^*$, each containing the high fidelity model with index $1$. 
   % Set $ \mathcal{\widehat S}_1=\mathcal{S}^*$.

    % $(2 \le j \le 2^{K-1})$
    \For{each subset $\mathcal{\widehat S}_j$\,}{

    {
    \If{ Condition \eqref{eq:Sample_size_real_assumptions_b} from Theorem \ref{thm:Sample_size_real} is satisfied}{
    Compute $\Delta_k$ and $v = \left(\sum_{k=1}^K \sqrt{C_k (\rho_{1,k}^2 - \rho_{1,k+1}^2) }\right)^{\!2}$.
    \MH{What is $K$?}
    
    \If{$v<v_{\min}$}{
    {
    Update $\mathcal{S} = \mathcal{\widehat S}_j$ and $v_{\min} = v$.
    }
    } 
    }
    }
    $j=j+1$.
    }
    Return  $\mathcal{S}$.
\caption{Multi-fidelity Model Selection}
\end{algorithm}
\ULforem











