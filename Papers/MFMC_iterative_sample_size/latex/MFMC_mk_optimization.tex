%!TEX root = main.tex
% ====================================================
\section{Optimal Sample Size Allocation}\label{sec:MFMC_mk_optimize}
% ====================================================
Because of \eqref{eq:MFMC_variance_kb}, we assume that the models satisfy
\begin{equation} \label{eq:correlation_assumption}
         |\rho_{1,1}| > \cdots > |\rho_{1,K}| \qquad \text{(monotone correlations)}.
\end{equation}
Under the assumption \eqref{eq:correlation_assumption}, the 
MFMC estimator  variance \eqref{eq:MFMC_variance} is convex in $m_1,  m_2,  \ldots,  m_K > 0$ 


% ====================================================
\subsection{Integer Programming Formulation}  \label{sec:MFMC_mk_optimize_IP}
% ====================================================
The optimal samples $1 \le m_1 \le m_2 \le \ldots \le m_K$ are computed to minimize
the variance \eqref{eq:MFMC_variance} of the MFMC estimator
subject to (s.t.) a constraint on the cost \eqref{eq:MFMC_cost} to execute the MFMC estimator.
A first version of the optimization problem to compute $1 \le m_1 \le m_2 \le \ldots \le m_K$ is
\begin{subequations}\label{eq:Optimization_sample_size_N}
    \begin{align}
    \min \quad &\sigma_1^2  \sum_{k=1}^K \frac{ \rho_{1,k}^2 - \rho_{1,k+1}^2}{m_k},   \\
       \text{s.t.}\quad & \sum_{k=1}^K C_km_k \le b,       \label{eq:Optimization_sample_size_m_budget}  \\
                                & m_1\ge 1,\quad  m_k \ge m_{k-1}, \quad k=2\ldots,K,\\
                                &m_1,\ldots, m_K\in \nat.
    \end{align}
\end{subequations}
The issue with this formulation is that if $m_k = m_{k-1}$, the $k$-th model
does not contribute to the variance, which is easier to see from \eqref{eq:MFMC_variance_a}.
Therefore, if $m_k = m_{k-1}$, the $k$-th model should not be executed, but it still contributes to
the computing budget \eqref{eq:Optimization_sample_size_m_budget}.
To fix this issue we introduce binary variables $z_2, \ldots, z_K \in \{0,1\}$ such that
$z_k = 1$ if $m_k > m_{k-1}$ and model $k$ will be sampled $m_k$ times, and 
$z_k = 0$ if $m_k = m_{k-1}$ and model $k$ will be skippted.
The optimization formulation requires 
an upper bound $M$ for all possible differences $m_k - m_{k-1}$, $k=2\ldots,K$.
For example, $M = p/ C_K$ is such an upper bound because  $p/ C_K \ge m_K \ge m_k - m_{k-1}$, $k=2\ldots,K$.
The optimization problem formulation for the optimal sample size selection is
\begin{subequations}\label{eq:Optimization_sample_size_Nz}
    \begin{align}
    \min \quad &\sigma_1^2  \sum_{k=1}^K \frac{ \rho_{1,k}^2 - \rho_{1,k+1}^2}{m_k},   \\
       \text{s.t.}\quad &  C_1 m_1 + \sum_{k=2}^K z_k C_k m_k \le b,       \label{eq:Optimization_sample_size_Nz_budget}  \\
                                & m_1\ge 1,\quad  m_k \ge m_{k-1}, \quad k=2\ldots,K,\\
                                & m_k - m_{k-1} \ge z_k, \quad M z_k \ge m_k - m_{k-1},  \quad k=2\ldots,K,  \label{eq:Optimization_sample_size_Nz_z}  \\
                                &m_1,\ldots, m_K\in \nat, \quad z_2 ,\ldots, z_K\in \{0,1\}.
    \end{align}
\end{subequations}
The constraints \eqref{eq:Optimization_sample_size_Nz_z} ensures that $z_k = 0$ if $m_k = m_{k-1}$ and 
$z_k = 1$ if $m_k >  m_{k-1}$. If $z_k = 0$, model $k$ will not be executed and does not contribute to the 
computational cost \eqref{eq:Optimization_sample_size_Nz_budget}.




% ====================================================
\subsection{Relaxation}  \label{sec:MFMC_mk_optimize_relax}
% ====================================================
Instead of solving the integer programming problem \eqref{eq:Optimization_sample_size_N},
let alone \eqref{eq:Optimization_sample_size_Nz}, previous papers including
\cite{BPeherstorfer_KWillcox_MDGunzburger_2016a} and \cite{AGruber_MGunzburger_LJu_ZWang_2023a}
have considered a relaxation and then used rounding to obtain integer sample sizes.

In the following, we use $m_k$ to denote an integer-valued sample size for model $k$, 
and $x_k$ to denote real-valued relaxations of the sample size for model $k$.
Moreoer, for $x \in \real$, we use $\lfloor x \rfloor$ to denote the largest integer less than or equal to $x$,
and we use $\lceil x \rceil$ to denote the smallest integer greater than or equal to $x$.



In \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a} the following relaxation 
of \eqref{eq:Optimization_sample_size_N} was considered,
\begin{subequations}\label{eq:Optimization_sample_size_m_relaxed}
    \begin{align}
    \label{eq:Optimization_sample_size_m_relaxed_obj}
    \min \quad &\sigma_1^2  \sum_{k=1}^K \frac{ \rho_{1,k}^2 - \rho_{1,k+1}^2}{x_k},   \\
       \text{s.t.}\quad & \sum_{k=1}^K C_k x_k \le b,       \\
                                & x_1\ge 0,\quad  x_k \ge x_{k-1}, \quad k=2\ldots,K,\\
                                &x_1,\ldots, x_K\in \real.
    \end{align}
\end{subequations}
The formulation \eqref{eq:Optimization_sample_size_m_relaxed} potentially suffers from the same issues as 
\eqref{eq:Optimization_sample_size_N}, namely that if  $x_k = x_{k-1}$, the $k$-th model does not
provide variance reduction, but its cost is included. Moreover, $x_1\ge 0$ is imposed instead of 
$x_1\ge 1$ to allow the analytical solution of \eqref{eq:Optimization_sample_size_m_relaxed}  under
suitable conditions.


Because of the monotone correlations assumption \eqref{eq:correlation_assumption},
\eqref{eq:Optimization_sample_size_m_relaxed} is a convex optimization problem and has a unique
solution.
Under conditions specified in the following Theorem~\ref{thm:Sample_size_real}, the 
real valued solution
of \eqref{eq:Optimization_sample_size_m_relaxed} can be computed analytically.
The following theorem is proven in \cite[Th.~3.4]{BPeherstorfer_KWillcox_MDGunzburger_2016a}.

\begin{theorem}[Optimal MFMC Real-Valued Sample Allocation]   \label{thm:Sample_size_real}
   Let models $u_1, \ldots, u_k \in   L_{\mathbb{b}}^2(W, {\mathcal U})$ with
   standard deviations $\sigma_k$, correlation coefficients $\rho_{1,k}$, $k = 2, \ldots, K$,
   between the HFM $u_1$  and the  LFMs $u_k$, $k = 2, \ldots, K$, and 
   per-sample costs $C_1, \ldots, C_K$ be given.
   If 
   \begin{subequations}\label{eq:Sample_size_real_assumptions}
   \begin{align}
       \label{eq:Sample_size_real_assumptions_a}
        & |\rho_{1,1}| > \cdots > |\rho_{1,K}|, & \text{(monotone correlations)} \\
      \label{eq:Sample_size_real_assumptions_b}
        & \frac{ \rho_{1,k}^2 - \rho_{1,k+1}^2 }{C_k} > \frac{ \rho_{1,k-1}^2 - \rho_{1,k}^2 }{C_{k-1}}, \quad k=2,\ldots,K, 
                                                                  & \text{(cost-correlation ratio)}
    \end{align}
    \end{subequations}
    hold, where $\rho_{1,K+1} :=0$, then the unique solution of \eqref{eq:Optimization_sample_size_m_relaxed} is
    \begin{equation}\label{eq:MFMC_RealValued_Sample_Size}
             x_k^* = \sqrt{\frac{\rho_{1,k}^2 - \rho_{1,k+1}^2}{C_k}} \; 
                          \frac{b}{\sum_{l=1}^K \sqrt{C_l (\rho_{1,l}^2 - \rho_{1,l+1}^2)}},
             \quad k = 1, \ldots, K, 
     \end{equation}
    the cost constraint is active $\sum_{k=1}^K C_k x_k^* = p$, 
    and the resulting minimal variance of the MFMC estimator is
     \begin{equation}\label{eq:MFMC_variance_optimal}
           \mathcal{V}^{\text{MF}}(x_1^*, \ldots, x_K^*)
           =   \sigma_1^2  \sum_{k=1}^K \frac{ \rho_{1,k}^2 - \rho_{1,k+1}^2}{x_k^*}
           =  \frac{\sigma_1^2}{b}\!\left(\sum_{k=1}^K \sqrt{C_k (\rho_{1,k}^2 - \rho_{1,k+1}^2) }\right)^{\!2}.
       \end{equation}
\end{theorem}

Note that because of the cost-correlation ratio assumption \eqref{eq:Sample_size_real_assumptions_b}, 
the  solution \eqref{eq:MFMC_RealValued_Sample_Size}
satisfies $0  < x_1^* < x_2^* <  \ldots < x_K^*$.



To obtain integer samples,  \cite[p.~A3171]{BPeherstorfer_KWillcox_MDGunzburger_2016a}
round down, i.e., use $m_1 = \lfloor x_1^* \rfloor, \ldots, m_K = \lfloor x_K^* \rfloor$. 
Rounding down reduces the cost, i.e., the rounded down sample sizes are still feasible for
\eqref{eq:Optimization_sample_size_m_relaxed}.  However, rounding down increases the variance.
Thus, rounding down the sample sizes of more costly, higher fidelity samples, frees up computational 
budget that could be used for additional samples of less costly, lower fidelity samples to reduce the variance.
Our adaptive sample size computation adresses this shorcoming of rounding down.

Rounding down may lead to $\lfloor x_1^* \rfloor = 0$, which introduces bias,  
$\mathbb{E}[A^{\text{MF}}] \not=  \mathbb{E}[u_1]$.
Rounding down may also lead to $\lfloor x_k^* \rfloor = \lfloor x_{k-1}^* \rfloor$ for some $k \in \{ 2, \ldots, K\}$, 
which means the $k$-th model does not contribute to variance reduction, but its cost is included in the
computational budget.
The case $\lfloor x_1^* \rfloor = 0$ can happen if the computational budget $b$ is small.
If $0< x_1^* < 1$, \cite{AGruber_MGunzburger_LJu_ZWang_2023a} use $m_1 = \lceil x_1^* \rceil = 1$,
and iteratively recompute integer sample sizes from a modified version of 
Theorem~\ref{thm:Sample_size_real}. See Algorithm~2 in \cite{AGruber_MGunzburger_LJu_ZWang_2023a}.
However, their iterative sample size computation can generate integer sample sizes
with $1 = m_1 =  m_2 = \ldots = m_l$ for some $l \in \{ 2, \ldots, K\}$. 
See Tables~1 and 2 in \cite{AGruber_MGunzburger_LJu_ZWang_2023a}.
In this case, the models $2$ to $l$ do not contribute to variance reduction, but their cost is included in the
computational budget.

Finally, while the  solution \eqref{eq:MFMC_RealValued_Sample_Size} satisfies $0  < x_1^* < x_2^* <  \ldots < x_K^*$,
some rounded down sample sizes may be identical,  $\lfloor x_k^* \rfloor = \lfloor x_{k-1}^* \rfloor$ for some 
$k \in \{ 2, \ldots, K\}$, in which case the samples of the $k$-th model do not contribute to variance
reduction, but their costs are charged to the budget. 
The issues $\lfloor x_1^* \rfloor = 0$ or $\lfloor x_k^* \rfloor = \lfloor x_{k-1}^* \rfloor$ for some 
$k \in \{ 2, \ldots, K\}$, are more likely to occur when the total computational budget $b$ is small relative
 to the cost $C_1$ of the HF.
See, e.g., the numerical examples in  \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a} or \cite{AGruber_MGunzburger_LJu_ZWang_2023a}.
We propose to compute sample sizes recursively to deal with this and other issues arising when simply
rounding down the real-valued solution of \eqref{eq:MFMC_RealValued_Sample_Size}.



% ====================================================
\subsection{Model selection}
% ====================================================
The analytical solution of \eqref{eq:Optimization_sample_size_m_relaxed} derived in Theorem~\ref{thm:Sample_size_real} is valid only when the underlying models satisfy the assumptions  \eqref{eq:Sample_size_real_assumptions}. 
Thus, given a pool of candidate models, one must identify a subset of models whose parameters satisfy 
 \eqref{eq:Sample_size_real_assumptions}. There are many such subsets.
 For each subset of models that satisfy \eqref{eq:Sample_size_real_assumptions}, the corresponding
 minimal variance of the MFMC estimator is \eqref{eq:MFMC_variance_optimal}.
 The model selection Algorithm~1 in \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a} computes
 a subset of models that satisfy  \eqref{eq:Sample_size_real_assumptions} and yield the smalled 
 variance  \eqref{eq:MFMC_variance_optimal}.
 Once the models are identified, then \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a} compute 
 real-valued sample sizes \eqref{eq:MFMC_RealValued_Sample_Size} and obtain integer sample
 sizes from rounding.
 
 Note that $\sigma_1/b$ in  \eqref{eq:MFMC_variance_optimal} is a scaling independent of the 
 model. Thus,  the model selection Algorithm~1 in \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a} 
 independent of the total computational budget~$b$.




% Let $\mathcal{S}^*=\{1, \ldots, K^*\}$ be the indices of $K^*$ available models. 
% We seek a subset $\mathcal{S}=\{i_1,i_2, \ldots,i_{K}\}\subseteq \mathcal{S}^* (K\le K^*)$ of indices that minimizes the sampling cost of multifidelity Monte Carlo estimator. Note that $\mathcal{S}$ is non-empty and $i_1=1$ since the high fidelity model must be included. 
% We will follow the exhaustive algorithm in \cite[Algorithm~1]{BPeherstorfer_KWillcox_MDGunzburger_2016a} 
% for $2^{K^*-1}$ subsets of $\mathcal{S}^*$.  This algorithm gives the indices of the selected model.

% \normalem
% \begin{algorithm}[!ht]
% \label{algo:MFMC_Algo_model_selection}
% \DontPrintSemicolon    
%    \KwIn{Models $u_1, \ldots, u_{K^*}$ ordered such that $\rho_{1,2}^2 \ge \ldots \ge \rho_{1,K^*}$,
%              and corresponding sample costs  $C_1, \ldots, C_{K^*}$.}\vspace{1ex}
    
%     \KwOut{ Selected index set $\mathcal{S}$.}\vspace{1ex}
%     \hrule \vspace{1ex}

%    % Estimate $\rho_{1,k}$ and $C_k$ for each model $f_k$ using $m_0$ samples.
   
   
%    Set $\mathcal{S}=\{1,\ldots, K^*\}$. 
   
%    Initialize $v_{\min}=C_1$, $\mathcal{S}=\{1\}$. Let $ \mathcal{\widehat S}$ be all $2^{K-1}$ ordered subsets of $\mathcal{S}^*$, each containing the high fidelity model with index $1$. 
%    % Set $ \mathcal{\widehat S}_1=\mathcal{S}^*$.

%     % $(2 \le j \le 2^{K-1})$
%     \For{each subset $\mathcal{\widehat S}_j$\,}{

%     {
%     \If{ Condition \eqref{eq:Sample_size_real_assumptions_b} from Theorem \ref{thm:Sample_size_real} is satisfied}{
%     Compute $\Delta_k$ and $v = \left(\sum_{k=1}^K \sqrt{C_k (\rho_{1,k}^2 - \rho_{1,k+1}^2) }\right)^{\!2}$.
%     \MH{What is $K$?}
    
%     \If{$v<v_{\min}$}{
%     {
%     Update $\mathcal{S} = \mathcal{\widehat S}_j$ and $v_{\min} = v$.
%     }
%     } 
%     }
%     }
%     $j=j+1$.
%     }
%     Return  $\mathcal{S}$.
% \caption{Multi-fidelity Model Selection}
% \end{algorithm}
% \ULforem










