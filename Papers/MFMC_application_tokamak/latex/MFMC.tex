%!TEX root = main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% ====================================================
\section{Multi-fidelity Monte Carlo}\label{sec:MFMC}
% ====================================================
%
MLMC methods \cite{MBGiles_2015a,SHeinrich_2001a} and 
MFMC methods  \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a, BPeherstorfer_KWillcox_MDGunzburger_2018b} 
build on control variate techniques to reduce the statistical error below $(1-\theta)\epsilon^2$  
at a computational cost less than \eqref{eq:MC-work}.
While both MFMC and MLMC share the goal of variance reduction, they differ in structure and sampling.
In MLMC, corrections consisting of differences between successive levels are added to the coarse level, 
each difference between successive levels is computed with independent samples,
and a decreasing number of samples is used, the higher the level (i.e., fidelity) of the models.
MFMC, by contrast, adds corrections of low-fidelity models to the high-fidelity model and incorporates increasing numbers of inexpensive, low-fidelity samples. 
A key distinction is that MFMC reuses samples across fidelity levels. 

Next we briefly review the core principles underlying multi-fidelity Monte Carlo, drawing on \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a}.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Multi-fidelity Monte Carlo revisited}
The MFMC framework combines a high-fidelity model $u_{h,1} = u_h \in L_{\mathbb{P}}^2(W, \cU)$ 
with a series of lower-fidelity models $u_{h,k} \in L_{\mathbb{P}}^2(W, \cU)$, $k =1,\dots, K$.
Here, as we will make precise below,
fidelity is not related to an almost everywhere pointwise error $\| u_{h,1}(\omega) - u_{h,k}(\omega) \|_U$,
but is related to the correlation between the $k$-th model $u_{h,k}$ and the high-fidelity model $u_{h,1}$.
We assume that as $k$ increases, the fidelity of the $k$-th model decreases and also
that the computational cost of evaluating the $k$-th model at a sample decreases.



For each  $u_{h,k}(\omega)$  and each pair of $u_{h,k}(\omega)$ and $u_{h,j}(\omega)$, 
we define the variance and the Pearson correlation coefficient as
\begin{equation*}
    \sigma_k^2 = \mathbb{V}\left[u_{h,k}(\omega)\right],\qquad \rho_{k,j} 
                       = \frac{\text{Cov}\left[ u_{h,k}(\omega), u_{h,j}(\omega)\right]}{\sigma_k\sigma_j}, \quad k,j=1,\dots, K,
\end{equation*}
where the covariance is 
$\text{Cov}[u_{h,k}, u_{h,j}] := \mathbb{E}[\langle u_{h,k} - \mathbb{E}[u_{h,k}], u_{h,j} - \mathbb{E}[u_{h,j}]\rangle_U]$.
By definition, $\rho_{k,k}=1$. 

Given $N_1 < N_2 < \ldots < N_K$ and i.i.d.\ samples $\omega^{(1)}, \ldots, \omega^{(N_K)}$,
the MFMC estimator $A^{\text{MF}}$ combines an MC estimate of the high-fidelity model with differences 
of MC estimates of the lower-fidelity level models.
The MFMC estimator is defined as
\begin{equation}\label{eq:MFMC_estimator}
    A^{\text{MF}}
     := A^{\text{MC}}_{1,N_1} + \sum_{k=2}^K \alpha_k\left(A^{\text{MC}}_{k,N_k} - A^{\text{MC}}_{k,N_{k-1}} \right),
\end{equation}
where
\begin{equation}\label{eq:MFMC_estimator_MCk}
     A^{\text{MC}}_{k,M} :=  \frac{1}{M} \sum_{i=1}^{M}   u_{h,k}(\omega^{(i)}), \quad M \in \{ N_{k-1}, N_k \},
\end{equation}
and $\alpha_k\in \mathbb{R}$ are weights that will be determined below.
Note that the $N_{k-1}$ evaluations $u_{h,k}(\omega^{(i)})$, $i = 1, \ldots,  N_{k-1}$,
used in  $A^{\text{MC}}_{k,N_{k-1}}$ are reused in  the computation of $A^{\text{MC}}_{k,N_k}$.
This reuse introduces statistical dependence between 
$A^{\text{MC}}_{k,N_{k-1}}$ and $A^{\text{MC}}_{k,N_k}$. 
If we denote the MC average over the $N_k - N_{k-1}$ samples $u_{h,k}(\omega^{(i)})$ 
not included in $A^{\text{MC}}_{k,N_{k-1}}$ by
\[
     A^{\text{MC}}_{k,N_k \backslash N_{k-1}}
      =  \frac{1}{N_k-N_{k-1}}  \sum_{i=N_{k-1}+1}^{N_k}   u_{h,k}(\omega^{(i)}), 
\]
and if we define 
\begin{equation} \label{eq:MFMC_Yk}
       Y_1 := A^{\text{MC}}_{1,N_1},\qquad 
       Y_k := A^{\text{MC}}_{k,N_k} - A^{\text{MC}}_{k,N_{k-1}}
               =\left(1-\frac{N_{k-1}}{N_k}\right)
                 \left(A_{k,N_k\backslash N_{k-1}}^{\text{MC}}-A_{k,N_{k-1}}^{\text{MC}}\right), \quad k=2\ldots, K,
\end{equation}
then the MFMC estimator \eqref{eq:MFMC_estimator} can be written as
\begin{equation}\label{eq:MFMC_estimator_independent}
       A^{\text{MF}} = Y_1 + \sum_{k=2}^K \alpha_k Y_k.
\end{equation}
In this formulation, the two terms in each correction $Y_k$, $k \ge 2$, in \eqref{eq:MFMC_Yk}
are evaluated on independent sample sets, which simplifies variance analysis. 

Because $\mathbb{E}\big[ A^{\text{MC}}_{k,M} \big] = \mathbb{E}\big[ u_{h,k} \big]$, $M \in \{ N_{k-1}, N_k \}$,
$\mathbb{E}[Y_1] =  \mathbb{E}[u_{h,1}]  = \mathbb{E}\big[ u_h \big]$ and  $\mathbb{E}[Y_k] = 0$ for $k\ge 2$.
Consequently, for any selection of weights $\alpha_k$, 
the MFMC estimator is  unbiased, satisfying 
\begin{equation}\label{eq:AMF-unbiased}
        \mathbb{E}[A^{\text{MF}}] =  \mathbb{E}[u_h].
\end{equation}


The variances of the correction terms $Y_k$ are
\begin{equation}\label{eq:Var_Yk}
    \mathbb{V}\left[Y_1\right] = \frac{\sigma_1^2}{N_1}, \quad \mathbb{V}\left[Y_k\right] = \left(1-\frac{N_{k-1}}{N_k}\right)^2\left(\frac{\sigma_k^2}{N_{k-1}}+\frac{\sigma_k^2}{N_k-N_{k-1}}\right) = \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\sigma_k^2.
\end{equation}
The corrections term $Y_k$, $k\ge 2$, are correlated with the high-fidelity estimator $Y_1$.
However, although $Y_k$ and $Y_j$, $2\le k<j \le K$, share overlapping sample sets and are therefore statistically dependent, they are uncorrelated.

\begin{lemma}\label{lemma:Y_k_Y_j}
Let $2\le k<j\le K$. Then 
  The correction terms $Y_k$ and $Y_j$, $2\le k<j \le K$, defined in \eqref{eq:MFMC_Yk} are uncorrelated,
  $\operatorname{Cov} [Y_k,Y_j ]=0$,  $2\le k<j \le K$.
\end{lemma}
The proof of Lemma~\ref{lemma:Y_k_Y_j} is given in Appendix~\ref{sec:proof_lemma:Y_k_Y_j}.

 Using the covariance identity derived from \cite[Lemma~3.2]{PeWiGu:2016}, yields
%
\begin{equation}\label{eq:Cov_Yk}
% \text{Cov}(Y_k,Y_j) =0,\quad \text{for } \;2\le k<j \le K,\qquad 
\text{Cov}[Y_1,Y_k] = - \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\rho_{1,k}\sigma_1\sigma_k, \quad \text{for } \; k\ge 2.
\end{equation}
%
Combining \eqref{eq:Var_Yk} and \eqref{eq:Cov_Yk}, the total variance of the MFMC estimator is 
%
\begin{align}
    \nonumber
    \mathbb{V}\big[A^{\text{MF}}\big] &= \mathbb{V}\left[Y_1\right] + \mathbb{V}\left[\sum_{k=2}^K \alpha_kY_k\right]+2\;\text{Cov}\left[Y_1,\sum_{k=2}^K \alpha_k Y_k \right],\\
    \nonumber
    &=\mathbb{V}\left[Y_1\right] + \sum_{k=2}^K \alpha_k^2 \mathbb{V}\left[Y_k\right]+2\sum_{2\le k<j\le K} \alpha_k\alpha_j\; \text{Cov}[Y_k,Y_j] +2\sum_{k=2}^K \alpha_k\;\text{Cov}\left[Y_1, Y_k\right],\\
    % \nonumber
    % &=\mathbb{V}\left(Y_1\right) + \sum_{k=2}^K \alpha_k^2 \mathbb{V}\left(Y_k\right) +2\sum_{k=2}^K \alpha_k\;\text{Cov}\left(Y_1, Y_k\right),\\
    \label{eq:MFMC_variance}
    &=\frac{\sigma_1^2}{N_1} + \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\left(\alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k\right).
\end{align}
%

Analogously to \eqref{eq:MC-MSE}, using \eqref{eq:AMF-unbiased},
the MSE of the multi-fidelity Monte Carlo estimator is written as 
%
\[
\mathbb E\left[ \big\| \mathbb{E}[u]-A^{\text{MC}}_{N}  \big\| _{U}^2\right]
= \Big\| \mathbb{E}[u]-\mathbb{E}\big[A^{\text{MF}}\big] \Big\|_{U}^2
    +\mathbb E\Big[ \left\Vert\mathbb{E}\big[A^{\text{MF}}\big]-A^{\text{MF}} \right\Vert_{U}^2 \Big]
=\big\| \mathbb{E}[u]-\mathbb{E}[u_h] \big\|_{U}^2
    + \mathbb{V}\big[A^{\text{MF}}\big],
\]
%
where the variance term $\mathbb{V}[A^{\text{MF}}]$ is given by \eqref{eq:MFMC_variance}. 
Assuming \eqref{eq:bias-error}, the bias error is controlled by selecting the mesh-size \eqref{eq:bias-error-h}.
We want to compute an MFMC estimator \eqref{eq:MFMC_estimator_independent}, i.e., 
sample sizes $N_k$ and weights $\alpha_k$, such that the statistical error 
satisfies $\mathbb{V}\big[A^{\text{MF}}\big] \le (1-\theta)\epsilon^2$.
To determine  optimal sample sizes $N_k$ and weights $\alpha_k$ we also
consider the total computational cost for the MFMC estimator
\[
        \mathcal{W}^{\text{MF}} = \sum_{k=1}^K C_kN_k,
\]
where $C_k$ is the cost of generating a single sample of model $u_{h,k}$, and $N_k$ is the corresponding sample count. Unlike \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a} where sample sizes and weights are computed 
by minimizing the variance \eqref{eq:MFMC_estimator_independent} subject to a fixed computational budget, 
we minimize the computational budget subject to the bound $(1-\theta)\epsilon^2$ on the variance.
If sample sizes are allowed to be real, then the optimization problem of \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a} 
and our optimization problem have the same solution. However, differences arise when sample sizes are rounded
to become integer values. In \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a}, sample sizes are rounded down
to preserve the computational budget bound, whereas in our case sample sizes are rounded up to preserve
the variance bound. In particular, our rounding up ensures that $N_1 \ge 1$, which is not guaranteed by the approach 
in  \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a} and which motivated the modification in 
\cite{AGruber_MGunzburger_LJu_ZWang_2023a}.

We compute sample sizes $N_k$ and weights $\alpha_k$ as the solution of
\begin{subequations}\label{eq:Optimization_pb_sample_size0}
    \begin{align}
    \min  \; &\sum\limits_{k=1}^K C_kN_k,\\
    \label{eq:Optimization_pb_sample_size0_var}
       \text{subject to } \; & \frac{\sigma_1^2}{N_1} + \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\left(\alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k\right) \le  (1-\theta)\epsilon^2,\\[2pt]
       & -N_1\le 0,\quad  N_{k-1}-N_k\le 0, \quad k=2\ldots,K.
    \end{align}
\end{subequations}
The problem \eqref{eq:Optimization_pb_sample_size0} is an optimization problem in 
$N_1,\ldots, N_K\in \mathbb{R}$ and $\alpha_2,\ldots,\alpha_K\in \mathbb{R}$.

Because the weights $\alpha_2,\ldots,\alpha_K\in \mathbb{R}$ only enter in the variance constraint
\eqref{eq:Optimization_pb_sample_size0_var}, they can be solved for.
For fixed sample sizes $N_k$, weights that minimize the variance are
\begin{equation}    \label{eq:MFMC_SampleSize}
      \alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k}, \quad k \ge 2,
\end{equation}
and the corresponding variance is
\[
      \sigma_1^2 \left(  \frac{1}{N_1}  -  \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right) \rho_{1,k}^2 \right).
\]
If $N_1,\ldots, N_K\in \mathbb{R}$ and $\alpha_2,\ldots,\alpha_K\in \mathbb{R}$ is feasible for 
\eqref{eq:Optimization_pb_sample_size0}, then $N_1,\ldots, N_K\in \mathbb{R}$ and 
$\alpha_2^*,\ldots,\alpha_K^*\in \mathbb{R}$ given by \eqref{eq:MFMC_SampleSize} is also feasible for
\eqref{eq:Optimization_pb_sample_size0}.
Therefore, the optimization problem \eqref{eq:Optimization_pb_sample_size0} is equivalent to
\begin{subequations}\label{eq:Optimization_pb_sample_size}
    \begin{align}
    \min  \; &\sum\limits_{k=1}^K C_kN_k,\\
       \text{subject to } \; & \sigma_1^2 \left(  \frac{1}{N_1} 
                      -  \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right) \rho_{1,k}^2 \right) \le  (1-\theta)\epsilon^2,\\[2pt]
       & -N_1\le 0,\quad  N_{k-1}-N_k\le 0, \quad  k=2\ldots,K.
    \end{align}
\end{subequations}




%
Since the finite variance implicitly indicates $N_1 > 0$, the problem remains well-posed. The solution to this problem, which yields explicit expressions for the optimal real-valued sample sizes and weights, is presented in Theorem~\ref{thm:Sample_size_est}. The proof of Theorem~\ref{thm:Sample_size_est} is provided in the appendix.




%
\begin{theorem}[Optimal MFMC Sample Allocation]
\label{thm:Sample_size_est}
    Consider an ensemble of $K$ models $\{u_{h,k}\}_{k=1}^K$ each characterized by the standard deviation $\sigma_k$ 
    of its output, the correlation coefficient $\rho_{1,k}$ with the highest-fidelity model $u_{h,1}$, 
    and the computational cost per sample evaluation $C_k$. 
    If 
   \begin{subequations}\label{eq:Sample_size_real_assumptions}
   \begin{align}
       \label{eq:Sample_size_real_assumptions_a}
        & |\rho_{1,1}| > \cdots > |\rho_{1,K}|, & \text{(monotone correlations)} \\
      \label{eq:Sample_size_real_assumptions_b}
        & \frac{ \rho_{1,k}^2 - \rho_{1,k+1}^2 }{C_k} > \frac{ \rho_{1,k-1}^2 - \rho_{1,k}^2 }{C_{k-1}}, \quad k=2,\ldots,K, 
                                                                  & \text{(cost-correlation ratio)}
    \end{align}
    \end{subequations}
    where $\rho_{1,K+1} :=0$, then the solution of \eqref{eq:Optimization_pb_sample_size} is
    \begin{equation}  \label{eq:MFMC_SampleSize}
        N_k^* = \frac{\sigma_1^2}{(1-\theta)\epsilon^2}\sqrt{\frac{\rho_{1,k}^2 - \rho_{1,k+1}^2}{C_k}}
                      \sum_{j=1}^K\sqrt{C_j(\rho_{1,j}^2 - \rho_{1,j+1}^2)}, \quad k= 1, \ldots, K,
    \end{equation}
    the  variance of the resulting MFMC estimator \eqref{eq:MFMC_variance} is
    \begin{equation}  \label{eq:MFMC_variance_optimal}
        \mathbb{V}\big[A^{\text{MF}}\big] 
        = \sigma_1^2\sum_{k=1}^K\frac{\rho_{1,k}^2 - \rho_{1,k+1}^2}{N_k^*},
    \end{equation}
    and its total computational cost is
    \begin{equation}\label{eq:MFMC_sampling_cost}
        \mathcal{W}^\text{MF} 
        = \sum_{k=1}^K C_k N_k^* 
        = \frac{\sigma_1^2}{(1-\theta)\epsilon^2}\left(\sum_{k=1}^K\sqrt{C_k (\rho_{1,k}^2 - \rho_{1,k+1}^2)}\right)^2.
    \end{equation}
\end{theorem}

In practical implementation, the correlation coefficients $\rho_{1,k}$ and computational costs $C_k$ are typically unknown a priori and must be estimated via pilot sampling. Additionally, the theoretically optimal sample sizes $N_k^* \in \mathbb{R}$ require integer rounding for implementation. Departing from the conditional rounding strategy in \cite{GrGuJuWa:2023, PeWiGu:2016} (floor function when $N_k^* \ge 1$, ceiling otherwise), we adopt a uniform rounding scheme: all sample sizes are rounded up using the ceiling function $\lceil N_k^* \rceil$. 
This approach guarantees $\mathbb{V}[A^{\mathrm{MF}}] \leq (1-\theta)\epsilon^2$ since increased sample sizes reduce estimator variance in \eqref{eq:MFMC_variance_optimal}. The computational cost after rounding satisfies
%
\begin{equation}\label{eq:sampling_cost_bound}
    \sum_{k=1}^K C_k N_k^*\le \sum_{k=1}^K C_k \left\lceil N_k^*\right\rceil<\sum_{k=1}^K C_k N_k^* + \sum_{k=1}^K C_k,
\end{equation}
%
where the additive overhead $\sum_{k=1}^K C_k$ arises from the bound $N_k^*\le \lceil N_k^*\rceil< N_k^*+1$. Under Theorem~\ref{thm:Sample_size_est}'s cost-correlation ratio assumption, optimal sample sizes exhibit strict monotonicity $N_1^* < \cdots < N_K^*$. We impose the feasibility condition $\sum_{k=1}^K C_k N_k^* \geq \sum_{k=1}^K C_k$ to exclude degenerate cases where $N_k^* < 1$ for all $k$ models; in such cases, the additive rounding overhead is asymptotically dominated by $\sum_{k=1}^K C_k N_k^*$. Consequently, integer-rounded cost preserve the asymptotic scaling of \eqref{eq:MFMC_sampling_cost}.

% define $B_k := C_k(\rho_{1,k}^2 - \rho_{1,k+1}^2)$ for $k=1,\dots,K$ with $\rho_{1,K+1} = 0$. Condition (ii) of Theorem~\ref{thm:Sample_size_est} implies
% % Substituting into the sampling cost expression,  \eqref{eq:MFMC_sampling_cost} becomes
% % %
% % \begin{equation*}\label{eq:MFMC_sampling_cost_2}
% %     \mathcal{W}^{\text{MF}} = \sum_{k=1}^K C_k N_k^* = \frac{\sigma_1^2}{(1-\theta)\epsilon^2}\left(\sum_{k=1}^K\sqrt{B_k} \right)^2.
% % \end{equation*}
% %
% % The quantity $B_k$ depends on the product of the cost per sample $C_k$ and the difference between two successive correlations $(\rho_{1,k}^2 - \rho_{1,k+1}^2)$. Depending on how these components interact, $B_k$ may decay, grow, or remain constant as $k$ increases.
% %
% \begin{equation}
% \label{eq:Bk_Ck_decay_rate}
%     \frac{\sqrt{B_k}}{\sqrt{B_{k-1}}}>\frac{C_k}{C_{k-1}}, \quad k=2,\ldots,K.
% \end{equation}
% %
% This inequality induces a strictly increasing sequence $\{\sqrt{B_k}/C_k\}_{k=1}^K$. Consequently, as $K \to \infty$, the sequence $\sqrt{B_k}$ decays slower (or grows faster) than $C_k$ in relative terms. Combined with the feasibility condition ($\sum_{k=1}^K C_kN_k^*\ge \sum_{k=1}^K C_k$), this ensures the overhead $\sum_{k=1}^K C_k$ is asymptotically negligible compared to $\sum_{k=1}^K C_k N_k^* \propto \left( \sum_{k=1}^K \sqrt{B_k} \right)^2$. 


% This inequality implies that, in the asymptotic regime where $K$ is large,  the sequence $\sqrt{B_k}$ decays more slowly -- or grows more rapidly -- than the cost sequence $C_k$, regardless of the specific trend of $\sqrt{B_k}$. \JLcolor{Using this fact and the assumption that $\sum_{k=1}^K C_kN_k^*\ge \sum_{k=1}^K C_k$}, the additive overhead term $\sum_{k=1}^K C_k$ in the cost bounds becomes asymptotically negligible relative to the leading-order term $\sum_{k=1}^K C_kN_k^*$. 


% Using the fact that $N_k$ increases and the value of $\alpha_k$, we observe that the MFMC estimator variance $\mathbb{V}\left(A^{\text{MFMC}}\right)$ in \eqref{eq:MFMC_variance2} always decreases as the model number $K$ increases. This reflects the fact that the low fidelity models are used as control variates to reduce the variance of the high fidelity model. However, this $K$ cannot be arbitrarily large, since the first summation term in \eqref{eq:MFMC_sampling_cost} grows, the second summation reflect the variance decay of the MFMC estimator. Thus this is a tie between these two terms. If $K$ is sufficiently large,  in order to achieve an optimal sampling cost, we need to study the decay and growth of these two terms. We will choose the $K$ such that the product of two summation terms in \eqref{eq:MFMC_sampling_cost} is minimum, i.e. If $K$ is sufficiently large, we need to find $K\in \mathbb{N}$ such that 
% \begin{equation}\label{eq:Optimal_K}
%    K = \text{argmin} \sum_{k=1}^K\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}\sum_{k=1}^K\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2. 
% \end{equation}
The efficiency gain relative to standard Monte Carlo is quantified through the cost ratio
%
\begin{equation}\label{eq:MFMC_sampling_cost_efficiency}
    \xi(\boldsymbol{\rho}) = \frac{\mathcal{W}^\text{MF}}{\mathcal{W}^\text{MC}} = \frac{1}{C_1} \left(\sum_{k=1}^K\sqrt{C_k\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)}\right)^2,
\end{equation}
%
where $\boldsymbol{\rho} = (\rho_{1,1},\ldots, \rho_{1,K})$ is the correlation coefficient vector and smaller $\xi$ indicates greater efficiency gains for the MFMC estimator.


% Further more, we observe that
% \begin{align*}
%     \mathcal{W}_\text{MC}\mathbb{V}\left(A^{\text{MC}}\right) &=\frac{C_1\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{U}^2},\\
%  \mathcal{W}_\text{MFMC}\mathbb{V}\left(A^{\text{MFMC}}\right) &=  \frac{\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{U}^2}\sum_{k=1}^K\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}\sum_{k=1}^K\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2.
% \end{align*}
% This implies that if both Monte Carlo and multifidelity Monte Carlo have  a same sampling cost, then $\mu=  \mathbb{V}\left(A^{\text{MFMC}}\right)/\mathbb{V}\left(A^{\text{MC}}\right)$. Therefore, 

