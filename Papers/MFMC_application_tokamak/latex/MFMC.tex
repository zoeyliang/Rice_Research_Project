%!TEX root = main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% ====================================================
\section{Multi-fidelity Monte Carlo}\label{sec:MFMC}
% ====================================================
%
MLMC methods \cite{MBGiles_2015a,SHeinrich_2001a} and 
MFMC methods  \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a, BPeherstorfer_KWillcox_MDGunzburger_2018b} 
build on control variate techniques to reduce the statistical error below $(1-\theta)\epsilon^2$  
at a computational cost less than \eqref{eq:MC-work}.
While both MFMC and MLMC share the goal of variance reduction, they differ in structure and sampling.
In MLMC, corrections consisting of differences between successive levels are added to the coarse level, 
each difference between successive levels is computed with independent samples,
and a decreasing number of samples is used, the higher the level (i.e., fidelity) of the models.
MFMC, by contrast, adds corrections of low-fidelity models to the high-fidelity model and incorporates increasing numbers of inexpensive, low-fidelity samples. 
A key distinction is that MFMC reuses samples across fidelity levels. 

Next we briefly review the core principles underlying multi-fidelity Monte Carlo, drawing on \cite{BPeherstorfer_KWillcox_MDGunzburger_2016a}.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Multi-fidelity Monte Carlo revisited}
The MFMC framework combines a high-fidelity model $u_{h,1} = u_h \in L_{\mathbb{P}}^2(W, \cU)$ 
with a series of lower-fidelity models $u_{h,k} \in L_{\mathbb{P}}^2(W, \cU)$, $k =1,\dots, K$.
Here, as we will make precise below,
fidelity is not related to an almost everywhere pointwise error $\| u_{h,1}(\omega) - u_{h,k}(\omega) \|_U$,
but is related to the correlation between the $k$-th model $u_{h,k}$ and the high-fidelity model $u_{h,1}$.
We assume that as $k$ increases, the fidelity of the $k$-th model decreases and also
that the computational cost of evaluating the $k$-th model at a sample decreases.



For each  $u_{h,k}(\omega)$  and each pair of $u_{h,k}(\omega)$ and $u_{h,j}(\omega)$, 
we define the variance and the Pearson correlation coefficient as
\begin{equation*}
    \sigma_k^2 = \mathbb{V}\left[u_{h,k}(\omega)\right],\qquad \rho_{k,j} 
                       = \frac{\text{Cov}\left[ u_{h,k}(\omega), u_{h,j}(\omega)\right]}{\sigma_k\sigma_j}, \quad k,j=1,\dots, K,
\end{equation*}
where the covariance is 
$\text{Cov}[u_{h,k}, u_{h,j}] := \mathbb{E}[\langle u_{h,k} - \mathbb{E}[u_{h,k}], u_{h,j} - \mathbb{E}[u_{h,j}]\rangle_U]$.
By definition, $\rho_{k,k}=1$. 

Given $N_1 < N_2 < \ldots < N_K$ and i.i.d.\ samples $\omega^{(1)}, \ldots, \omega^{(N_K)}$,
the MFMC estimator $A^{\text{MF}}$ combines an MC estimate of the high-fidelity model with differences 
of MC estimates of the lower-fidelity level models.
The MFMC estimator is defined as
\begin{equation}\label{eq:MFMC_estimator}
    A^{\text{MF}}
     := A^{\text{MC}}_{1,N_1} + \sum_{k=2}^K \alpha_k\left(A^{\text{MC}}_{k,N_k} - A^{\text{MC}}_{k,N_{k-1}} \right),
\end{equation}
where
\begin{equation}\label{eq:MFMC_estimator_MCk}
     A^{\text{MC}}_{k,M} :=  \frac{1}{M} \sum_{i=1}^{M}   u_{h,k}(\omega^{(i)}), \quad M \in \{ N_{k-1}, N_k \},
\end{equation}
and $\alpha_k\in \mathbb{R}$ are weights that will be determined below.
Note that the $N_{k-1}$ evaluations $u_{h,k}(\omega^{(i)})$, $i = 1, \ldots,  N_{k-1}$,
used in  $A^{\text{MC}}_{k,N_{k-1}}$ are reused in  the computation of $A^{\text{MC}}_{k,N_k}$.
are reused. This reuse introduces statistical dependence between 
$A^{\text{MC}}_{k,N_{k-1}}$ and $A^{\text{MC}}_{k,N_k}$. 
If we denote the MC average over the $N_k - N_{k-1}$ samples $u_{h,k}(\omega^{(i)})$ 
not included in $A^{\text{MC}}_{k,N_{k-1}}$ by
\[
     A^{\text{MC}}_{k,N_k \backslash N_{k-1}}
      =  \frac{1}{N_k-N_{k-1}}  \sum_{i=N_{k-1}+1}^{N_k}   u_{h,k}(\omega^{(i)}), 
\]
the MFMC estimator \eqref{eq:MFMC_estimator} can be written as
\begin{equation}\label{eq:MFMC_estimator_independent}
    A^{\text{MF}} 
    = A^{\text{MC}}_{1,N_1} 
      +  \sum_{k=2}^K \alpha_k\left(1-\frac{N_{k-1}}{N_k}\right)
                               \left(A_{k,N_k\backslash N_{k-1}}^{\text{MC}}-A_{k,N_{k-1}}^{\text{MC}}\right).
\end{equation}
In this formulation, the two terms in each correction are evaluated on independent sample sets, 
which simplifies variance analysis. 
We also express the MFMC estimator in compact form
\begin{equation*}\label{eq:MFMC_estimator_Correction}
          A^{\text{MF}} = Y_1 + \sum_{k=2}^K \alpha_k Y_k,
\end{equation*}
where the correction terms $Y_k$ are defined as
\begin{equation} \label{eq:MFMC_Yk}
       Y_1 := A^{\text{MC}}_{1,N_1},\qquad 
       Y_k := A^{\text{MC}}_{k,N_k} - A^{\text{MC}}_{k,N_{k-1}}
               =\left(1-\frac{N_{k-1}}{N_k}\right)
                 \left(A_{k,N_k\backslash N_{k-1}}^{\text{MC}}-A_{k,N_{k-1}}^{\text{MC}}\right), \quad k=2\ldots, K.
\end{equation}
%
Because $\mathbb{E}\big[ A^{\text{MC}}_{k,M} \big] = \mathbb{E}\big[ u_{h,k} \big]$, $M \in \{ N_{k-1}, N_k \}$,
$\mathbb{E}[Y_1] =  \mathbb{E}[u_{h,1}]  = \mathbb{E}\big[ u_h \big]$ and  $\mathbb{E}[Y_k] = 0$ for $k\ge 2$.
Consequently, for any selection of weights $\alpha_k$, 
the MFMC estimator is  unbiased, satisfying $\mathbb{E}[A^{\text{MF}}] =  \mathbb{E}[u_h]$. 


The variances of the correction terms $Y_k$ are
\begin{equation}\label{eq:Var_Yk}
    \mathbb{V}\left[Y_1\right] = \frac{\sigma_1^2}{N_1}, \quad \mathbb{V}\left[Y_k\right] = \left(1-\frac{N_{k-1}}{N_k}\right)^2\left(\frac{\sigma_k^2}{N_{k-1}}+\frac{\sigma_k^2}{N_k-N_{k-1}}\right) = \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\sigma_k^2.
\end{equation}
The corrections term $Y_k$, $k\ge 2$, are correlated with the high-fidelity estimator $Y_1$.
However, although $Y_k$ and $Y_j$, $2\le k<j \le K$, share overlapping sample sets and are therefore statistically dependent, they are uncorrelated.

\begin{lemma}\label{lemma:Y_k_Y_j}
Let $2\le k<j\le K$. Then 
  The correction terms $Y_k$ and $Y_j$, $2\le k<j \le K$, defined in \eqref{eq:MFMC_Yk} are uncorrelated,
  $\operatorname{Cov} [Y_k,Y_j ]=0$,  $2\le k<j \le K$.
\end{lemma}
The proof of Lemma~\ref{lemma:Y_k_Y_j} is given in Appendix~\ref{sec:proof_lemma:Y_k_Y_j}.

 Using the covariance identity derived from \cite[Lemma~3.2]{PeWiGu:2016}, yields
%
\begin{equation}\label{eq:Cov_Yk}
% \text{Cov}(Y_k,Y_j) =0,\quad \text{for } \;2\le k<j \le K,\qquad 
\text{Cov}[Y_1,Y_k] = - \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\rho_{1,k}\sigma_1\sigma_k, \quad \text{for } \; k\ge 2.
\end{equation}
%
Combining \eqref{eq:Var_Yk} and \eqref{eq:Cov_Yk}, the total variance of the MFMC estimator is 
%
\begin{align}
    \nonumber
    \mathbb{V}\left[A^{\text{MF}}\right] &= \mathbb{V}\left[Y_1\right] + \mathbb{V}\left[\sum_{k=2}^K \alpha_kY_k\right]+2\;\text{Cov}\left[Y_1,\sum_{k=2}^K \alpha_k Y_k \right],\\
    \nonumber
    &=\mathbb{V}\left[Y_1\right] + \sum_{k=2}^K \alpha_k^2 \mathbb{V}\left[Y_k\right]+2\sum_{2\le k<j\le K} \alpha_k\alpha_j\; \text{Cov}[Y_k,Y_j] +2\sum_{k=2}^K \alpha_k\;\text{Cov}\left[Y_1, Y_k\right],\\
    % \nonumber
    % &=\mathbb{V}\left(Y_1\right) + \sum_{k=2}^K \alpha_k^2 \mathbb{V}\left(Y_k\right) +2\sum_{k=2}^K \alpha_k\;\text{Cov}\left(Y_1, Y_k\right),\\
    \label{eq:MFMC_variance}
    &=\frac{\sigma_1^2}{N_1} + \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)\left(\alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k\right).
\end{align}
%
The normalized mean square error of the multi-fidelity Monte Carlo estimator, $\mathcal{E}_{A^{\text{MF}}}^2$, quantifies its accuracy and is decomposed into two components -- the bias error $\mathcal{E}_{\text{Bias}}^2$ and the statistical error $\mathcal{E}_{\text{Stat}}^2$, the decomposition is written as 
%
\[
\mathcal{E}_{A^{\text{MF}}}^2= \frac{\left\Vert\mathbb{E}[u]-\mathbb{E}\left[A^{\text{MF}}\right] \right\Vert_{U}^2+\mathbb E\left[\left\Vert\mathbb{E}\left[A^{\text{MF}}\right]-A^{\text{MF}} \right\Vert_{U}^2\right]}{\left\Vert\mathbb{E}[u] \right\Vert_{U}^2} =\frac{\left\Vert\mathbb{E}[u]-\mathbb{E}\left[A^{\text{MF}}\right] \right\Vert_{U}^2}{\left\Vert\mathbb{E}[u] \right\Vert_{U}^2}+ \frac{\mathbb{V}\left[A^{\text{MF}}\right]}{\left\Vert\mathbb{E}[u] \right\Vert_{U}^2}=\mathcal{E}_{\text{Bias}}^2 + \mathcal{E}_{\text{Stat}}^2,
\]
%
where the variance term $\mathbb{V}[A^{\text{MF}}]$  can be explicitly expressed using \eqref{eq:MFMC_variance}. 

\MH{Need to revise the following.
Multifidelity methods do not have a level and their costs shouldn't be estimated using $M_L$.} \JLcolor{$M_L$ is the spatial grid point nodes that is required to satisfy the discretization error of high fidelity model. }
A splitting ratio $\theta$ is introduced as before to balance the contributions between these two components. The spatial resolution required to achieve the biased tolerance $\theta \epsilon^2$ is determined by estimating the number of spatial grid points $M_L$ at refinement level $L$, given by
\MH{Multifidelity methods do not have a level and their costs shouldn't be estimated using $M_L$.} \JLcolor{You are correct that multifidelity methods, in general, do not rely on a notion of levels. However, in my later cost estimate (Theorem 2), the parameter L appears because the low-fidelity models are constructed using a spatial hierarchy. Thus, the cost estimate is specifically tailored to the class of multifidelity models I consider. My goal is to clarify why, in this setting, the cost scales like $\epsilon^{-1}$}
%
\begin{equation}
    \label{eq:SLSGC_MLS_SpatialGridsNo}
    M_L = M_0s^{-L} \ge \left(\frac{\theta\epsilon}{c_u}\right)^{-\frac 1 {\alpha}} \qquad \text{ and } \qquad     L = \left\lceil \frac{1}{\alpha}\log_s \left(\frac{c_u M_0^\alpha}{\theta\epsilon}\right) \right\rceil,
\end{equation}
%
where $M_0$ is the number of grid points at the coarsest level, $s>1$ is the spatial refinement factor, $\alpha$ represents the convergence rate of the spatial discretization, and $c_u$ is a constant characterizing the discretization scheme. To determine the optimal sample sizes $N_k$ and control variate weights $\alpha_k$ in the MFMC estimator \eqref{eq:MFMC_estimator_independent}, we express the total computational cost for the MFMC estimator
%
\[
\mathcal{W}^{\text{MF}} = \sum_{k=1}^K C_kN_k,
\]
%
where $C_k$ is the cost of generating a single sample of model $u_{h,k}$, and $N_k$ is the corresponding sample count. Unlike previous formulations \cite{PeWiGu:2016} that derive sample sizes based on a fixed computational budget, our approach directly expresses the sample sizes and computational resources in terms of the desired accuracy $\epsilon$. This formulation offers greater flexibility in applications where accuracy targets are more relevant than rigid cost constraints. We formulate an optimization problem to determine the optimal sample sizes $N_k$ and weights $\alpha_k$ by minimizing the total sampling cost $\mathcal{W}^{\text{MF}}$, subject to three constraints. First, the normalized statistical error $\mathcal{E}_{\text{Stat}}^2$ enforces the desired estimator accuracy $(1-\theta)\epsilon^2$. Second,  the monotonicity constraints $N_{k-1}\le N_k$ for $k=2,\ldots, K$ ensures consistent sample reuse across fidelity levels. Third, all sample sizes must be non-negative. This leads to the following constrained optimization problem
%
\begin{equation}\label{eq:Optimization_pb_sample_size}
    \begin{array}{ll}
    \min \limits_{\begin{array}{c}\scriptstyle N_1,\ldots, N_K\in \mathbb{R} \\[-4pt]
\scriptstyle \alpha_2,\ldots,\alpha_K\in \mathbb{R}
\end{array}} &\displaystyle\sum\limits_{k=1}^K C_kN_k,\\
       \;\,\text{subject to} &\mathbb{V}\left[A^{\text{MF}}\right]- \epsilon_{\text{tar}}^2 = 0,\\[2pt]
       &\displaystyle -N_1\le 0,\quad \displaystyle N_{k-1}-N_k\le 0, \;\; k=2\ldots,K.
    \end{array}
\end{equation}
%
Since the finite variance implicitly indicates $N_1 > 0$, the problem remains well-posed. The solution to this problem, which yields explicit expressions for the optimal real-valued sample sizes and weights, is presented in Theorem~\ref{thm:Sample_size_est}. The proof of Theorem~\ref{thm:Sample_size_est} is provided in the appendix.




%
\begin{theorem}[Optimal MFMC Sample Allocation]
\label{thm:Sample_size_est}
Consider an ensemble of $K$ models $\{u_{h,k}\}_{k=1}^K$ each characterized by the standard deviation $\sigma_k$ of its output, the correlation coefficient $\rho_{1,k}$ with the highest-fidelity model $u_{h,1}$, and the computational cost per sample evaluation $C_k$. Define $\Delta_k = \rho_{1,k}^2 - \rho_{1,k+1}^2$ for $k = 1, \dots, K$, with the boundary convention $\rho_{1,K+1} = 0$. Assume the following conditions hold
%
\begin{alignat*}{3}
&(i)\;\; \textit{Correlation monotonicity}: \quad && |\rho_{1,1}| > \cdots > |\rho_{1,K}|, \\ 
&(ii)\;\; \textit{Cost-correlation ratio}: \quad && \frac{\Delta_k}{C_k} > \frac{\Delta_{k-1}}{C_{k-1}}, \quad k=2,\ldots,K. 
\end{alignat*}
%
Under these assumptions, the solution to the optimization problem \eqref{eq:Optimization_pb_sample_size} yields optimal weights $\alpha_k^*$ and sample sizes $N_k^*$
%
\begin{align}
    % \label{eq:MFMC_coefficients}
    % &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\\
    \label{eq:MFMC_SampleSize}
    &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\qquad \;N_k^*=\frac{\sigma_1^2}{\epsilon_\text{tar}^2}\sqrt{\frac{\Delta_k}{C_k}}\sum_{j=1}^K\sqrt{C_j\Delta_{j}}.
\end{align}
%
The resulting MFMC estimator \eqref{eq:MFMC_variance} achieves a variance of
%
\begin{equation}
\label{eq:MFMC_variance_optimal}
\mathbb{V}\left[A^{\text{MF}}\right] =
% \frac{\sigma_1^2}{N_1^*} - \sum_{k=2}^K \left(\frac{1}{N_{k-1}^*} - \frac{1}{N_k^*}\right)\rho_{1,k}^2\sigma_1^2=
\sigma_1^2\sum_{k=1}^K\frac{\Delta_k}{N_k^*},
\end{equation}
%
with total computational cost
%
\begin{equation}\label{eq:MFMC_sampling_cost}
    \mathcal{W}^\text{MF} = \sum_{k=1}^K C_k N_k^* = \frac{\sigma_1^2}{\epsilon_{\text{tar}}^2}\left(\sum_{k=1}^K\sqrt{C_k\Delta_k}\right)^2.
\end{equation}
%
\end{theorem}

In practical implementation, the correlation coefficients $\rho_{1,k}$ and computational costs $C_k$ are typically unknown a priori and must be estimated via pilot sampling. Additionally, the theoretically optimal sample sizes $N_k^* \in \mathbb{R}$ require integer rounding for implementation. Departing from the conditional rounding strategy in \cite{GrGuJuWa:2023, PeWiGu:2016} (floor function when $N_k^* \ge 1$, ceiling otherwise), we adopt a uniform rounding scheme: all sample sizes are rounded up using the ceiling function $\lceil N_k^* \rceil$. This approach guarantees $\mathbb{V}[A^{\mathrm{MF}}] \leq \epsilon_{\mathrm{tar}}^2$ since increased sample sizes reduce estimator variance in \eqref{eq:MFMC_variance_optimal}. The computational cost after rounding satisfies
%
\begin{equation}\label{eq:sampling_cost_bound}
    \sum_{k=1}^K C_k N_k^*\le \sum_{k=1}^K C_k \left\lceil N_k^*\right\rceil<\sum_{k=1}^K C_k N_k^* + \sum_{k=1}^K C_k,
\end{equation}
%
where the additive overhead $\sum_{k=1}^K C_k$ arises from the bound $N_k^*\le \lceil N_k^*\rceil< N_k^*+1$. Under Theorem~\ref{thm:Sample_size_est}'s cost-correlation ratio assumption, optimal sample sizes exhibit strict monotonicity $N_1^* < \cdots < N_K^*$. We impose the feasibility condition $\sum_{k=1}^K C_k N_k^* \geq \sum_{k=1}^K C_k$ to exclude degenerate cases where $N_k^* < 1$ for all $k$ models; in such cases, the additive rounding overhead is asymptotically dominated by $\sum_{k=1}^K C_k N_k^*$. Consequently, integer-rounded cost preserve the asymptotic scaling of \eqref{eq:MFMC_sampling_cost}.

% define $B_k := C_k(\rho_{1,k}^2 - \rho_{1,k+1}^2)$ for $k=1,\dots,K$ with $\rho_{1,K+1} = 0$. Condition (ii) of Theorem~\ref{thm:Sample_size_est} implies
% % Substituting into the sampling cost expression,  \eqref{eq:MFMC_sampling_cost} becomes
% % %
% % \begin{equation*}\label{eq:MFMC_sampling_cost_2}
% %     \mathcal{W}^{\text{MF}} = \sum_{k=1}^K C_k N_k^* = \frac{\sigma_1^2}{\epsilon_{\text{tar}}^2}\left(\sum_{k=1}^K\sqrt{B_k} \right)^2.
% % \end{equation*}
% %
% % The quantity $B_k$ depends on the product of the cost per sample $C_k$ and the difference between two successive correlations $(\rho_{1,k}^2 - \rho_{1,k+1}^2)$. Depending on how these components interact, $B_k$ may decay, grow, or remain constant as $k$ increases.
% %
% \begin{equation}
% \label{eq:Bk_Ck_decay_rate}
%     \frac{\sqrt{B_k}}{\sqrt{B_{k-1}}}>\frac{C_k}{C_{k-1}}, \quad k=2,\ldots,K.
% \end{equation}
% %
% This inequality induces a strictly increasing sequence $\{\sqrt{B_k}/C_k\}_{k=1}^K$. Consequently, as $K \to \infty$, the sequence $\sqrt{B_k}$ decays slower (or grows faster) than $C_k$ in relative terms. Combined with the feasibility condition ($\sum_{k=1}^K C_kN_k^*\ge \sum_{k=1}^K C_k$), this ensures the overhead $\sum_{k=1}^K C_k$ is asymptotically negligible compared to $\sum_{k=1}^K C_k N_k^* \propto \left( \sum_{k=1}^K \sqrt{B_k} \right)^2$. 


% This inequality implies that, in the asymptotic regime where $K$ is large,  the sequence $\sqrt{B_k}$ decays more slowly -- or grows more rapidly -- than the cost sequence $C_k$, regardless of the specific trend of $\sqrt{B_k}$. \JLcolor{Using this fact and the assumption that $\sum_{k=1}^K C_kN_k^*\ge \sum_{k=1}^K C_k$}, the additive overhead term $\sum_{k=1}^K C_k$ in the cost bounds becomes asymptotically negligible relative to the leading-order term $\sum_{k=1}^K C_kN_k^*$. 


% Using the fact that $N_k$ increases and the value of $\alpha_k$, we observe that the MFMC estimator variance $\mathbb{V}\left(A^{\text{MFMC}}\right)$ in \eqref{eq:MFMC_variance2} always decreases as the model number $K$ increases. This reflects the fact that the low fidelity models are used as control variates to reduce the variance of the high fidelity model. However, this $K$ cannot be arbitrarily large, since the first summation term in \eqref{eq:MFMC_sampling_cost} grows, the second summation reflect the variance decay of the MFMC estimator. Thus this is a tie between these two terms. If $K$ is sufficiently large,  in order to achieve an optimal sampling cost, we need to study the decay and growth of these two terms. We will choose the $K$ such that the product of two summation terms in \eqref{eq:MFMC_sampling_cost} is minimum, i.e. If $K$ is sufficiently large, we need to find $K\in \mathbb{N}$ such that 
% \begin{equation}\label{eq:Optimal_K}
%    K = \text{argmin} \sum_{k=1}^K\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}\sum_{k=1}^K\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2. 
% \end{equation}
The efficiency gain relative to standard Monte Carlo is quantified through the cost ratio
%
\begin{equation}\label{eq:MFMC_sampling_cost_efficiency}
    \xi(\boldsymbol{\rho}) = \frac{\mathcal{W}^\text{MF}}{\mathcal{W}^\text{MC}} = \frac{1}{C_1} \left(\sum_{k=1}^K\sqrt{C_k\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)}\right)^2,
\end{equation}
%
where $\boldsymbol{\rho} = (\rho_{1,1},\ldots, \rho_{1,K})$ is the correlation coefficient vector and smaller $\xi$ indicates greater efficiency gains for the MFMC estimator.


% Further more, we observe that
% \begin{align*}
%     \mathcal{W}_\text{MC}\mathbb{V}\left(A^{\text{MC}}\right) &=\frac{C_1\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{U}^2},\\
%  \mathcal{W}_\text{MFMC}\mathbb{V}\left(A^{\text{MFMC}}\right) &=  \frac{\sigma_1^2}{\left\Vert\mathbb{E}(f_1) \right\Vert_{U}^2}\sum_{k=1}^K\sqrt{\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)C_k}\sum_{k=1}^K\left(\sqrt{\frac{C_k}{\rho_{1,k}^2 - \rho_{1,k+1}^2}} - \sqrt{\frac{C_{k-1}}{\rho_{1,{k-1}}^2 - \rho_{1,k}^2}}\right)\rho_{1,k}^2.
% \end{align*}
% This implies that if both Monte Carlo and multifidelity Monte Carlo have  a same sampling cost, then $\mu=  \mathbb{V}\left(A^{\text{MFMC}}\right)/\mathbb{V}\left(A^{\text{MC}}\right)$. Therefore, 

