%!TEX root = main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ====================================================
\section{Appendix}\label{sec:Appendix}
% ====================================================
\subsection{Proof of Lemma~\ref{lemma:Y_k_Y_j} }   \label{sec:proof_lemma:Y_k_Y_j}


\begin{proof}
Fix indices $2\le k<j\le K$. Since $N_{k-1}\le N_k\le N_{j-1}$, we can partition the $N_{j-1}$ samples into three mutually disjoint subsets with sizes $N_{k-1}$, $N_{k}-N_{k-1}$, and $N_{j-1} - N_{k}$. The samples in these three subsets are mutually independent. From the definition of $Y_k$ in \eqref{eq:MFMC_Yk},  the covariance between $Y_k$ and $Y_j$ is then given by
\begin{align*}
    \operatorname{Cov}\left[Y_k,Y_j\right] &= \left(\frac{N_{k-1}}{N_k}-1\right) \left(\frac{N_{j-1}}{N_j}-1\right)\operatorname{Cov}\left[A_{k, N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}} - A_{j,N_{j}\backslash N_{j-1}}^{\text{MC}}\right]\\
    & = M \left(\operatorname{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}}\right] - \operatorname{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j}\backslash N_{j-1}}^{\text{MC}}\right] \right)\\
    & = M \operatorname{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}}\right].
\end{align*}
where we define $M = (N_{k-1}/N_k-1) (N_{j-1}/N_j-1)$. The second term in the covariance vanishes due to independence between the samples used in $A_{j,N_{j}\backslash N_{j-1}}^{\text{MC}}$ and those in $Y_k$. Next, we express $A_{j,N_{j-1}}^{\text{MC}}$ as a weighted Monte Carlo estimator over the three disjoint sample subsets
%
\begin{equation*}
    A_{j,N_{j-1}}^{\text{MC}} = \frac{N_{k-1}}{N_{j-1}}A_{j,N_{k-1}}^{\text{MC}} + \frac{N_k - N_{k-1}}{N_{j-1}} A_{j,N_{k}\backslash N_{k-1}}^{\text{MC}} + \frac{N_{j-1} - N_k}{N_{j-1}} A_{j,N_{j-1}\backslash N_{k}}^{\text{MC}}.
\end{equation*}
%
Substituting this expansion into the covariance expression yields
%
\begin{align*}
    % \frac{\operatorname{Cov}\left[Y_k,Y_j\right]}{M} &= 
    &\operatorname{Cov}\left[A_{k,N_{k-1}}^{\text{MC}} - A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, A_{j,N_{j-1}}^{\text{MC}}\right]
    % &= \operatorname{Cov}\left[A_{N_{k-1}}^k, \frac{N_{k-1}}{N_{j-1}}A_{N_{k-1}}^j + \frac{N_k - N_{k-1}}{N_{j-1}} A_{N_{k}\backslash N_{k-1}}^j + \frac{N_{j-1} - N_k}{N_{j-1}} A_{N_{j-1}\backslash N_{k}}^j\right] \\
    % &- \operatorname{Cov}\left[ A_{N_{k}\backslash N_{k-1}}^k, \frac{N_{k-1}}{N_{j-1}}A_{N_{k-1}}^j + \frac{N_k - N_{k-1}}{N_{j-1}} A_{N_{k}\backslash N_{k-1}}^j + \frac{N_{j-1} - N_k}{N_{j-1}} A_{N_{j-1}\backslash N_{k}}^j\right]\\
    =\operatorname{Cov}\left[A_{k,N_{k-1}}^{\text{MC}}, \frac{N_{k-1}}{N_{j-1}}A_{j,N_{k-1}}^{\text{MC}}\right]-\operatorname{Cov}\left[ A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, \frac{N_k - N_{k-1}}{N_{j-1}} A_{j,N_{k}\backslash N_{k-1}}^{\text{MC}} \right]\\
    &=\frac{\operatorname{Cov}\left[N_{k-1}A_{k,N_{k-1}}^{\text{MC}}, N_{k-1} A_{j,N_{k-1}}^{\text{MC}}\right]}{N_{j-1}N_{k-1}}-\frac{\operatorname{Cov}\left[(N_k-N_{k-1}) A_{k,N_{k}\backslash N_{k-1}}^{\text{MC}}, (N_k - N_{k-1}) A_{j,N_{k}\backslash N_{k-1}}^{\text{MC}} \right]}{N_{j-1}(N_k-N_{k-1})}\\
    &=\frac{\operatorname{Cov}\left[\sum_{i=1}^{N_{k-1}}u_{k}^{(i)},\sum_{i=1}^{N_{k-1}}u_{j}^{(i)}\right]}{N_{j-1}N_{k-1}}
    -\frac{\operatorname{Cov}\left[\sum_{i=1}^{N_k-N_{k-1}}u_{k}^{(i)}, \sum_{i=1}^{N_k-N_{k-1}}u_{j}^{(i)}\right]}{N_{j-1}(N_k-N_{k-1})}\\
    &=\frac{\sum_{i=1}^{N_{k-1}}\operatorname{Cov}\left[u_{k}^{(i)},u_{j}^{(i)}\right]}{N_{j-1}N_{k-1}} -\frac{\sum_{i=1}^{N_k-N_{k-1}}\operatorname{Cov}\left[u_{k}^{(i)}, u_{j}^{(i)}\right]}{N_{j-1}(N_k-N_{k-1})}=\frac{N_{k-1}\rho_{k,j}\sigma_k\sigma_j}{N_{j-1}N_{k-1}}-\frac{(N_k-N_{k-1})\rho_{k,j}\sigma_k\sigma_j}{N_{j-1}(N_k-N_{k-1})}=0.
\end{align*}
\end{proof}

\subsection{Proof of Theorem \ref{thm:Sample_size_est}}

\begin{proof}
This proof establishes: (A) feasibility of the closed-form solution, (B) local optimality via KKT analysis and convexity, and (C) global optimality through block comparison, for problem~\eqref{eq:Optimization_pb_sample_size}.

\medskip
\noindent {\bf  Part A: Feasibility Verification.}

First, we verify that the proposed solution $(\alpha_k^*, N_k^*)$ satisfies all constraints of \eqref{eq:Optimization_pb_sample_size}.
\begin{enumerate}
    \item \textit{Variance constraint}: $N_k^*$ satisfies the variance constraint $\mathbb{V}[A^{\text{MF}}] = \epsilon_{\text{tar}}^2$.
    
    \item \textit{Monotonicity $N_k^* > N_{k-1}^*$ follows from assumption (ii)}:
    %
    \begin{align*}
    \frac{N_k^*}{N_{k-1}^*} &= \sqrt{ \frac{\Delta_k/C_k}{\Delta_{k-1}/C_{k-1}} } = \sqrt{ \frac{C_{k-1}}{C_k} \cdot \frac{\Delta_k}{\Delta_{k-1}}} > 1,\quad k=2,\ldots,K.
    \end{align*}
    %
    
    \item \textit{Positivity}: $N_k^* > 0$ since $\sigma_1 > 0$, $\epsilon_{\text{tar}} > 0$, and $\rho_{1,k}^2 - \rho_{1,k+1}^2 > 0$ by assumption (i).
\end{enumerate}

\medskip
\noindent {\bf Part B: Local optimality for block structures via KKT.}

The sample size for \eqref{eq:Optimization_pb_sample_size} may not increase monotonically, instead, constant values may appear within contiguous index blocks. For solutions with block-constant sample sizes, consider a partition $\mathscr{B} = \{B_i\}_{i=1}^q$ of model indices $\{1,\ldots,K\}$ defined by breakpoints $\ell_1=1 < \ell_2 < \cdots < \ell_q$ with $\ell_{q+1} = K+1$. Each block 
% 
\[
B_i = \{\ell_i, \ldots, \ell_{i+1}-1\}
\]
%
has constant sample size with increasing sample sizes across blocks satisfying
%
\begin{equation}\label{eq:sample_size_block}
    N_{\ell_i} < N_{\ell_{i+1}},\quad\quad  N_{\ell_i}=N_{\ell_i+1}=\ldots = N_{\ell_{i+1}-1} <N_{\ell_{i+1}}, \qquad \text{for}\;\;  i=1,\ldots,q-1.
\end{equation}
%
We now establish local optimality for solutions with block-structured constant sample sizes for partition $\mathscr{B}$. The Lagrangian with multiplier $\lambda_0$ for the variance constraint and $\lambda_{\ell_i}$ for monotonicity is
%
\begin{equation*}
L= \sum_{i=1}^q N_{\ell_i}\sum_{k\in B_i} C_k +\lambda_0 \left(\frac{\sigma_1^2}{N_{\ell_1}} + \sum_{i=2}^q \left(\frac{1}{N_{\ell_i-1}} - \frac{1}{N_{\ell_i}}\right)G_{\ell_i}- \epsilon_{\text{tar}}^2\right)-\lambda_{\ell_1} N_{\ell_1}+\sum_{i=2}^q\lambda_{\ell_{i}}(N_{\ell_{i-1}} - N_{\ell_{i}}),
\end{equation*}
%
where $G_k = \alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k$, with $\alpha_{\ell_1} = 1$.
% $ \alpha_{\ell_{q+1}} = 0$, $\lambda_{\ell_{q+1}} = 0$.
The KKT conditions includes
%
\[
\begin{array}{ll}
\left[\text{Stationarity}\right]&\partial L/\partial \alpha_{\ell_i}=0,\quad \partial L/{\partial N_{\ell_i}}=0,\quad i=1\ldots,q,\\
\left[\text{Primal feasibility}\right]&\mathbb{V}\left[A^{\text{MF}}\right]- \epsilon_{\text{tar}}^2 = 0, \\ 
\left[\text{Primal feasibility}\right] &-N_{\ell_1}\le 0,\qquad N_{\ell_{i-1}}-N_{\ell_i} \le 0, \quad i=2\ldots,q,\\ 
\left[\text{Dual feasibility}\right]  &\lambda_{\ell_i} \ge 0,\quad i=1\ldots,q, \\ 
\left[\text{Complementary slackness}\right]  &\lambda_{\ell_1} N_{\ell_1}=0,\qquad\lambda_{\ell_i}(N_{\ell_{i-1}}-N_{\ell_i})=0,\quad i=2\ldots,q.
\end{array}
\]
%
Stationarity of the Lagrangian with respect to $\alpha_{\ell_i}$ yields the optimal estimator coefficients
%
\begin{align}
\label{eq:partial_L_alpha_k}
    \frac{\partial L}{\partial \alpha_{\ell_i}}&=\lambda_0\left(\frac{1}{N_{\ell_i-1}} - \frac{1}{N_{\ell_i}}\right)\left(2\alpha_{\ell_i}\sigma_{\ell_i}^2 - 2\rho_{1,\ell_i}\sigma_1\sigma_{\ell_i}\right)=0,\quad\quad  \alpha_{\ell_i}^* = \frac{\rho_{1,\ell_i}\sigma_1}{\sigma_{\ell_i}}, \quad i=1,\dots,q.
    % \frac{\partial L}{\partial N_1}&=C_1 + \lambda_0\left(-\frac{\sigma_1^2}{N_1^2} - \frac{\alpha_2^2\sigma_2^2-2\alpha2\rho_{1,2}\sigma_1\sigma_2}{N_1^2}\right)-\lambda_1+\lambda_2,\\
    % \label{eq:partial_L_N_k}
    % \frac{\partial L}{\partial N_k}&=C_k+\lambda_0\left(\frac{G_k}{N_k^2}-\frac{G_{k+1}}{N_k^2}\right)-\lambda_k+\lambda_{k+1}, \quad k=1,\dots,K,
    % \frac{\partial L}{\partial N_K}&=C_K + \lambda_0\left(\frac{\alpha_K^2\sigma_K^2 - 2\alpha_K\rho_{1,K}\sigma_1\sigma_K}{N_K^2}\right)-\lambda_K.
\end{align}
%
Note that only block-start indices $k=\ell_i$ contribute nontrivially to the estimator weights in \eqref{eq:MFMC_Yk}; internal block terms with indices $k = \ell_i+1,\ldots, \ell_{i+1}-1$ vanish in the control variate structure and thus eliminate their explicit appearance in the final estimator. Using the fact that $N_{\ell_{i}-1} = N_{\ell_{i-1}}$ for $i\ge 2$ from \eqref{eq:sample_size_block}, we substitute the optimal coefficients $\alpha_k^*$ into the variance expression to obtain
%
\begin{equation}\label{eq:MFMC_var_convex}
    \mathbb{V}\left[A^{\text{MF}}\right] = \frac{\sigma_1^2}{N_1}+\sum_{i=2}^q
\left(\frac{1}{N_{\ell_{i}}}-\frac{1}{N_{\ell_{i}-1}}\right)\rho_{1,\ell_i}^2\sigma_1^2=\sum_{i=1}^{q} \frac{ \left(\rho_{1,\ell_i}^2-\rho_{1,\ell_{i+1}}^2\right)\sigma_1^2}{N_{\ell_i}}=\sigma_1^2\sum_{i=1}^{q} \frac{\Delta_{\ell_i}}{N_{\ell_i}},
\end{equation}
%
with $\rho_{1,\ell_{q+1}} = \rho_{1,K+1} = 0$. Stationarity with respect to \(N_{\ell_i}\) then gives
%
\[
\frac{\partial L}{\partial N_{\ell_i}} =\sum_{k\in B_i}C_{k} -  \lambda_0\frac{\sigma_1^2\Delta_{\ell_i}}{N_{\ell_i}^2}-\lambda_{\ell_{i}}+\lambda_{\ell_{i+1}}=0,\quad i = 1, \ldots,q-1.
\]
%
Complementary slackness implies $\lambda_{\ell_i} = 0$ for $i=1,\ldots, q$ between blocks due to strict monotonicity of sample size ($N_{\ell_{i}} < N_{\ell_{i+1}}$ for $i = 1, \ldots, q-1$). The sample sizes are then
%
\begin{equation}\label{eq:sample_size_1}
    N_{\ell_i} = \sigma_1\sqrt{\lambda_0} \sqrt{\frac{\Delta_{\ell_i}}{\sum_{k\in B_i} C_{k}}}, \;\text{ for }\; i=1,\ldots,q,
\end{equation}
%
Substituting $N_{\ell_i}$ in \eqref{eq:sample_size_1} into the variance expression \eqref{eq:MFMC_var_convex} yields
%
\begin{equation*} \label{eq:MFMC_variance2}
    \mathbb{V}\left[A^{\text{MF}}\right] = \frac{\sigma_1}{\sqrt{\lambda_0}}\sum_{i=1}^q\sqrt{\Delta_{\ell_i}\sum_{k\in B_i} C_k}.
\end{equation*}
%
Enforcing the variance constraint $\mathbb{V}[A^{\text{MF}}] = \epsilon_{\text{tar}}^2$ determines 
%
\[
\sqrt{\lambda_0}=\frac{\sigma_1}{\epsilon_{\text{tar}}^2} \sum_{i=1}^{q} \sqrt{\Delta_{\ell_i}\sum_{k\in B_i} C_{k}}.
\]
%
Substituting $\sqrt{\lambda_0}$ into \eqref{eq:sample_size_1} gives the sample sizes at each block start
%
\[
N_{\ell_i}^* = \frac{\sigma_1^2}{\epsilon_{\text{tar}}^2}\sqrt{\frac{\Delta_{\ell_i}}{\sum_{k\in B_i} C_{k}}}  \sum_{j=1}^{q} \sqrt{\Delta_{\ell_j}\sum_{k\in B_j} C_{k}} \;\text{ for }\; i=1,\ldots,q.
\]
%
The total sampling cost for a given block partition $\mathscr{B} = \{B_i\}_{i=1}^q$ is
%
\begin{equation*}
\mathcal{W}_{\mathscr{B}}^{\text{MF}} = \sum_{i=1}^q \sum_{k\in B_i} C_k N_k = \sum_{i=1}^q N_{\ell_i}\sum_{k\in B_i} C_k =\frac{\sigma_1^2}{\epsilon_{\text{tar}}^2}\left(\sum_{i=1}^{q} \sqrt{\Delta_{\ell_i}\sum_{k\in B_i} C_{k}}\right)^2.
\end{equation*}
%
This solution is globally optimal within the fixed partition $\mathscr{B}$ due to convexity of the following transformed optimization problem, though not necessarily optimal across different  partitions.

\medskip
\noindent {\bf Transformed optimization problem.}

For a fixed block partition $\mathscr{B}$ with optimal coefficients $\alpha_{\ell_i}^*$, we reformulate the sample size optimization problem \eqref{eq:Optimization_pb_sample_size} through the variable transformation $y_{\ell_i}^* = 1/N_{\ell_i}^*$, yielding the convex problem
%
\begin{equation}\label{eq:Optimization_pb_sample_size3}
    \begin{array}{ll}
    \min \limits_{\begin{array}{c}\scriptstyle y_{\ell_1},\ldots, y_{\ell_q}\in \mathbb{R}
\end{array}} &\displaystyle \sum_{i=1}^q \left(\sum_{k\in B_i}C_{k}\right)y_{\ell_i}^{-1},\\
       \;\,\text{subject to} &\displaystyle \sum_{i=1}^q \Delta_{\ell_i} y_{\ell_i}= \epsilon_{\text{tar}}^2,\\[2pt]
       &\displaystyle -y_{\ell_1}\le 0,\quad \displaystyle y_{\ell_i}-y_{\ell_{i-1}}\le 0, \;\; k=2\ldots,K.
    \end{array}
\end{equation}
%
The transformed problem is convex as established: the objective function is convex in $y_{\ell_i}$, the equality constraint is affine, and the feasible set defined by monotonicity constraints is convex.  

Since the first-order KKT conditions yield a candidate local minimizer $N_{\ell_i}^*$ for the original problem \eqref{eq:Optimization_pb_sample_size}, and the transformation preserves local optimality structures through its monotonicity \cite{AgVeDiBo:2018}, the transformed point $y_{\ell_i}^*$ constitutes a local minimizer for \eqref{eq:Optimization_pb_sample_size3}. Convexity of \eqref{eq:Optimization_pb_sample_size3} then guarantees this local minimizer is globally optimal. Applying the inverse transformation $N_{\ell_i}^* = 1/y_{\ell_i}^*$ then preserves global optimality for the original problem \eqref{eq:Optimization_pb_sample_size}. However, this solution is global optimal only within partition $\mathscr{B}$, it is not necessarily globally optimal for the original problem \eqref{eq:Optimization_pb_sample_size} over all possible partitions of blocks.




% % Note that by ensuring the condition $(ii)$ is satisfied, we can guarantee that $N_k^*$ increases strictly as $k$ grows. 
% The total cost $\mathcal{W}_{\text{block}}^{\text{MF}}$ associated with these optimal sample sizes in this block partition is
% %
% \begin{equation*}
% \mathcal{W}_{\text{block}}^{\text{MF}} = \sum_{i=1}^q \sum_{k=\ell_i}^{\ell_{i+1}-1} C_k N_k = \sum_{i=1}^q N_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_k =\frac{1}{\epsilon_{\text{tar}}^2}\left(\sum_{i=1}^{q} \sqrt{\Delta_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_{k}}\right)^2.
% \end{equation*}
% %





\medskip
\noindent {\bf Part C: Global optimality via block comparison.}

The global optimality of \eqref{eq:Optimization_pb_sample_size} is established by comparing costs across all possible block partitions. For any partition $\mathscr{B} = \{B_i\}_{i=1}^q$ with $q \leq K$ blocks, applying the Cauchy-Schwarz inequality to each block yields
%
\[
\sum_{k=1}^K \sqrt{\Delta_k C_k}  = \sum_{i=1}^K \sum_{k\in \{i\}} \sqrt{\Delta_k C_k} = \underbrace{\sum_{i=1}^q \sum_{k\in B_i} \sqrt{\Delta_k C_k}}_{\substack{\text{Samples in } B_i: \\ N_{\ell_i}<N_{\ell_{i}+1}<\ldots<N_{{\ell_{i+1}-1}}}} \le \sum_{i=1}^q \sqrt{ \sum_{k\in B_i}\Delta_k \sum_{k\in B_i} C_k } = \underbrace{\sum_{i=1}^q \sqrt{ \Delta_{\ell_i} \sum_{k\in B_i} C_k }}_{\substack{\text{Samples in } B_i: \\ N_{\ell_i}=N_{\ell_{i}+1}=\ldots=N_{{\ell_{i+1}-1}}}}.
\]
%
Equality holds if and only if either each block contains exactly one model ($q = K$), or $\Delta_k / C_k$ is constant within blocks. Let $\mathcal{W}^{\mathrm{MF}}$ denote the cost for the singleton partition ($q = K$) and $\mathcal{W}_{\mathscr{B}}^{\mathrm{MF}}$ for any coarser partition ($q < K$). The cost-correlation ratio assumption of Theorem \ref{thm:Sample_size_est} ensures the strict monotonically $\mathcal{W}^{\mathrm{MF}} < \mathcal{W}_{\mathscr{B}}^{\mathrm{MF}}$ for all $q < K$, indicating that the singleton partition is the unique global minimizer. This relation also yields reasonable sample sizes satisfying $N_{k-1}^* < N_k^*$, which maximally exploits the control variate structure by avoiding wasteful replication of low-fidelity models.

If $\Delta_{k-1}/C_{k-1} = \Delta_k/C_k$ within a block, the optimal solution forces $N_{k-1} = N_k$. 
This equality renders the correction terms for model $k$ in the MFMC estimator~\eqref{eq:MFMC_estimator} statistically redundant, inflating the cost without improving the reduction of variance. When $\Delta_{k-1}/C_{k-1} < \Delta_k/C_k$, sample sizes would decrease as fidelity decreases, contradicting the control variate structure of \eqref{eq:MFMC_estimator} and impairing variance reduction.





% This indicates that the singleton block structure on the left is always less than or equal to the block structure on the right. Equality holds if and only if each block contains exactly one model (i.e., $q=K$), strictly increasing sample size structure. However, we note that equality could also hold if multiple models in a block happen to have identical $\Delta_k/C_k$ ratios. But under general problem conditions, we cannot guarantee this coincidence. Therefore, the optimal block structure must have $q=K$. Let $\mathcal{W}^\text{MF}$ be the total cost when sample sizes increase strictly with model index $k$, i.e., without block repetitions. Thus $\mathcal{W}_{\text{block}}^{\text{MF}} \geq \mathcal{W}^{\text{MF}}$, confirming the singleton block structure ($q=K$) is globally optimal. 


% In this case, the optimal coefficients and sample sizes are
% %
% \begin{align*}
%     % \label{eq:MFMC_coefficients}
%     % &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\\
%     \label{eq:MFMC_SampleSize}
%     &\alpha_k^*=\frac{\rho_{1,k}\sigma_1}{\sigma_k},\;\; \;N_k^*=\frac{1}{\epsilon_\text{tar}^2}\sqrt{\frac{\Delta_k}{C_k}}\sum_{j=1}^K\sqrt{C_j\Delta_j},\;\; \mathcal{W}^\text{MF} = \sum_{k=1}^K C_k N_k^* = \frac{1}{\epsilon_{\text{tar}}^2}\left(\sum_{k=1}^K\sqrt{C_k\Delta_k}\right)^2\quad \text{with}\;\;\rho_{K+1}=0.
% \end{align*}
% %








% The total cost expression follows directly by substituting $N_k^*$ into $\sum C_k N_k^*$.


\end{proof}



\subsection{Proof of Theorem \ref{thm:Sample_cost_est}}

\begin{proof}
The minimal cost to achieve nMSE $\epsilon^2$ is given by
\begin{equation}\label{eq:MFMC_SC_cost_core}
\mathcal{W}^{\mathrm{MF}} \simeq \epsilon^{-2} \left( \sqrt{C_1 \left(1 - \rho_{1,i_2}^2\right)} + \sum_{k=2}^{K^*} \sqrt{C_{i_k} \left(\rho_{1,i_k}^2 - \rho_{1,i_{k+1}}^2\right)} \right)^2.
\end{equation}
Substituting conditions (ii)-(v) and using the geometric refinement $M_\ell \simeq s^\ell$, we obtain
%
\begin{align*}
\sqrt{C_1 \left(1 - \rho_{1,i_2}^2\right)} \simeq M_L^{(\gamma - \beta)/2} \simeq s^{L(\gamma - \beta)/2}, \qquad \sqrt{C_{i_k} \left(\rho_{1,i_k}^2 - \rho_{1,i_{k+1}}^2\right)} &\simeq M_{\ell(i_k)}^{(\gamma_1 - \beta_1)/2} \simeq s^{\ell(i_k)(\gamma_1 - \beta_1)/2}.
\end{align*}
%
Since $\ell(i_k) = L + 1 - i_k$ and the indices $\{i_k\}_{k=2}^{K}$ are ordered increasingly, the grid levels $\ell(i_k)$ are decreasing and $1\le \ell(i_k)\le L-1$. The summation over low-fidelity models satisfies
%
\[
\sum_{k=2}^{K} s^{\ell(i_k)(\gamma_1 - \beta_1)/2} \leq \sum_{p = L + 1 - i_{K}}^{L+1-i_2} s^{p (\gamma_1 - \beta_1)/2}\lesssim\sum_{p=1}^{L-1} s^{p\eta},
\]
%
where $\eta = (\gamma_1 - \beta_1)/2$. Using $M_L \simeq s^L \simeq \epsilon^{-1/\alpha}$ from (i) and \eqref{eq:SLSGC_MLS_SpatialGridsNo}, the geometric sum on the right is
%
\begin{align*}
\sum_{p=1}^{L-1} s^{p\eta} \;\simeq \;\begin{cases} 
1 & \eta < 0, \\
L & \eta = 0, \\
s^{L\eta} & \eta > 0, 
\end{cases}
\;\;\simeq \;\;
\begin{cases} 
1 & \beta_1 > \gamma_1, \\
|\log \epsilon| & \beta_1 = \gamma_1, \\
\epsilon^{-\eta/\alpha} & \beta_1 < \gamma_1,
\end{cases} \;\;: = \Phi(\epsilon).
\end{align*}
%
Combining with the high-fidelity term $s^{L(\gamma - \beta)/2} \simeq \epsilon^{(\beta - \gamma)/(2\alpha)}$ through \eqref{eq:MFMC_SC_cost_core} yields the cost
%
\[
\mathcal{W}^{\mathrm{MF}} \lesssim \epsilon^{-2} \left( c_1 \epsilon^{-\frac{\gamma-\beta}{2\alpha}} + c_2 \Phi(\epsilon) \right)^2,
\]
where $c_1$ and $c_2$ are the constants reflecting the cost per sample for the high and low-fidelity terms. 
%
% The simplified scaling $\mathcal{W}^{\mathrm{MF}} \lesssim \epsilon^{-2 - \frac{\gamma-\beta}{\alpha}}$ holds when $\epsilon^{-\frac{\gamma-\beta}{2\alpha}} \gg \Phi(\epsilon)$, which requires:
% \begin{itemize}
% \item $\beta_1 > \gamma_1$ and $\gamma > \beta$ (since $\epsilon^{-\frac{\gamma-\beta}{2\alpha}} \to \infty$)
% \item $\beta_1 = \gamma_1$ and $\epsilon^{-\frac{\gamma-\beta}{2\alpha}} \gg |\log \epsilon|$
% \item $\beta_1 < \gamma_1$ and $\gamma - \beta > \gamma_1 - \beta_1$
% \end{itemize}
% with sufficient constant dominance  $c_1 \gg c_2$ ensuring asymptotic hierarchy preservation.
% with the dominance condition $c_1 \gg c_2$ ensuring constant factors don't alter the asymptotic hierarchy.
The summation over low-fidelity models is bounded by
\[
\sum_{k=2}^{K} \sqrt{C_{i_k} \left(\rho_{1,i_k}^2 - \rho_{1,i_{k+1}}^2\right)} \lesssim c_2 \Phi(\epsilon),
\]
% When $c_1 \gg c_2$ (i.e., the high-fidelity constant dominates the low-fidelity aggregate constant), we have
When the high-fidelity contribution dominates the low-fidelity aggregate contributions, this implies
%
\begin{equation}\label{eq:high_greater_low}
    \sqrt{C_1 \left(1 - \rho_{1,i_2}^2\right)} \gg \sum_{k=2}^{K} \sqrt{C_{i_k} \left(\rho_{1,i_k}^2 - \rho_{1,i_{k+1}}^2\right)},
\end{equation}
%
and thus
%
\begin{equation*}
    \mathcal{W}^{\mathrm{MF}} \lesssim \epsilon^{-2} \left( \epsilon^{-\frac{\gamma-\beta}{2\alpha}} \right)^2 = \epsilon^{-2 - \frac{\gamma-\beta}{\alpha}}.
\end{equation*}
%

The dominance condition \eqref{eq:high_greater_low} ensures this simplification holds regardless of the specific asymptotic regime of $\beta_1$ and $\gamma_1$.

% \[
% \sum_{k=1}^K C_{i_k}\simeq M_L^\gamma  + \sum_{k=2}^K M_{\ell(i_k)}^{\gamma_1}\simeq s^{\gamma L} + \sum_{k=2}^K s^{\gamma_1 (L+1-i_k)}
% \]
\end{proof}









%% ============================================================================
% Consider feasible solutions in which the sample sizes $N_k^*$ are non-decreasing but may include segments where they are constant. Let $\{\ell_1, \ldots, \ell_q\}\subseteq \{1,\ldots, K\}$ be the indices marking the start of each constant block, with $\ell_1=1$ and $\ell_{q+1} = K+1$, such that
% %
% \[
% N_{\ell_1}<N_{\ell_2}<\ldots < N_{\ell_{q}},\quad\quad  N_{\ell_i}=N_{\ell_i+1}=\ldots = N_{\ell_{i+1}-1} <N_{\ell_{i+1}}, \qquad \text{for}\;\;  i=1,\ldots,q-1.
% \]
% %
% This partitions the indices ${1,\dots,K}$ into $q$ contiguous blocks of constant sample sizes, increasing from one block to the next. 

% Three special cases illustrate the structure of such feasible solutions. When $q=1$, all sample sizes are equal, i.e., $N_1=\ldots=N_K$. From \eqref{eq:MFMC_variance}, we then have $N_k=\sigma_1^2/\epsilon_{\text{tar}}^2$ for $k=1,\ldots, K$, $\alpha_k\in \mathbb{R}$, and the total cost is $\mathcal{W}^\text{MF} = \sigma_1^2/\epsilon_{\text{tar}}^2 \sum_{k=1}^K C_k$. When $q=K$, the sample sizes are strictly increasing, i.e., $N_1<\ldots<N_K$,  and we will show that this configuration yields the globally optimal solution. For intermediate values $1<q<K$, the sample sizes exhibit piecewise-constant blocks. In the following, to analyze this general regime, we formulate the corresponding Lagrangian and derive the associated Karush–Kuhn–Tucker (KKT) conditions.



% The Lagrangian for the constrained problem, enforcing the variance constraint with multiplier $\lambda_0$ and monotonicity with $\lambda_1,\ldots, \lambda_k$, is
% %
% \begin{equation*}
% L =\sum_{k=1}^K C_kN_k +\lambda_0 \left(\frac{\sigma_1^2}{N_1} + \sum_{k=2}^K \left(\frac{1}{N_{k-1}} - \frac{1}{N_k}\right)G_k- \epsilon_{\text{tar}}^2\right)-\lambda_1 N_1+\sum_{k=2}^K\lambda_k(N_{k-1} - N_k),
% \end{equation*}
% %
% with $\alpha_1 = 1, \alpha_{K+1} = 0$, $\lambda_{K+1} = 0$ and $G_k = \alpha_k^2\sigma_k^2 - 2\alpha_k\rho_{1,k}\sigma_1\sigma_k$.  


% Within each constant block, complementary slackness implies $\lambda_{\ell_i}= 0$ all $i\ge 1$, since the sample sizes within each block are equal. Under the blockwise structure $N_{\ell_i}=N_{\ell_i+1}=\ldots=N_{\ell_{i+1}-1}< N_{\ell_{i+1}}$ for $i=1,\ldots,q$, the Lagrangian simplifies to
% %
% \begin{equation*}
% L= \sum_{i=1}^q N_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_k +\lambda_0 \left(\frac{\sigma_1^2}{N_{\ell_1}} + \sum_{i=2}^q \left(\frac{1}{N_{\ell_i-1}} - \frac{1}{N_{\ell_i}}\right)G_{\ell_i}- \epsilon_{\text{tar}}^2\right)-\lambda_{\ell_1} N_{\ell_1}+\sum_{i=2}^q\lambda_{\ell_{i}}(N_{\ell_{i-1}} - N_{\ell_{i}}),
% \end{equation*}
% %
% Stationarity of the Lagrangian with respect to $\alpha_{\ell_i}$ yields
% %
% \begin{align}
% \label{eq:partial_L_alpha_k}
%     \frac{\partial L}{\partial \alpha_{\ell_i}}&=\lambda_0\left(\frac{1}{N_{\ell_i-1}} - \frac{1}{N_{\ell_i}}\right)\left(2\alpha_{\ell_i}\sigma_{\ell_i}^2 - 2\rho_{1,\ell_i}\sigma_1\sigma_{\ell_i}\right),\quad i=1,\dots,q-1,
%     % \frac{\partial L}{\partial N_1}&=C_1 + \lambda_0\left(-\frac{\sigma_1^2}{N_1^2} - \frac{\alpha_2^2\sigma_2^2-2\alpha2\rho_{1,2}\sigma_1\sigma_2}{N_1^2}\right)-\lambda_1+\lambda_2,\\
%     % \label{eq:partial_L_N_k}
%     % \frac{\partial L}{\partial N_k}&=C_k+\lambda_0\left(\frac{G_k}{N_k^2}-\frac{G_{k+1}}{N_k^2}\right)-\lambda_k+\lambda_{k+1}, \quad k=1,\dots,K,
%     % \frac{\partial L}{\partial N_K}&=C_K + \lambda_0\left(\frac{\alpha_K^2\sigma_K^2 - 2\alpha_K\rho_{1,K}\sigma_1\sigma_K}{N_K^2}\right)-\lambda_K.
% \end{align}
% %
% Solving $\partial L/\partial \alpha_{\ell_i} = 0$ gives the optimal weights
% %
% \[
% \alpha_{k}^* = \frac{\rho_{1,k}\sigma_1}{\sigma_{k}}, \text{ if } k=\ell_i, \text{ for }i= 1,\ldots,q.
% \]
% %
% Note only indices $k=\ell_i$ contribute non-trivially to the estimator weights in \eqref{eq:MFMC_Yk}. For indices $k = \ell_i+1,\ldots, \ell_{i+1}-1$, the corresponding Monte Carlo estimators cancel due to shared sample sets, and thus do not appear explicitly in the final estimator. Using the optimal coefficients $\alpha_k^*$, and observing that $N_{\ell_{i}-1} = N_{\ell_{i-1}}$ for $i \ge 2$. Substituting into the variance expression yields the simplified form
% %
% \begin{equation}\label{eq:MFMC_var_convex}
%     \mathbb{V}\left[A^{\text{MF}}\right] = \frac{\sigma_1^2}{N_1}+\sum_{i=2}^q
% \left(\frac{1}{N_{\ell_{i}}}-\frac{1}{N_{\ell_{i}-1}}\right)\rho_{1,\ell_i}^2\sigma_1^2=\sum_{i=1}^{q} \frac{ \left(\rho_{1,\ell_i}^2-\rho_{1,\ell_{i+1}}^2\right)\sigma_1^2}{N_{\ell_i}}=\sum_{i=1}^{q} \frac{\Delta_{\ell_i}}{N_{\ell_i}}.
% \end{equation}
% %

% Substituting the optimal coefficients into the Lagrangian and using the block-wise sample size notation, we obtain
% %
% \begin{equation*}
% L= \sum_{i=1}^q N_{\ell_i}\sum_{k=\ell_i}^{\ell_{i+1}-1} C_k +\lambda_0 \left( \sum_{i=1}^{q} \frac{\Delta_{\ell_i}}{N_{\ell_i}}- \epsilon_{\text{tar}}^2\right)-\lambda_{\ell_1} N_{\ell_1}+\sum_{i=2}^q\lambda_{\ell_{i}}(N_{\ell_{i-1}} - N_{\ell_{i}}).
% \end{equation*}
% %
% The stationarity condition for $N_{\ell_i}$ yields
% %
% \[
% \frac{\partial L}{\partial N_{\ell_i}} =\sum_{k=\ell_i}^{\ell_{i+1}-1} C_{k} -  \frac{\lambda_0\Delta_{\ell_i}}{N_{\ell_i}^2}-\lambda_{\ell_{i}}+\lambda_{\ell_{i+1}}=0,\quad i = 1, \ldots,q,
% \]
% %
% Since the sample sizes increase strictly ($N_{\ell_{i-1}} < N_{\ell_i}$ for $i = 2, \ldots, q$), all inequality constraints are inactive, implying $\lambda_{\ell_i} = 0$ by complementary slackness. The optimal sample sizes are then
% %
% \begin{equation}\label{eq:sample_size_1}
%     N_{\ell_i} = \sqrt{\lambda_0} \sqrt{\frac{\Delta_{\ell_i}}{\sum_{k=\ell_i}^{\ell_{i+1}-1} C_{k}}}, \;\text{ for }\; i=1,\ldots,q,
% \end{equation}
% %



%% ============================================================================

% %
% \begin{theorem}[Sampling Cost for MFMC Estimator]\label{thm:Sample_cost_est}
% Let $\mathcal{I}^* = \{i_k \mid k = 2, \dots, K^*\}$ index optimally selected ordered low-fidelity models $u_{\ell(i_k),i_k}$ with correlations $\rho_{1,i_k}$ to the high-fidelity model $u_{L,1}$ and cost per sample $C_{i_k} \ll C_1$. For the family of high-fidelity models $\{u_{L,1}\}_{L\le L_m}$ with $M_L$ degrees of freedom, assume the folloing conditions
% %
% \begin{alignat*}{2}
%     \text{(i)}\quad & \left\Vert \mathbb{E}[u] - \mathbb{E}[u_{L,1}] \right\Vert_Z \simeq M_L^{-\alpha}, \qquad
%     \text{(ii)}\quad & 1 - \rho_{1,i_2}^2 \simeq M_L^{-\beta}, \qquad
%     \text{(iii)}\quad & C_1 \simeq M_L^{\gamma},
% \end{alignat*}
% %
% where $\rho_{1,i_2}$ corresponds to the best low-fidelity model. Then for any $\epsilon \in (0,e^{-1})$, there exist $L$ and sample sizes $\{N_{i_k}\}$ (defined in~\eqref{eq:MFMC_SampleSize}) such that the multifidelity estimator $A^{\mathrm{MF}}$ achieves relative error
% %
% \[
% \frac{\left\Vert \mathbb{E}[u] - A^{\mathrm{MF}} \right\Vert_{L^2(\boldsymbol{W},Z)}}{\left\Vert \mathbb{E}[u] \right\Vert_{L^2(\boldsymbol{W},Z)}} < \epsilon,
% \]
% with total computational cost scaling as
% \[
% \mathcal{W}^{\mathrm{MF}} \simeq \epsilon^{-2 + \frac{\beta - \gamma}{\alpha}}.
% \]
% \end{theorem}
% %
% \begin{proof}\label{eq:Sample_cost_est}

% From Theorem \eqref{thm:Sample_size_est}, the minimal computational cost \eqref{eq:MFMC_sampling_cost} to achieve the relative mean-squared error $\epsilon^2$ is 
% %
% \begin{equation}\label{eq:MFMC_SC_cost_core}
%     \mathcal{W}^\text{MF} \simeq \epsilon^{-2}\left(\sqrt{C_1\left(1 - \rho_{1,i_2}^2\right)}+\sum_{k=2}^{K^*} \sqrt{C_{i_k}\left(\rho_{1,{i_k}}^2 - \rho_{1,i_{k+1}}^2\right)} \right)^2.
% \end{equation}
% %
% where $\rho_{1,i_{K^*+1}}^2 := 0$. Substituting relations (ii) and (iii) yields
% %
% \begin{equation}\label{eq:dominant_term}
% \sqrt{C_1 (1 - \rho_{1,i_2}^2)} \simeq M_L^{(\gamma - \beta)/2}.
% \end{equation}
% %
% To bound the summation term, we apply the Cauchy-Schwarz inequality
% %
% \[
% \sum_{k=2}^{K^*} \sqrt{C_{i_k}\left(\rho_{1,{i_k}}^2 - \rho_{1,i_{k+1}}^2\right)}\le \sqrt{\sum_{k=2}^{K^*}C_{i_k}}\cdot \sqrt{\sum_{k=2}^{K^*}\left(\rho_{1,i_k}^2 -\rho_{1,i_{k+1}}^2\right) }=\sqrt{\sum_{k=2}^{K^*}C_{i_k}} \cdot |\rho_{1,i_2}|\le \sqrt{\sum_{k=2}^{K^*}C_{i_k}} \rightarrow 0
% \]
% %
% subsequent terms in \eqref{eq:MFMC_SC_cost_core} are asymptotically dominated by the first term since $C_{i_k} \ll C_1$. Condition (i) bounds the bias by $\mathcal{O}(M_L^{-\alpha})$. To achieve relative error $\epsilon$, Equating the bias to $\mathcal{O}(\epsilon)$ gives $M_L^{-\alpha} \simeq \epsilon$, implying $M_L \simeq \epsilon^{-1/\alpha}$. Thus:
% \[
% \mathcal{W}^{\mathrm{MF}} \simeq \epsilon^{-2} M_L^{\gamma - \beta}\simeq \epsilon^{-2+\frac{\beta-\gamma}{\alpha}}.
% \]
% % %
% % \begin{align*}
% %     \mathcal{W}^\text{MF} &\simeq \epsilon^{-2}\left(\sqrt{C_1\left(\rho_{1,1}^2 - \rho_{1,2}^2\right)}+\sum_{k=2}^{L+1} \sqrt{C_k\left(\rho_{1,k}^2 - \rho_{1,k+1}^2\right)} \right)^2 \simeq \epsilon^{-2} \left(c_1 M_L^{\frac{\gamma-\beta}{2}}+c_2\sum_{k=2}^{L+1}M_{L-k+1}^\frac{\gamma_1-\beta_1}{2}\right)^2,\\
% %     &=\epsilon^{-2} \left(c_1 M_L^{\frac{\gamma-\beta}{2}}+c_2\sum_{p=0}^{L-1}M_{p}^\frac{\gamma_1-\beta_1}{2}\right)^2\simeq \epsilon^{-2}\left(c_1s^{\frac{(\gamma-\beta)}{2}L}+c_2\sum_{p=0}^{L-1}s^{\frac{(\gamma_1-\beta_1)}{2}p}\right)^2,
% % \end{align*}
% % %
% % Since $K^*\simeq L$, we use the geometric sum approximation
% % %
% % \begin{equation}
% % \label{eq:Geo_sum_for_s}
% % \sum_{p=0}^L s^{\eta p}\simeq\left\{\begin{array}{ll}
% % \frac{1}{1-s^{\eta}}, & \eta<0,\\
% % |\log \epsilon|, & \eta = 0,\\
% % \epsilon^{-\frac{\eta}{\alpha}}, & \eta>0,
% % \end{array}
% % \right.
% % \end{equation}
% % %
% % Applying \eqref{eq:SLSGC_MLS_SpatialGridsNo} and condition (i), we substitute $s^{\frac{(\gamma-\beta)}{2}L}\simeq \epsilon^{\frac{\beta-\gamma}{2\alpha}}$, leading to the asymptotic result
% % \[
% % \mathcal{W}^\text{MF} \simeq \epsilon^{-2+\frac{\beta-\gamma}{\alpha}}.
% % \]
% \end{proof}